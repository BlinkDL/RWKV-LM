{
    "summary": "This code utilizes DeepSpeed and implements a transformer layer with CUDA operations. It creates RWKV model architecture, initializes an optimizer, and optimizes memory usage for transformer models.",
    "details": [
        {
            "comment": "Importing necessary libraries, setting profiling mode for torch.jit, defining custom modules and functions, integrating DeepSpeed library if available, and configuring environment variables for testing and JIT usage.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":0-30",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport os, math, gc, importlib\nimport torch\n# torch._C._jit_set_profiling_executor(True)\n# torch._C._jit_set_profiling_mode(True)\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities import rank_zero_info, rank_zero_only\nfrom pytorch_lightning.strategies import DeepSpeedStrategy\nif importlib.util.find_spec('deepspeed'):\n    import deepspeed\n    from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n# from deepspeed.runtime.fp16.onebit.zoadam import ZeroOneAdam\ntry:\n    print('RWKV_MY_TESTING', os.environ[\"RWKV_MY_TESTING\"])\nexcept:\n    os.environ[\"RWKV_MY_TESTING\"] = ''\ndef __nop(ob):\n    return ob\nMyModule = nn.Module\nMyFunction = __nop\nif os.environ[\"RWKV_JIT_ON\"] == \"1\":"
        },
        {
            "comment": "This code imports a CUDA kernel and uses it in the WKV_5 class, which performs operations on tensors of specific data types and dimensions. It asserts the input tensor sizes to ensure they match the expected format and checks the value of HEAD_SIZE to be equal to C divided by H. The code then instantiates the wkv5 CUDA kernel from the \"wkv5\" module with specific sources, flags, and verbose settings.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":31-54",
            "content": "    MyModule = torch.jit.ScriptModule\n    MyFunction = torch.jit.script_method\n########################################################################################################\n# CUDA Kernel\n########################################################################################################\nfrom torch.utils.cpp_extension import load\nHEAD_SIZE = int(os.environ[\"RWKV_HEAD_SIZE_A\"])\nwkv5_cuda = load(name=\"wkv5\", sources=[\"cuda/wkv5_op.cpp\", f\"cuda/wkv5_cuda.cu\"],\n                verbose=True, extra_cuda_cflags=[\"-res-usage\", \"--use_fast_math\", \"-O3\", \"-Xptxas -O3\", \"--extra-device-vectorization\", f\"-D_N_={HEAD_SIZE}\"])\nclass WKV_5(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, B, T, C, H, r, k, v, w, u):\n        with torch.no_grad():\n            assert r.dtype == torch.bfloat16\n            assert k.dtype == torch.bfloat16\n            assert v.dtype == torch.bfloat16\n            assert w.dtype == torch.bfloat16\n            assert u.dtype == torch.bfloat16\n            assert HEAD_SIZE == C // H"
        },
        {
            "comment": "This code appears to be part of a model's forward and backward pass implementation. In the forward pass, it initializes variables for batch size (B), sequence length (T), feature dimensions (C), hidden state dimension (H), and tensors r, k, v, eew, ew, u, and y. It then calls a forward method to compute y using these variables and returns y. In the backward pass, it checks the gradient tensor gy's type and shape, retrieves saved tensors from the context, and initializes an empty tensor gr of the same shape as gy for gradients computation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":55-81",
            "content": "            ctx.B = B\n            ctx.T = T\n            ctx.C = C\n            ctx.H = H\n            assert r.is_contiguous()\n            assert k.is_contiguous()\n            assert v.is_contiguous()\n            assert w.is_contiguous()\n            assert u.is_contiguous()\n            ew = (-torch.exp(w.float())).contiguous()\n            eew = (torch.exp(ew)).contiguous()\n            ctx.save_for_backward(r, k, v, eew, ew, u)\n            y = torch.empty((B, T, C), device=r.device, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)\n            wkv5_cuda.forward(B, T, C, H, r, k, v, eew, u, y)\n            return y\n    @staticmethod\n    def backward(ctx, gy):\n        with torch.no_grad():\n            assert gy.dtype == torch.bfloat16\n            B = ctx.B\n            T = ctx.T\n            C = ctx.C\n            H = ctx.H\n            assert gy.is_contiguous()\n            r, k, v, eew, ew, u = ctx.saved_tensors\n            gr = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)"
        },
        {
            "comment": "This code is initializing various tensors for model parameters, and then calling a function to apply CUDA operations on these tensors. The function RUN_CUDA_RWKV5 calls the apply method of class WKV_5 with given input dimensions and parameters. It returns the gradients of the input parameters after applying the CUDA operations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":82-94",
            "content": "            gk = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)\n            gv = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)\n            gw = torch.empty((B, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)\n            gu = torch.empty((B, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)\n            wkv5_cuda.backward(B, T, C, H, r, k, v, eew, ew, u, gy, gr, gk, gv, gw, gu)\n            gw = torch.sum(gw, 0).view(H, C//H)\n            gu = torch.sum(gu, 0).view(H, C//H)\n            return (None, None, None, None, gr, gk, gv, gw, gu)\ndef RUN_CUDA_RWKV5(B, T, C, H, r, k, v, w, u):\n    return WKV_5.apply(B, T, C, H, r, k, v, w, u)\n########################################################################################################"
        },
        {
            "comment": "This code defines a class for RWKV_TimeMix_RWKV5, a type of MyModule. It initializes instance variables based on input arguments and asserts the head size consistency. The time_mix parameters are calculated using layer ID, number of layers, and embedding dimension.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":96-118",
            "content": "class RWKV_TimeMix_RWKV5(MyModule):\n    def __init__(self, args, layer_id):\n        super().__init__()\n        self.args = args\n        self.layer_id = layer_id\n        self.head_size = args.head_size_a\n        assert HEAD_SIZE == self.head_size # change HEAD_SIZE to match args.head_size_a\n        self.n_head = args.dim_att // self.head_size\n        assert args.dim_att % self.n_head == 0\n        self.head_size_divisor = args.head_size_divisor\n        with torch.no_grad():\n            ratio_0_to_1 = layer_id / (args.n_layer - 1)  # 0 to 1\n            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)  # 1 to ~0\n            ddd = torch.ones(1, 1, args.n_embd)\n            for i in range(args.n_embd):\n                ddd[0, 0, i] = i / args.n_embd\n            # fancy time_mix\n            self.time_mix_k = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n            self.time_mix_v = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0) + 0.3 * ratio_0_to_1)\n            self.time_mix_r = nn.Parameter(torch.pow(ddd, 0.5 * ratio_1_to_almost0))"
        },
        {
            "comment": "This code is initializing parameters for a transformer layer in RWKV-v5. It defines time_mix_g, time_decay, and time_faaaa as learnable parameters. The time_decay and time_faaaa parameters control the time-based attention mechanism, while time_shift is a zero padding operator and receptance and key are linear layers for processing the input embeddings.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":119-137",
            "content": "            self.time_mix_g = nn.Parameter(torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n            # fancy time_decay\n            decay_speed = torch.ones(args.dim_att)\n            for n in range(args.dim_att):\n                decay_speed[n] = -6 + 5 * (n / (args.dim_att - 1)) ** (0.7 + 1.3 * ratio_0_to_1)\n            self.time_decay = nn.Parameter(decay_speed.reshape(self.n_head, self.head_size))\n            # print(layer_id, self.time_decay.flatten()[:3].cpu().numpy(), '...', self.time_decay.flatten()[-3:].cpu().numpy())\n            tmp = torch.zeros(args.dim_att)\n            for n in range(args.dim_att):\n                zigzag = ((n + 1) % 3 - 1) * 0.1\n                tmp[n] = ratio_0_to_1 * (1 - (n / (args.dim_att - 1))) + zigzag\n            self.time_faaaa = nn.Parameter(tmp.reshape(self.n_head, self.head_size))\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        self.receptance = nn.Linear(args.n_embd, args.dim_att, bias=False)\n        self.key = nn.Linear(args.n_embd, args.dim_att, bias=False)"
        },
        {
            "comment": "This code defines a class with three linear layers for key, value, and gate calculations. It also includes a GroupNorm layer and two JIT functions for forward pass and normalization. The functions apply time shifting, calculate key, value, and receptance, and perform normalization using GroupNorm.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":139-168",
            "content": "        self.value = nn.Linear(args.n_embd, args.dim_att, bias=False)\n        self.output = nn.Linear(args.dim_att, args.n_embd, bias=False)\n        self.gate = nn.Linear(args.n_embd, args.dim_att, bias=False)\n        self.ln_x = nn.GroupNorm(self.n_head, args.dim_att)\n    @MyFunction\n    def jit_func(self, x):\n        B, T, C = x.size()\n        xx = self.time_shift(x) # Mix x with the previous timestep to produce xk, xv, xr\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        xg = x * self.time_mix_g + xx * (1 - self.time_mix_g)\n        r = self.receptance(xr)\n        k = self.key(xk)\n        v = self.value(xv)\n        g = F.silu(self.gate(xg))\n        return r, k, v, g\n    @MyFunction\n    def jit_func_2(self, x, g):\n        B, T, C = x.size()\n        x = x.view(B * T, C)\n        x = self.ln_x(x / self.head_size_divisor).view(B, T, C)\n        x = self.output(x * g)\n        return x"
        },
        {
            "comment": "The code defines a forward pass for a model, with input size BTC and H being the number of heads. It uses jit_func to calculate r, k, v, and g. RUN_CUDA_RWKV5 is applied on these calculated values. RWKV_ChannelMix is a subclass of MyModule with an args parameter for arguments and layer_id for layer identification. It initializes time_shift with a zero pad 2D, performs fancy initialization of time_mix by setting ratio_1_to_almost0 and calculating ddd. It also initializes key as a linear layer without bias.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":170-197",
            "content": "    def forward(self, x):\n        B, T, C = x.size()\n        H = self.n_head\n        r, k, v, g = self.jit_func(x)\n        x = RUN_CUDA_RWKV5(B, T, C, H, r, k, v, w=self.time_decay, u=self.time_faaaa)\n        return self.jit_func_2(x, g)\n########################################################################################################\nclass RWKV_ChannelMix(MyModule):\n    def __init__(self, args, layer_id):\n        super().__init__()\n        self.args = args\n        self.layer_id = layer_id\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        with torch.no_grad():  # fancy init of time_mix\n            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)  # 1 to ~0\n            ddd = torch.ones(1, 1, args.n_embd)\n            for i in range(args.n_embd):\n                ddd[0, 0, i] = i / args.n_embd\n            self.time_mix_k = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n            self.time_mix_r = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n        self.key = nn.Linear(args.n_embd, args.dim_ffn, bias=False)"
        },
        {
            "comment": "This code initializes layers for a MishGLU module, including time shifting and mixing parameters. The time_shift layer pads the input tensor with a one-pixel shift, while time_mix_k and time_mix_r are learned parameters for mixing inputs. These operations are used to calculate key and receptance values in the forward pass before returning the final result through a sigmoid function multiplied by the value tensor.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":198-226",
            "content": "        self.receptance = nn.Linear(args.n_embd, args.n_embd, bias=False)\n        self.value = nn.Linear(args.dim_ffn, args.n_embd, bias=False)\n    @MyFunction\n    def forward(self, x):\n        xx = self.time_shift(x)\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        k = self.key(xk)\n        k = torch.relu(k) ** 2\n        kv = self.value(k)\n        return torch.sigmoid(self.receptance(xr)) * kv\nclass MishGLU(MyModule):\n    def __init__(self, args, layer_id):\n        super().__init__()\n        self.args = args\n        self.layer_id = layer_id\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        with torch.no_grad():\n            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)\n            x = torch.ones(1, 1, args.n_embd)\n            for i in range(args.n_embd):\n                x[0, 0, i] = i / args.n_embd\n            self.time_mix_k = nn.Parameter(torch.pow(x, ratio_1_to_almost0))\n            self.time_mix_r = nn.Parameter(torch.pow(x, ratio_1_to_almost0))"
        },
        {
            "comment": "This code defines the architecture of an RWKV model with layers for time-shifting, mixing, and applying a Mish activation function. The `Block` class represents each layer in the model, which includes layer normalization, feed forward network (FFN), and optional additional layer normalization for the first layer. It also defines functions to perform time shifting and linear transformations using fully connected layers.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":227-255",
            "content": "            self.aa = nn.Linear(args.n_embd, args.dim_ffn, bias=False)\n            self.bb = nn.Linear(args.n_embd, args.dim_ffn, bias=False)\n            self.value = nn.Linear(args.dim_ffn, args.n_embd, bias=False)\n    @MyFunction\n    def forward(self, x):\n        xx = self.time_shift(x)\n        xa = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xb = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        a = self.aa(xa)\n        b = self.bb(xb)\n        return self.value(a * F.mish(b))\n########################################################################################################\n# The RWKV Model with our blocks\n########################################################################################################\nclass Block(nn.Module):\n    def __init__(self, args, layer_id):\n        super().__init__()\n        self.args = args\n        self.layer_id = layer_id\n        self.ln1 = nn.LayerNorm(args.n_embd)\n        self.ln2 = nn.LayerNorm(args.n_embd)\n        if self.layer_id == 0:\n            self.ln0 = nn.LayerNorm(args.n_embd)"
        },
        {
            "comment": "The code initializes model components based on provided arguments and layer ID. If `my_pos_emb > 0`, it creates positional embedding parameters. If `pre_ffn > 0`, it creates an FFN layer (RWKV_ChannelMix) for the first layer. Depending on environment, it also initializes ffn layers (MishGLU or RWKV_ChannelMix). For tiny_att_dim and specific layer ID, it registers linear layers and buffer for tiny attention implementation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":256-275",
            "content": "            if args.my_pos_emb > 0:\n                self.pos_emb_x = nn.Parameter(torch.zeros((1,args.my_pos_emb,args.n_embd)))\n                self.pos_emb_y = nn.Parameter(torch.zeros((args.my_pos_emb,1,args.n_embd)))\n        if self.layer_id == 0 and self.args.pre_ffn > 0:\n            self.ffnPre = RWKV_ChannelMix(args, 0)\n        else:\n            self.att = RWKV_TimeMix_RWKV5(args, layer_id)\n        if 'g' in os.environ[\"RWKV_MY_TESTING\"]:\n            self.ffn = MishGLU(args, layer_id)\n        else:\n            self.ffn = RWKV_ChannelMix(args, layer_id)\n        if args.tiny_att_dim > 0 and self.layer_id == args.tiny_att_layer:\n            self.tiny_ln = nn.LayerNorm(args.n_embd)\n            self.tiny_q = nn.Linear(args.n_embd, args.tiny_att_dim, bias=False)\n            self.tiny_k = nn.Linear(args.n_embd, args.tiny_att_dim, bias=False)\n            self.tiny_v = nn.Linear(args.n_embd, args.n_embd, bias=False)\n            self.register_buffer(\"tiny_mask\", torch.tril(torch.ones(args.ctx_len, args.ctx_len)))"
        },
        {
            "comment": "The code implements a layer of the RWKV-v5 model, including dropout regularization, layers normalization, feed-forward network (FFN), and attention mechanism. The dropout rate is determined by the \"args\" parameter and can be zero. If the pre_ffn argument is greater than 0, it executes FFN before other operations. The code also includes a tiny attention layer at a specific layer specified by the \"tiny_att_dim\" argument.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":277-304",
            "content": "        if args.dropout > 0:\n            self.drop0 = nn.Dropout(p = args.dropout)\n            self.drop1 = nn.Dropout(p = args.dropout)\n    def forward(self, x, x_emb=None):\n        args = self.args\n        B, T, C = x.size()\n        if self.layer_id == 0:\n            x = self.ln0(x)\n            if args.my_pos_emb > 0:\n                pos_emb = (self.pos_emb_x + self.pos_emb_y).reshape(T+1, -1)[:-1,:]\n                x = x + pos_emb\n        if self.args.dropout == 0:\n            if self.layer_id == 0 and args.pre_ffn > 0:\n                x = x + self.ffnPre(self.ln1(x))\n            else:\n                x = x + self.att(self.ln1(x))\n            x = x + self.ffn(self.ln2(x))\n        else:\n            if self.layer_id == 0 and args.pre_ffn > 0:\n                x = self.drop0(x + self.ffnPre(self.ln1(x)))\n            else:\n                x = self.drop0(x + self.att(self.ln1(x)))\n            x = self.drop1(x + self.ffn(self.ln2(x)))\n        if args.tiny_att_dim > 0 and self.layer_id == args.tiny_att_layer:\n            xx = self.tiny_ln(x)"
        },
        {
            "comment": "This code defines a class RWKV, which inherits from LightningModule. It includes an initialization method that takes arguments and creates instance variables. The class also has a forward method for the L2Wrap function, which computes a loss and applies a specific gradient calculation. This appears to be part of a deep learning model implementation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":305-337",
            "content": "            q = self.tiny_q(xx)[:, :T, :]\n            k = self.tiny_k(xx)[:, :T, :]\n            c = (q @ k.transpose(-2, -1)) * (args.tiny_att_dim ** (-0.5))\n            c = c.masked_fill(self.tiny_mask[:T, :T] == 0, 0)\n            x = x + c @ self.tiny_v(x_emb)\n        return x\nclass L2Wrap(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, loss, y):\n        ctx.save_for_backward(y)\n        return loss\n    @staticmethod\n    def backward(ctx, grad_output):\n        y = ctx.saved_tensors[0]\n        # to encourage the logits to be close to 0\n        factor = 1e-4 / (y.shape[0] * y.shape[1])\n        maxx, ids = torch.max(y, -1, keepdim=True)\n        gy = torch.zeros_like(y)\n        gy.scatter_(-1, ids, maxx * factor)\n        return (grad_output, gy)\nclass RWKV(pl.LightningModule):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        if not hasattr(args, 'dim_att'):\n            args.dim_att = args.n_embd\n        if not hasattr(args, 'dim_ffn'):\n            args.dim_ffn = args.n_embd * 4"
        },
        {
            "comment": "The code defines a model with multiple layers, embeddings, and optional components depending on the provided arguments. It also sets up optimizer configurations based on the specified arguments.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":338-365",
            "content": "        if not hasattr(args, 'tiny_att_layer'):\n            args.tiny_att_layer = -1\n        if not hasattr(args, 'tiny_att_dim'):\n            args.tiny_att_dim = -1\n        assert args.n_embd % 32 == 0\n        assert args.dim_att % 32 == 0\n        assert args.dim_ffn % 32 == 0\n        self.emb = nn.Embedding(args.vocab_size, args.n_embd)\n        self.blocks = nn.ModuleList([Block(args, i) for i in range(args.n_layer)])\n        self.ln_out = nn.LayerNorm(args.n_embd)\n        self.head = nn.Linear(args.n_embd, args.vocab_size, bias=False)\n        if args.head_qk > 0:\n            self.head_q = nn.Linear(args.n_embd, args.head_qk, bias=False)\n            self.head_k = nn.Linear(args.n_embd, args.head_qk, bias=False)\n            self.register_buffer(\"copy_mask\", torch.tril(torch.ones(args.ctx_len, args.ctx_len)))\n        if args.dropout > 0:\n            self.drop0 = nn.Dropout(p = args.dropout)\n    def configure_optimizers(self):\n        args = self.args\n        lr_decay = set()\n        lr_1x = set()\n        lr_2x = set()"
        },
        {
            "comment": "This code is assigning different learning rates based on parameter names and certain conditions. It first defines four sets (lr_1x, lr_2x, lr_3x, lr_decay) and then populates them according to specific criteria: if a parameter's name matches certain strings or has a layerwise learning rate greater than 0, it will be added to the corresponding set. Finally, the sets are sorted and stored.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":366-393",
            "content": "        lr_3x = set()\n        for n, p in self.named_parameters():\n            if (\"time_mix\" in n) and (args.layerwise_lr > 0):\n                if args.my_pile_stage == 2:\n                    lr_2x.add(n)\n                else:\n                    lr_1x.add(n)\n            elif (\"time_decay\" in n) and (args.layerwise_lr > 0):\n                if args.my_pile_stage == 2:\n                    lr_3x.add(n)\n                else:\n                    lr_2x.add(n)\n            elif (\"time_faaaa\" in n) and (args.layerwise_lr > 0):\n                if args.my_pile_stage == 2:\n                    lr_2x.add(n)\n                else:\n                    lr_1x.add(n)\n            elif (\"time_first\" in n) and (args.layerwise_lr > 0):\n                lr_3x.add(n)\n            elif (len(p.squeeze().shape) >= 2) and (args.weight_decay > 0):\n                lr_decay.add(n)\n            else:\n                lr_1x.add(n)\n        lr_decay = sorted(list(lr_decay))\n        lr_1x = sorted(list(lr_1x))\n        lr_2x = sorted(list(lr_2x))\n        lr_3x = sorted(list(lr_3x))"
        },
        {
            "comment": "The code defines different optimizer groups for layers based on the desired layer-wise learning rate. It separates parameters into three groups with scaling factors of 1x, 2x, and 3x (relative to initial learning rate), and sets their respective weights decay to zero.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":394-411",
            "content": "        # print('decay', lr_decay)\n        # print('1x', lr_1x)\n        # print('2x', lr_2x)\n        # print('3x', lr_3x)\n        param_dict = {n: p for n, p in self.named_parameters()}\n        if args.layerwise_lr > 0:\n            if args.my_pile_stage == 2:\n                optim_groups = [\n                    {\"params\": [param_dict[n] for n in lr_1x], \"weight_decay\": 0.0, \"my_lr_scale\": 1.0},\n                    {\"params\": [param_dict[n] for n in lr_2x], \"weight_decay\": 0.0, \"my_lr_scale\": 5.0},# test: 2e-3 / args.lr_init},\n                    {\"params\": [param_dict[n] for n in lr_3x], \"weight_decay\": 0.0, \"my_lr_scale\": 5.0},# test: 3e-3 / args.lr_init},\n                ]\n            else:\n                optim_groups = [\n                    {\"params\": [param_dict[n] for n in lr_1x], \"weight_decay\": 0.0, \"my_lr_scale\": 1.0},\n                    {\"params\": [param_dict[n] for n in lr_2x], \"weight_decay\": 0.0, \"my_lr_scale\": 2.0},\n                    {\"params\": [param_dict[n] for n in lr_3x], \"weight_decay\": 0.0, \"my_lr_scale\": 3.0},"
        },
        {
            "comment": "This code sets up an optimizer for the model. It creates a list of optimization groups, adding either a group with no weight decay or a group with weight decay based on arguments provided. Depending on the presence of a weight decay argument and whether DeepSpeed is being used, it returns a corresponding optimizer - either DeepSpeedCPUAdam or FusedAdam.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":412-424",
            "content": "                ]\n        else:\n            optim_groups = [{\"params\": [param_dict[n] for n in lr_1x], \"weight_decay\": 0.0, \"my_lr_scale\": 1.0}]\n        if args.weight_decay > 0:\n            optim_groups += [{\"params\": [param_dict[n] for n in lr_decay], \"weight_decay\": args.weight_decay, \"my_lr_scale\": 1.0}]\n            if self.deepspeed_offload:\n                return DeepSpeedCPUAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, adamw_mode=True, amsgrad=False)\n            return FusedAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, adam_w_mode=True, amsgrad=False)\n        else:\n            if self.deepspeed_offload:\n                return DeepSpeedCPUAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, adamw_mode=False, weight_decay=0, amsgrad=False)\n            return FusedAdam(optim_groups, lr=self.args.lr_init, betas="
        },
        {
            "comment": "Lines 424-448: Initialize Adam optimizer with specified arguments, including learning rate, betas, epsilon, bias correction, weight decay, and amsgrad flag.\nLine 451-456: Check if the current strategy is a DeepSpeedStrategy and return whether to offload the optimizer or parameters from this model.\nLine 458-470: Define the forward function for the model, applying embedding, dropout (if enabled), and potentially using DeepSpeed checkpointing with gradient clip (if enabled).",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":424-448",
            "content": "self.args.betas, eps=self.args.adam_eps, bias_correction=True, adam_w_mode=False, weight_decay=0, amsgrad=False)\n        # return ZeroOneAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, weight_decay=0, amsgrad=False, cuda_aware=False)\n    @property\n    def deepspeed_offload(self) -> bool:\n        strategy = self.trainer.strategy\n        if isinstance(strategy, DeepSpeedStrategy):\n            cfg = strategy.config[\"zero_optimization\"]\n            return cfg.get(\"offload_optimizer\") or cfg.get(\"offload_param\")\n        return False\n    def forward(self, idx):\n        args = self.args\n        B, T = idx.size()\n        assert T <= args.ctx_len, \"Cannot forward, model ctx_len is exhausted.\"\n        x = self.emb(idx)\n        x_emb = x\n        if args.dropout > 0:\n            x = self.drop0(x)\n        if args.tiny_att_dim > 0:\n            for block in self.blocks:\n                if args.grad_cp == 1:\n                    x = deepspeed.checkpointing.checkpoint(block, x, x_emb)"
        },
        {
            "comment": "This code is defining a multi-head attention mechanism in a transformer model, including options for head masking and different float modes (FP16 or BF16). It applies checkpointing for gradient accumulation and performs matrix operations to generate context vectors.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":449-477",
            "content": "                else:\n                    x = block(x, x_emb)\n        else:\n            for block in self.blocks:\n                if args.grad_cp == 1:\n                    x = deepspeed.checkpointing.checkpoint(block, x)\n                else:\n                    x = block(x)\n        x = self.ln_out(x)\n        if args.head_qk > 0:\n            q = self.head_q(x)[:, :T, :]\n            k = self.head_k(x)[:, :T, :]\n            c = (q @ k.transpose(-2, -1)) * (1.0 / args.head_qk)\n            c = c.masked_fill(self.copy_mask[:T, :T] == 0, 0)\n            if \"32\" in os.environ[\"RWKV_FLOAT_MODE\"]:\n                c = c @ F.one_hot(idx, num_classes=args.vocab_size)\n            elif os.environ[\"RWKV_FLOAT_MODE\"] == \"fp16\":\n                c = c @ F.one_hot(idx, num_classes=args.vocab_size).half()\n            elif os.environ[\"RWKV_FLOAT_MODE\"] == \"bf16\":\n                c = c @ F.one_hot(idx, num_classes=args.vocab_size).bfloat16()\n            x = self.head(x) + c\n        else:\n            x = self.head(x)\n        return x"
        },
        {
            "comment": "The code defines a training step function for the model. It checks an environment variable and performs different operations based on its value. If the variable is not equal to 1, it computes logits from input idx, calculates cross-entropy loss, and prints logits and idx if certain conditions are met. If the variable is equal to 1, it handles batches with masks, checks for mask sum, and adjusts the loss calculation based on whether all values in the mask are 0 or not. It also prints the loss for specific ranks.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":479-502",
            "content": "    def training_step(self, batch, batch_idx):\n        args = self.args\n        if args.my_qa_mask != 1:\n            idx, targets = batch\n            logits = self(idx)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n            # if '0' in os.environ[\"RWKV_MY_TESTING\"]:\n            #     print('logits', logits)\n            #     torch.set_printoptions(threshold=10000)\n            #     print('idx', idx)\n            #     exit(0)\n        else:\n            idx, targets, mask = batch\n            mask = mask.view(-1)\n            sum_mask = torch.sum(mask).item()\n            # if sum_mask == 0:\n            #     return torch.tensor([0.0], requires_grad=True)\n            logits = self(idx)\n            if sum_mask == mask.shape[0]:\n                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n                # print('rank', self.global_rank, 'loss', loss.item())\n            else:\n                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction='none')"
        },
        {
            "comment": "In this code snippet, the model calculates the loss for a batch of data. The loss is calculated by summing up the element-wise multiplication of the loss and a mask, then dividing by the sum of the mask. The code also includes additional logging functionality to gather statistics on the training process across multiple GPUs using PyTorch's all_gather function.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":503-529",
            "content": "                # loss_raw = loss\n                loss = torch.sum(loss * mask) / sum_mask\n                # torch.set_printoptions(threshold=10000)\n                # if True: #self.global_rank == 1:\n                #     tmp = ''\n                #     sss = 0\n                #     ccc = 0\n                #     for i in range(mask.shape[0]):\n                #         if mask[i] > 0:\n                #             tmp += str(idx.view(-1)[i].item()) + ','\n                #             sss += loss_raw.view(-1)[i].float().item()\n                #             ccc += 1\n                #     print('rank', self.global_rank, 'loss', loss.item(), 'lavg', sss / ccc)#, 'tmp', tmp, 'input', idx)\n        return L2Wrap.apply(loss, logits)\n    def training_step_end(self, batch_parts):\n        if pl.__version__[0]!='2':\n            all = self.all_gather(batch_parts)\n            if self.trainer.is_global_zero:\n                self.trainer.my_loss_all = all\n    def generate_init_weight(self):\n        print(\n            f\"\"\"\n############################################################################"
        },
        {
            "comment": "This code initializes the model's weights, handling specific layers and adjusting gains and scales accordingly. It also handles layer normalization (ln_x.weight), embedding scale initialization (-1 * lr_init), and certain zero-initialized layers.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":530-558",
            "content": "#\n# Init model weight (slow for large models)...\n#\n############################################################################\n\"\"\"\n        )\n        m = {}\n        for n in self.state_dict():\n            p = self.state_dict()[n]\n            shape = p.shape\n            gain = 1.0\n            scale = 1.0\n            if \"ln_\" in n or \".ln\" in n or \"time_\" in n or \"_mask\" in n or \"pos_emb\" in n or '.mask.' in n:\n                if 'ln_x.weight' in n:\n                    layer_scale = (1+int(n.split('.')[1])) / self.args.n_layer\n                    m[n] = (p * 0.0) + (layer_scale ** 0.7)\n                else:\n                    m[n] = p\n            else:\n                if n == \"emb.weight\":\n                    scale = -1 * self.args.lr_init\n                else:\n                    if shape[0] > shape[1]:\n                        gain = math.sqrt(shape[0] / shape[1])\n                    zero = [\".att.output.\", \".ffn.value.\", \".ffn.receptance.\", \".ffnPre.value.\", \".ffnPre.receptance.\", \"head_q.\", '.oo.', '.rr.']\n                    for kk in zero:"
        },
        {
            "comment": "This code initializes the model's weights with different scales based on their names. If the name contains \"head_k.\" or \"head_q.\", the scale is set to 0. If the name is \"head.weight\", the scale is set to 0.5. Otherwise, the scale is set to 0. The weights are initialized using PyTorch's initialization functions depending on their scales. Finally, the model parameters are moved to CPU and potentially converted to BF16 or FP16 if environment variables RWKV_FLOAT_MODE is set to \"bf16\" or \"fp16\".",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":559-585",
            "content": "                        if kk in n:\n                            scale = 0\n                    if n == \"head.weight\":\n                        scale = 0.5\n                    if \"head_k.\" in n:\n                        scale = 0.1\n                    if \"head_q.\" in n:\n                        scale = 0\n                print(f\"{str(shape[0]).ljust(5)} {str(shape[1]).ljust(5)} {str(scale).ljust(4)} {n}\")\n                if self.args.accelerator.upper() == \"GPU\":\n                    m[n] = torch.empty((shape[0], shape[1]), device=\"cuda\")\n                else:\n                    m[n] = torch.empty((shape[0], shape[1]))\n                if scale == 0:\n                    nn.init.zeros_(m[n])\n                elif scale < 0:\n                    nn.init.uniform_(m[n], a=scale, b=-scale)\n                else:\n                    nn.init.orthogonal_(m[n], gain=gain * scale)\n            m[n] = m[n].cpu()\n            if os.environ[\"RWKV_FLOAT_MODE\"] == \"fp16\":\n                m[n] = m[n].half()\n            elif os.environ[\"RWKV_FLOAT_MODE\"] == \"bf16\":"
        },
        {
            "comment": "In this code snippet, the model's parameters are converted to bfloat16 and then garbage collected, followed by clearing CUDA cache. This improves memory usage and efficiency.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/model.py\":586-593",
            "content": "                m[n] = m[n].bfloat16()\n            # if n == \"emb.weight\":\n            #     print(m[n])\n        gc.collect()\n        torch.cuda.empty_cache()\n        return m"
        }
    ]
}