{
    "summary": "This code initializes a custom PyTorch dataset, handling data loading and resizing for various scenarios. It manages token lists, WebDatasets, indices, and data selection for centralized or distributed training. The code selects random data based on stage, data type, and my_pile_version, converts indices to string tokens, checks input data patterns, generates random index if not found, converts to tensors, and handles different return values based on `args.my_qa_mask`.",
    "details": [
        {
            "comment": "This code is initializing a custom PyTorch dataset named MyDataset. It takes arguments and checks if the data type is \"binidx\". If so, it sets the vocabulary size, loads the dataset from a file using MMapIndexedDataset, and calculates the total number of tokens in the dataset. It also provides informational messages to the user about the vocabulary size and the total number of tokens in the dataset.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":0-24",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport json, math, random, os, sys\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom pytorch_lightning.utilities import rank_zero_info\nfrom .binidx import MMapIndexedDataset\nfrom .utils import MaybeIsPrime\nclass MyDataset(Dataset):\n    def __init__(self, args):\n        self.args = args\n        if args.data_type == \"binidx\":\n            self.vocab_size = args.vocab_size\n            rank_zero_info(f\"Current vocab size = {self.vocab_size} (make sure it's correct)\")\n            if args.my_pile_version == 1:\n                self.data = MMapIndexedDataset(args.data_file)\n                self.data_size = len(self.data._bin_buffer) // self.data._index._dtype_size\n                rank_zero_info(f\"Data has {self.data_size} tokens.\")"
        },
        {
            "comment": "This code block is checking if the `my_pile_version` argument is equal to 2. If so, it reads in the data file and prepares it for use. It splits the data into chunks and asserts that the size of each chunk matches the expected size. Finally, if the `my_qa_mask` argument is greater than zero, it assigns a specific dataset to `data_pile`.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":25-41",
            "content": "            elif args.my_pile_version == 2:\n                data_list = open(args.data_file, \"r\", encoding='utf-8').read().strip().split('\\n')\n                data_list = [i.strip().split(' ') for i in data_list]\n                self.data = []\n                self.data_size = int(data_list[-1][-1])\n                rank_zero_info(f\"Data has {self.data_size} chunks.\")\n                for d in data_list:\n                    data = MMapIndexedDataset(d[0])\n                    data_size = len(data._bin_buffer) // data._index._dtype_size\n                    assert (data_size - args.ctx_len) == int(d[1])\n                    self.data += [[int(d[-1]), int(d[1]), data]]\n                # rank_zero_info(self.data)\n            if args.my_qa_mask > 0:\n                # self.data_pile = MMapIndexedDataset('/fsx/pile/pile_20B_tokenizer_text_document')\n                self.data_pile = MMapIndexedDataset('/fsx/pile_deduped/pile_0.87_deduped_text_document')\n                self.data_pile_size = len(self.data_pile._bin_buffer) // self.data._index._dtype_size"
        },
        {
            "comment": "If the data is not tokenized, self.data_pile is set to None and self.data_pile_size is set to 0. If args.my_pile_stage is greater than 0, it checks if the dataset size is as expected (332115325534 tokens) and vocab size (50277). It calculates samples_per_epoch based on epoch_steps and real_bsz. Asserts that samples_per_epoch is 40320. Prints rank-zero info with stage number if args.my_pile_stage != 4. If args.data_type is \"numpy\", loads data from args.data_file, converts it to int, sets self.vocab_size, and prints current vocab size to ensure correctness.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":42-59",
            "content": "            else:\n                self.data_pile = None\n                self.data_pile_size = 0\n            if args.my_pile_stage > 0:\n                # assert self.data_size == 332115325534 and self.vocab_size == 50277\n                self.samples_per_epoch = args.epoch_steps * args.real_bsz\n                assert self.samples_per_epoch == 40320\n                rank_zero_info(f\"########## Pile 20b-tokenized stage {args.my_pile_stage} ##########\")\n                dataset_slot = self.data_size // args.ctx_len\n                if args.my_pile_stage != 4:\n                    assert MaybeIsPrime(args.magic_prime)\n                    assert args.magic_prime % 3 == 2\n                    assert args.magic_prime / dataset_slot > 0.99 and args.magic_prime / dataset_slot <= 1\n        elif args.data_type == \"numpy\":\n            self.data = np.load(args.data_file).astype(\"int\")\n            self.vocab_size = args.vocab_size\n            rank_zero_info(f\"Current vocab size = {self.vocab_size} (make sure it's correct)\")"
        },
        {
            "comment": "The code handles loading and resizing data based on the specified data type. If no data type is specified, it loads uint16 data from the file, resizes vocab size, calculates number of samples, and notifies rank 0. If data type is \"wds_img\", it sets vocab size and data size to -1, sets data to None, and error count to 0. If data type is \"dummy\", it creates dummy data by concatenating numbers and notifies rank 0.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":60-81",
            "content": "            self.data_size = len(self.data)\n            rank_zero_info(f\"Data has {self.data_size} tokens.\")\n        elif args.data_type == \"uint16\":\n            self.data = np.fromfile(args.data_file, dtype=np.uint16).astype(\"int32\").reshape(-1, args.my_sample_len)\n            self.vocab_size = args.vocab_size\n            rank_zero_info(f\"Current vocab size = {self.vocab_size} (make sure it's correct)\")\n            self.data_size = self.data.shape[0]\n            rank_zero_info(f\"Data has {self.data_size} samples.\")\n        elif args.data_type == \"wds_img\":\n            self.vocab_size = -1\n            self.data_size = -1\n            self.data = None\n            self.error_count = 0\n        else:\n            if args.data_type == \"dummy\":\n                rank_zero_info(\"Building dummy data...\")\n                self.data = \"\"\n                for i in range(100000):\n                    aa = (i) % 10000\n                    bb = (i * i) % 10000\n                    cc = aa + bb\n                    self.data += f\".{aa}+{bb}={cc}.\""
        },
        {
            "comment": "The code reads data from a file and builds a token list, storing it in a JSON file. It then creates dictionaries for mapping tokens to indices and indices to tokens. Finally, it provides methods for the length of the dataset and accessing specific items within the dataset.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":82-106",
            "content": "            else:\n                self.data = open(args.data_file, \"r\", encoding=args.data_type).read()\n            rank_zero_info(\"Building token list...\")\n            unique = sorted(list(set(self.data)))\n            self.vocab_size = len(unique)\n            # rank_zero_info()\n            # for u in unique:\n            #     print(u, end=' ')\n            # rank_zero_info('\\n\\n')\n            xx = 0\n            xxObj = {}\n            for u in unique:\n                xxObj[xx] = u\n                xx += 1\n            with open(f\"{args.proj_dir}/vocab.json\", \"w\", encoding=\"utf-8\") as vocab_file:\n                vocab_file.write(json.dumps(xxObj, ensure_ascii=False))\n            self.data_size = len(self.data)\n            rank_zero_info(f\"Data has {self.data_size} tokens, {self.vocab_size} vocab size.\")\n            self.stoi = {ch: i for i, ch in enumerate(unique)}\n            self.itos = {i: ch for i, ch in enumerate(unique)}\n    def __len__(self):\n        return self.args.epoch_steps * self.args.micro_bsz\n    def __getitem__(self, idx):"
        },
        {
            "comment": "This code initializes a WebDataset for image data with specified transformation. It shuffles the dataset and decodes it into torchrgb format, then maps the tuple of jpg, json, and txt files to image transformations, identity mappings for other file types, and returns the initialized dataset.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":107-127",
            "content": "        args = self.args\n        rank = self.global_rank\n        epoch = self.real_epoch\n        world_size = self.world_size\n        # print(f\"epoch {epoch} idx {idx} rank {rank}/{world_size}\")\n        if args.data_type == \"wds_img\":\n            def init_wds(self, bias=0):\n                def identity(x):\n                    return x            \n                import webdataset as wds\n                import torchvision.transforms as transforms\n                # img_transform = transforms.Compose(\n                #     [transforms.CenterCrop(256)]\n                # )\n                img_transform = transforms.Compose([\n                    transforms.CenterCrop(512),\n                    transforms.Resize((args.my_img_size))\n                ])\n                self.data_raw = wds.WebDataset(args.data_file, resampled=True).shuffle(10000, initial=1000, rng=random.Random(epoch*100000+rank+bias*1e9)).decode(\"torchrgb\").to_tuple(\"jpg\", \"json\", \"txt\").map_tuple(img_transform, identity, identity)\n                for pp in self.data_raw.pipeline:"
        },
        {
            "comment": "This code initializes a WebDataset for distributed training, handling potential errors in data loading and maintaining worker seeds for determinism. If the dataset is not initialized, it calls init_wds() to do so. It attempts to load data from the dataset 10 times, printing an error message if there's a failure, then re-initializes the WebDataset before retrying. The code also prints information about the current epoch, rank, and progress when loading data successfully. Additionally, it has the potential to append data samples to a file named \"sample_{rank}.txt\".",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":128-149",
            "content": "                    if 'Resampled' in str(pp):\n                        pp.deterministic = True\n                        def worker_seed():\n                            return rank*100000+epoch+bias*1e9\n                        pp.worker_seed = worker_seed\n                self.data = iter(self.data_raw)\n                # print(f\"WebDataset loaded for rank {rank} epoch {epoch}\")\n            if self.data == None:\n                init_wds(self)\n            trial = 0\n            while trial < 10:\n                try:\n                    dd = next(self.data) # jpg, json, txt\n                    break\n                except:\n                    print(f'[dataloader error - epoch {epoch} rank {rank} - trying a new shuffle]')\n                    self.error_count += 1\n                    init_wds(self, self.error_count)\n                    trial += 1\n                    pass\n            # print(f\"epoch {epoch} idx {idx} rank {rank}/{world_size} {dd[2]}\")\n            # with open(f\"sample_{rank}.txt\", \"a\", encoding=\"utf-8\") as tmp:"
        },
        {
            "comment": "The code initializes a random index 'i' within the data range, creates two tensors 'x' and 'y' with torch.tensor() from the data slice. If the data type is uint16, it selects a random index 'i' from 0 to data_size-1, extracts x from the data at index i, y from the next element. Else, it calculates the context length, required length, and uses magic prime for data selection based on current epoch, idx, rank, world_size. If my_pile_stage is greater than 0, it computes the index 'ii', checks if my_qa_mask > 0 to determine if ii should be -1 or half of itself. Depending on data type and certain conditions, it selects data_pile for data tensor.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":150-174",
            "content": "            #     tmp.write(f\"epoch {epoch} idx {idx} rank {rank}/{world_size} {int(dd[1]['key'])}\\n\")\n            return dd[0], dd[2]\n        else:\n            if args.data_type == \"uint16\":\n                i = np.random.randint(0, self.data_size-1)\n                dix = self.data[i]\n                x = torch.tensor(dix[:-1], dtype=torch.long)\n                y = torch.tensor(dix[1:], dtype=torch.long)\n            else:\n                ctx_len = args.ctx_len\n                req_len = ctx_len + 1\n                magic_prime = args.magic_prime\n                data = self.data\n                if args.my_pile_stage > 0:\n                    ii = 1 + epoch * self.samples_per_epoch + (idx * world_size) + rank\n                    if args.my_qa_mask > 0:\n                        ii_orig = ii\n                        if ii % 2 == 0:\n                            ii = -1\n                            data = self.data_pile\n                        else:\n                            ii = ii // 2\n                    if data == self.data_pile:"
        },
        {
            "comment": "This code randomly selects a chunk of data for model training. If the stage is 4 or random steps are involved, it either picks a random spot in the dataset based on the current version, or uses a formula involving magic prime and pile shift to determine the position. It then adds the context length. Finally, if the input type is \"cheat\", it picks a random spot in the dataset. The code also prints some information about epoch, index, rank, world size, iterations, and selected position.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":175-191",
            "content": "                        i = np.random.randint(0, self.data_pile_size - req_len)\n                    else:\n                        if args.my_pile_stage == 4 or ii < args.my_random_steps:\n                            # cheat: pick a random spot in dataset\n                            if args.my_pile_version == 1:\n                                i = np.random.randint(0, self.data_size - req_len)\n                            else:\n                                i = np.random.randint(0, self.data_size)\n                        else:\n                            ii = ii - args.my_random_steps\n                            factor = (math.sqrt(5) - 1) / 2\n                            factor = int(magic_prime * factor)\n                            i = ((factor * ii * ii * ii) % magic_prime) * ctx_len\n                            i = i + args.my_pile_shift\n                    # print(f\"epoch {epoch} idx {idx} rank {rank}/{world_size} ii {ii} pos {round(i / self.data_size, 3)}\")\n                else:\n                    # cheat: pick a random spot in dataset"
        },
        {
            "comment": "The code retrieves a random index (i) within the data size and based on the data type, it selects the appropriate indices (dix) from the provided dataset. If data_type is \"binidx\", it checks if my_pile_version is 1 or not; then it gets the indices using different methods. If data_type is \"numpy\", it directly selects the indices using numpy's slicing. Finally, it converts string tokens to indices using self.stoi for non-\"binidx\" and \"numpy\" data types.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":192-212",
            "content": "                    i = np.random.randint(0, self.data_size - req_len)\n                if args.data_type == \"binidx\":\n                    if args.my_pile_version == 1:\n                        dix = data.get(idx=0, offset=i, length=req_len).astype(int)\n                    else:\n                        # self.data : cutoff, chunk_count, data\n                        for j in range(len(data)):\n                            if i < data[j][0]:\n                                ii = i\n                                i = (i - (data[j-1][0] if j > 0 else 0)) % data[j][1]\n                                dix = data[j][2].get(idx=0, offset=i, length=req_len).astype(int)\n                                # print(ii, j, i)\n                                break\n                elif args.data_type == \"numpy\":\n                    dix = data[i : i + req_len]\n                else:\n                    dix = [self.stoi[s] for s in data[i : i + req_len]]\n                if args.my_qa_mask == 1:\n                    if data == self.data_pile:"
        },
        {
            "comment": "This code checks if the input data has a specific pattern, and sets corresponding values in the 'z' list. If no such pattern is found, it generates a random index and retrieves the data from the dataset. It then converts the data into tensors for further processing.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":213-233",
            "content": "                        z = [1] * ctx_len\n                    else:\n                        z = [0] * ctx_len\n                        z_sum = 0\n                        isGood = False\n                        for i in range(3, ctx_len):\n                            if dix[i] == 27 and dix[i-1] == 34 and dix[i-2] == 187 and dix[i-3] == 187:\n                                isGood = True\n                            if dix[i] == 0:\n                                isGood = False\n                            if isGood:\n                                z[i] = 1\n                                z_sum += 1\n                        if z_sum == 0:\n                            z = [1] * ctx_len\n                            i = np.random.randint(0, self.data_pile_size - req_len)\n                            dix = self.data_pile.get(idx=0, offset=i, length=req_len).astype(int)\n                    z = torch.tensor(z, dtype=torch.bfloat16)\n                x = torch.tensor(dix[:-1], dtype=torch.long)\n                y = torch.tensor(dix[1:], dtype=torch.long)"
        },
        {
            "comment": "This code snippet appears to be part of a larger function. It checks if `ii_orig` is less than 50, and if it is, it prints some information related to rank, indices, and a portion of the array `x`. If `args.my_qa_mask` is equal to 1, it returns `x`, `y`, and `z`, otherwise it simply returns `x` and `y`. The purpose and functionality of this code may vary depending on the larger context in which it exists.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/dataset.py\":235-244",
            "content": "                # if ii_orig < 50:\n                #     # if rank == 1:\n                #     print('rank', rank, 'i', ii_orig, ii, i, 'x', x[:5], '...', x[-5:])\n                # else:\n                #     exit(0)\n                if args.my_qa_mask == 1:\n                    return x, y, z\n            return x, y"
        }
    ]
}