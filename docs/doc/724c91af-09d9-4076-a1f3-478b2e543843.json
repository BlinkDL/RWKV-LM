{
    "summary": "The code utilizes DeepSpeed, VGG16 and RWKV-LM layers, VGG-19 pretrained features, L2 pooling layers, calculates distances for score calculation, and defines the RWKV-v4neo language model with Conv2d layers, BatchNorm2d, Mish activation function, LightningModule, optimizer, and encoder-decoder architecture for multi-device training support.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines functions and modules for a machine learning model. It uses DeepSpeed for efficient training and includes functions like L2pooling for data processing.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":0-29",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport numpy as np\nimport os, math, gc\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision as vision\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities import rank_zero_info, rank_zero_only\nfrom pytorch_lightning.strategies import DeepSpeedStrategy\nimport deepspeed\nfrom deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n# from pytorch_msssim import MS_SSIM\ndef __nop(ob):\n    return ob\nMyModule = torch.jit.ScriptModule\n# MyFunction = __nop\nMyFunction = torch.jit.script_method\nimport clip\nfrom transformers import CLIPModel\nclass L2pooling(nn.Module):\n    def __init__(self, filter_size=5, stride=2, channels=None, pad_off=0):\n        super(L2pooling, self).__init__()\n        self.padding = (filter_size - 2) // 2"
        },
        {
            "comment": "This code defines a class for a neural network model. The model has stages, and each stage contains convolutional layers from the VGG16 model followed by a custom layer called RWKV-LM/RWKV-v4neo/src/model_img.py:30-62. The custom layer applies a Hanning window function to the filter, then normalizes it, and repeats it for each channel. Finally, it performs convolution with the input image using specified stride, padding, and groups.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":30-62",
            "content": "        self.stride = stride\n        self.channels = channels\n        a = np.hanning(filter_size)[1:-1]\n        g = torch.Tensor(a[:, None] * a[None, :])\n        g = g / torch.sum(g)\n        self.register_buffer(\n            \"filter\", g[None, None, :, :].repeat((self.channels, 1, 1, 1))\n        )\n    def forward(self, input):\n        input = input**2\n        out = F.conv2d(\n            input,\n            self.filter,\n            stride=self.stride,\n            padding=self.padding,\n            groups=input.shape[1],\n        )\n        return (out + 1e-12).sqrt()\nclass DISTS(torch.nn.Module):\n    def __init__(self, load_weights=True):\n        super(DISTS, self).__init__()\n        vgg_pretrained_features = vision.models.vgg16(\n            weights=\"VGG16_Weights.IMAGENET1K_V1\"\n        ).features\n        self.stage1 = torch.nn.Sequential()\n        self.stage2 = torch.nn.Sequential()\n        self.stage3 = torch.nn.Sequential()\n        self.stage4 = torch.nn.Sequential()\n        self.stage5 = torch.nn.Sequential()\n        for x in range(0, 4):"
        },
        {
            "comment": "The code creates a model architecture by adding modules to the stages of the network. It uses VGG-19 pretrained features for each stage and adds L2 pooling layers in between. The mean and std values are registered as buffers, and a list of channel numbers is created.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":63-85",
            "content": "            self.stage1.add_module(str(x), vgg_pretrained_features[x])\n        self.stage2.add_module(str(4), L2pooling(channels=64))\n        for x in range(5, 9):\n            self.stage2.add_module(str(x), vgg_pretrained_features[x])\n        self.stage3.add_module(str(9), L2pooling(channels=128))\n        for x in range(10, 16):\n            self.stage3.add_module(str(x), vgg_pretrained_features[x])\n        self.stage4.add_module(str(16), L2pooling(channels=256))\n        for x in range(17, 23):\n            self.stage4.add_module(str(x), vgg_pretrained_features[x])\n        self.stage5.add_module(str(23), L2pooling(channels=512))\n        for x in range(24, 30):\n            self.stage5.add_module(str(x), vgg_pretrained_features[x])\n        self.register_buffer(\n            \"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, -1, 1, 1)\n        )\n        self.register_buffer(\n            \"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, -1, 1, 1)\n        )\n        self.chns = [3, 64, 128, 256, 512, 512]\n        self.register_buffer("
        },
        {
            "comment": "The code initializes two parameters, \"alpha\" and \"beta\", with random values and normalizes their data. It then loads weights from a file and assigns them to the respective parameters. Finally, it sets the gradient flag to False for all parameters and defines forward functions to perform calculations on input features.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":86-115",
            "content": "            \"alpha\", nn.Parameter(torch.randn(1, sum(self.chns), 1, 1))\n        )\n        self.register_buffer(\"beta\", nn.Parameter(torch.randn(1, sum(self.chns), 1, 1)))\n        self.alpha.data.normal_(0.1, 0.01)\n        self.beta.data.normal_(0.1, 0.01)\n        weights = torch.load(\"test/DISTS_weights.pt\")\n        self.alpha.data = weights[\"alpha\"]\n        self.beta.data = weights[\"beta\"]\n        for param in self.parameters():\n            param.requires_grad = False\n    def forward_once(self, x):\n        h = (x - self.mean) / self.std\n        h = self.stage1(h)\n        h_relu1_2 = h\n        h = self.stage2(h)\n        h_relu2_2 = h\n        h = self.stage3(h)\n        h_relu3_3 = h\n        h = self.stage4(h)\n        h_relu4_3 = h\n        h = self.stage5(h)\n        h_relu5_3 = h\n        return [x, h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3]\n    def forward(self, x, y, require_grad=False, batch_average=False):\n        if require_grad:\n            feats0 = self.forward_once(x)\n            feats1 = self.forward_once(y)"
        },
        {
            "comment": "This code calculates the distances between two feature embeddings and assigns weights to them based on alpha and beta. It first performs a forward pass for x and y, then normalizes alpha and beta by dividing their sums with the total number of channels. For each channel, it computes the mean and variance of x and y, and also calculates the covariance between x and y. Finally, it applies weights to the distances and sums them up for both embeddings.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":116-140",
            "content": "        else:\n            with torch.no_grad():\n                feats0 = self.forward_once(x)\n                feats1 = self.forward_once(y)\n        dist1 = 0\n        dist2 = 0\n        c1 = 1e-6\n        c2 = 1e-6\n        w_sum = self.alpha.sum() + self.beta.sum()\n        alpha = torch.split(self.alpha / w_sum, self.chns, dim=1)\n        beta = torch.split(self.beta / w_sum, self.chns, dim=1)\n        for k in range(len(self.chns)):\n            x_mean = feats0[k].mean([2, 3], keepdim=True)\n            y_mean = feats1[k].mean([2, 3], keepdim=True)\n            S1 = (2 * x_mean * y_mean + c1) / (x_mean**2 + y_mean**2 + c1)\n            dist1 = dist1 + (alpha[k] * S1).sum(1, keepdim=True)\n            x_var = ((feats0[k] - x_mean) ** 2).mean([2, 3], keepdim=True)\n            y_var = ((feats1[k] - y_mean) ** 2).mean([2, 3], keepdim=True)\n            xy_cov = (feats0[k] * feats1[k]).mean(\n                [2, 3], keepdim=True\n            ) - x_mean * y_mean\n            S2 = (2 * xy_cov + c2) / (x_var + y_var + c2)\n            dist2 = dist2 + (beta[k] * S2).sum(1, keepdim=True)"
        },
        {
            "comment": "This code contains a function that calculates a score based on distances and returns it. If batch_average is True, the score is averaged across all elements in the batch. The ToBinary class performs binary rounding of input values. The R_ENCODER class initializes a model with BatchNorm2d and Conv2d layers for image processing.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":142-172",
            "content": "        score = 1 - (dist1 + dist2).squeeze()\n        if batch_average:\n            return score.mean()\n        else:\n            return score\n    class ToBinary(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, x):#, noise_scale):\n            # if noise_scale > 0:\n            #     noise_min = 0.5 - noise_scale / 2\n            #     noise_max = 0.5 + noise_scale / 2\n            #     return torch.floor(x + torch.empty_like(x).uniform_(noise_min, noise_max))\n            # else:\n            return torch.floor(x + 0.5) # no need for noise when we have plenty of data\n        @staticmethod\n        def backward(ctx, grad_output):\n            return grad_output.clone()#, None\n########################################################################################################\nclass R_ENCODER(MyModule):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        dd = 8\n        self.Bxx = nn.BatchNorm2d(dd*64)\n        self.CIN = nn.Conv2d(3, dd, kernel_size=3, padding=1)"
        },
        {
            "comment": "This code defines multiple convolutional layers (Conv2d) and batch normalization layers (BatchNorm2d) for a neural network model. The layers have different input/output dimensions, kernel sizes, and padding values to perform feature extraction and normalization in the model's architecture.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":173-192",
            "content": "        self.Cx0 = nn.Conv2d(dd, 32, kernel_size=3, padding=1)\n        self.Cx1 = nn.Conv2d(32, dd, kernel_size=3, padding=1)\n        self.B00 = nn.BatchNorm2d(dd*4)\n        self.C00 = nn.Conv2d(dd*4, 256, kernel_size=3, padding=1)\n        self.C01 = nn.Conv2d(256, dd*4, kernel_size=3, padding=1)\n        self.C02 = nn.Conv2d(dd*4, 256, kernel_size=3, padding=1)\n        self.C03 = nn.Conv2d(256, dd*4, kernel_size=3, padding=1)\n        self.B10 = nn.BatchNorm2d(dd*16)\n        self.C10 = nn.Conv2d(dd*16, 256, kernel_size=3, padding=1)\n        self.C11 = nn.Conv2d(256, dd*16, kernel_size=3, padding=1)\n        self.C12 = nn.Conv2d(dd*16, 256, kernel_size=3, padding=1)\n        self.C13 = nn.Conv2d(256, dd*16, kernel_size=3, padding=1)\n        self.B20 = nn.BatchNorm2d(dd*64)\n        self.C20 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        self.C21 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)\n        self.C22 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        self.C23 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)"
        },
        {
            "comment": "This code defines a forward function for a neural network. It uses Mish activation functions and applies convolutional layers with batch normalization for feature extraction and image processing. The output is generated by combining the outputs of multiple convolutional layers, and pixel unshuffling is used to change the channel dimension.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":193-220",
            "content": "        # self.B21 = nn.BatchNorm2d(dd*64)\n        # self.C24 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        # self.C25 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)\n        # self.C26 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        # self.C27 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)\n        self.COUT = nn.Conv2d(dd*64, args.my_img_bit, kernel_size=3, padding=1)\n    @MyFunction\n    def forward(self, img):\n        ACT = F.mish\n        x = self.CIN(img)\n        xx = self.Bxx(F.pixel_unshuffle(x, 8))\n        x = x + self.Cx1(ACT(self.Cx0(x)))\n        x = F.pixel_unshuffle(x, 2)\n        x = x + self.C01(ACT(self.C00(ACT(self.B00(x)))))\n        x = x + self.C03(ACT(self.C02(x)))\n        x = F.pixel_unshuffle(x, 2)\n        x = x + self.C11(ACT(self.C10(ACT(self.B10(x)))))\n        x = x + self.C13(ACT(self.C12(x)))\n        x = F.pixel_unshuffle(x, 2)\n        x = x + self.C21(ACT(self.C20(ACT(self.B20(x)))))\n        x = x + self.C23(ACT(self.C22(x)))\n        # x = x + self.C25(ACT(self.C24(ACT(self.B21(x)))))"
        },
        {
            "comment": "The given code is a part of the RWKV model implementation. It defines the Decoder class which takes in arguments and initializes multiple Conv2d layers for processing. The Conv2d layers are responsible for feature extraction and down-sampling. The batch normalization layer (BatchNorm2d) helps with speeding up the training and improving model performance by reducing internal covariate shift. However, there is a comment suggesting that the BatchNorm2d could be removed or reduced to improve performance and speed. Additionally, some of the Conv2d layers are commented out, indicating they might be unused or under development.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":221-244",
            "content": "        # x = x + self.C27(ACT(self.C26(x)))\n        x = self.COUT(x + xx)\n        return torch.sigmoid(x)\n########################################################################################################\nclass R_DECODER(MyModule):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        dd = 8\n        self.CIN = nn.Conv2d(args.my_img_bit, dd*64, kernel_size=3, padding=1)\n        self.B00 = nn.BatchNorm2d(dd*64)\n        self.C00 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        self.C01 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)\n        self.C02 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        self.C03 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)\n        # self.B01 = nn.BatchNorm2d(dd*64)\n        # self.C04 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        # self.C05 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)\n        # self.C06 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        # self.C07 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)"
        },
        {
            "comment": "This code defines a model for the RWKV-v4neo language model. It includes multiple convolutional and batch normalization layers, as well as the Mish activation function (F.mish). The forward function applies these layers to an input code and performs addition operations between different layer outputs.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":246-269",
            "content": "        self.B10 = nn.BatchNorm2d(dd*16)\n        self.C10 = nn.Conv2d(dd*16, 256, kernel_size=3, padding=1)\n        self.C11 = nn.Conv2d(256, dd*16, kernel_size=3, padding=1)\n        self.C12 = nn.Conv2d(dd*16, 256, kernel_size=3, padding=1)\n        self.C13 = nn.Conv2d(256, dd*16, kernel_size=3, padding=1)\n        self.B20 = nn.BatchNorm2d(dd*4)\n        self.C20 = nn.Conv2d(dd*4, 256, kernel_size=3, padding=1)\n        self.C21 = nn.Conv2d(256, dd*4, kernel_size=3, padding=1)\n        self.C22 = nn.Conv2d(dd*4, 256, kernel_size=3, padding=1)\n        self.C23 = nn.Conv2d(256, dd*4, kernel_size=3, padding=1)\n        self.Cx0 = nn.Conv2d(dd, 32, kernel_size=3, padding=1)\n        self.Cx1 = nn.Conv2d(32, dd, kernel_size=3, padding=1)\n        self.COUT = nn.Conv2d(dd, 3, kernel_size=3, padding=1)\n    @MyFunction\n    def forward(self, code):\n        ACT = F.mish\n        x = self.CIN(code)\n        x = x + self.C01(ACT(self.C00(ACT(self.B00(x)))))\n        x = x + self.C03(ACT(self.C02(x)))\n        # x = x + self.C05(ACT(self.C04(ACT(self.B01(x)))))"
        },
        {
            "comment": "This code snippet belongs to a LightningModule class in the RWKV-v4neo package. It includes a cosine_loss function and an RWKV_IMG class which has an encoder, decoder, and clip_model as its components. The encoder and decoder are instances of R_ENCODER and R_DECODER classes respectively. The code snippet defines operations to be performed on the input x using various transformations and normalizations before returning a sigmoid transformed output.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":270-305",
            "content": "        # x = x + self.C07(ACT(self.C06(x)))\n        x = F.pixel_shuffle(x, 2)\n        x = x + self.C11(ACT(self.C10(ACT(self.B10(x)))))\n        x = x + self.C13(ACT(self.C12(x)))\n        x = F.pixel_shuffle(x, 2)\n        x = x + self.C21(ACT(self.C20(ACT(self.B20(x)))))\n        x = x + self.C23(ACT(self.C22(x)))\n        x = F.pixel_shuffle(x, 2)\n        x = x + self.Cx1(ACT(self.Cx0(x)))\n        x = self.COUT(x)\n        return torch.sigmoid(x)\n########################################################################################################`\ndef cosine_loss(x, y):\n    x = F.normalize(x, dim=-1)\n    y = F.normalize(y, dim=-1)\n    return 1 - torch.einsum('ij,ij->i',[x,y])\nclass RWKV_IMG(pl.LightningModule):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.encoder = R_ENCODER(args)\n        self.decoder = R_DECODER(args)\n        self.clip_model = None\n        clip_name = args.my_img_clip\n        if clip_name == 'B32':\n            clip_name = 'ViT-B/32'\n        elif clip_name == 'B16':"
        },
        {
            "comment": "In the provided code snippet, the `clip_name` is assigned based on certain conditions. If it's 'ViT-B/16', no change. If 'L14', it's changed to 'ViT-L/14'. And if 'OB32', it becomes \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\" and a pretrained CLIP model is loaded for this case. The code also initializes `clip_mean` and `clip_std` buffers with specific values, sets parameters of 'clip_model' as non-trainable, and configures the optimizers based on 'args'.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":306-331",
            "content": "            clip_name = 'ViT-B/16'\n        elif clip_name == 'L14':\n            clip_name = 'ViT-L/14'\n        elif clip_name == 'OB32':\n            clip_name = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\n            self.clip_model = CLIPModel.from_pretrained(clip_name)\n            self.clip_model.encode_image = self.clip_model.get_image_features\n        if self.clip_model == None:\n            self.clip_model, _ = clip.load(clip_name, jit = True)\n        self.register_buffer(\n            \"clip_mean\", torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(1, 3, 1, 1)\n        )\n        self.register_buffer(\n            \"clip_std\", torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(1, 3, 1, 1)\n        )\n        for n, p in self.named_parameters():\n            if 'clip_model' in n:\n                p.requires_grad = False\n        self.loss_dists = DISTS()\n        # self.loss_ssim = MS_SSIM(data_range=1, size_average=True, channel=3)\n    def configure_optimizers(self):\n        args = self.args\n        optim_groups = ["
        },
        {
            "comment": "This code defines an optimizer function that chooses between DeepSpeedCPUAdam and FusedAdam based on the deepspeed_offload flag. The optimizer takes in optim_groups, lr (learning rate), betas, eps (epsilon), bias_correction, adam_w_mode, weight_decay, and amsgrad as parameters. It returns an instance of either DeepSpeedCPUAdam or FusedAdam depending on whether deepspeed_offload is True or False. The @property method deepspeed_offload retrieves the strategy from the trainer.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":332-359",
            "content": "            {\"params\": [p for n, p in self.named_parameters()], \"weight_decay\": 0.0},\n        ]\n        if self.deepspeed_offload:\n            return DeepSpeedCPUAdam(\n                optim_groups,\n                lr=self.args.lr_init,\n                betas=self.args.betas,\n                eps=self.args.adam_eps,\n                bias_correction=True,\n                adamw_mode=False,\n                weight_decay=0,\n                amsgrad=False,\n            )\n        return FusedAdam(\n            optim_groups,\n            lr=self.args.lr_init,\n            betas=self.args.betas,\n            eps=self.args.adam_eps,\n            bias_correction=True,\n            adam_w_mode=False,\n            weight_decay=0,\n            amsgrad=False,\n        )\n        # return ZeroOneAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, weight_decay=0, amsgrad=False, cuda_aware=False)\n    @property\n    def deepspeed_offload(self) -> bool:\n        strategy = self.trainer.strategy"
        },
        {
            "comment": "This code defines a model class that takes an image as input and outputs an image. It uses an encoder and decoder for processing the input. The model also has a training step where it saves images at specific global steps during training if running on multiple devices.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":360-384",
            "content": "        if isinstance(strategy, DeepSpeedStrategy):\n            config = strategy.config[\"zero_optimization\"]\n            return config.get(\"offload_optimizer\") or config.get(\"offload_param\")\n        return False\n    def forward(self, img):\n        z = self.encoder(img)\n        z = ToBinary.apply(z)#, self.args.my_img_noise_scale)\n        out = self.decoder(z)\n        return out\n    def training_step(self, batch, batch_idx):\n        args = self.args\n        img, txt = batch\n        out = self(img)\n        if self.trainer.is_global_zero:\n            if (self.trainer.global_step + 1) % (100 * int(args.devices)) == 0:\n                img_dir = f\"test/image_model/{args.run_name}\"\n                if not os.path.exists(img_dir):\n                    os.makedirs(img_dir)\n                vision.utils.save_image(\n                    img[:4], f\"{img_dir}/{self.trainer.global_step}-src.jpg\"#, padding=0\n                )\n                vision.utils.save_image(\n                    out[:4], f\"{img_dir}/{self.trainer.global_step}-out.jpg\"#, padding=0"
        },
        {
            "comment": "This code snippet is for a deep learning model that takes input image and produces output, then calculates loss for each step during training. It uses different types of losses such as SSIM (line 396) and cosine similarity (lines 390-395). The code also handles global average pooling and all-gathering for distributed training.\n\nQuestion: What is the purpose of the 'args' argument used in this code?",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":385-412",
            "content": "                )\n        # loss_ssim = 1 - self.loss_ssim(out, img)\n        loss_dists = self.loss_dists(out, img, require_grad=True, batch_average=True)\n        iii = self.clip_model.encode_image((img - self.clip_mean) / self.clip_std)\n        ooo = self.clip_model.encode_image((out - self.clip_mean) / self.clip_std)\n        loss_clip = torch.mean(cosine_loss(iii, ooo))\n        if args.my_img_l1_scale > 0:\n            loss_l1 = F.l1_loss(out, img)\n            return loss_dists + loss_clip * args.my_img_clip_scale + loss_l1 * args.my_img_l1_scale\n        else:\n            return loss_dists + loss_clip * args.my_img_clip_scale\n    def training_step_end(self, batch_parts):\n        all = self.all_gather(batch_parts)\n        if self.trainer.is_global_zero:\n            self.trainer.my_loss_all = all\n    def generate_init_weight(self):\n        print(\n            f\"\"\"\n############################################################################\n#\n# Init model weight (slow for large models)...\n#\n############################################################################"
        },
        {
            "comment": "This code snippet is used to create a dictionary of model parameters, where it loads each parameter from the model's state_dict and applies necessary scaling or conversion based on its name. It also handles different float modes such as fp16 and bf16 as specified in the environment variable RWKV_FLOAT_MODE. Finally, it collects garbage, empties CUDA cache, and returns the dictionary of parameters.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model_img.py\":413-445",
            "content": "\"\"\"\n        )\n        m = {}\n        for n in self.state_dict():\n            scale = 1\n            p = self.state_dict()[n]\n            shape = p.shape\n            ss = n.split('.')\n            # if ss[0] in ['encoder', 'decoder']:\n            #     if ss[2] == 'bias':\n            #         scale = 0\n            #     # elif n == 'encoder.CIN.weight':\n            #     #     nn.init.dirac_(p)\n            #     else:\n            #         try:\n            #             if ss[1][0] == 'C' and (int(ss[1][2]) % 2 == 1):\n            #                 scale = 0\n            #         except:\n            #             pass\n            # m[n] = p * scale\n            m[n] = p\n            m[n] = m[n].cpu()\n            if os.environ[\"RWKV_FLOAT_MODE\"] == \"fp16\":\n                m[n] = m[n].half()\n            elif os.environ[\"RWKV_FLOAT_MODE\"] == \"bf16\":\n                m[n] = m[n].bfloat16()\n        gc.collect()\n        torch.cuda.empty_cache()\n        return m"
        }
    ]
}