{
    "summary": "The code initializes the RWKV Language Model, sets training parameters and datafile details, configures efficient logging and numpy printing options, sets hyperparameters for RWKV-v1 model, trains a RWKV-LM model, prepares necessary data structures, initializes GPT model and trainer, loads state if available or provides setup details, and saves trained model.",
    "details": [
        {
            "comment": "This code snippet sets the seed for random numbers, configures numpy printing options, and initializes logging. It also defines the model type as 'RWKV' which stands for RWKV Language Model - a fast, VRAM-friendly model with good performance. The other types mentioned are MHA_rotary, MHA_shift, and MHA_pro, but RWKV is chosen for its specific characteristics.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/train.py\":0-20",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport os, sys, time, math, random, json, datetime, logging\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom src.trainer import Trainer, TrainerConfig\nfrom src.model import GPT, GPTConfig\nfrom src.utils import set_seed\nset_seed(42)\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)\nlogging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO,)\n# RWKV       : our new model - fastest when ctx_len is long - VRAM friendly - good performance\n# MHA_rotary : usual MultiheadAttention+Rotary+GeGLU - not as good\n# MHA_shift  : with time-shift - good performance\n# MHA_pro    : slow (lots of tricks) - VRAM hungry - very good performance\nmodel_type = 'RWKV'"
        },
        {
            "comment": "This code snippet defines the datafile, datafile_encoding, and datafile_type for the RWKV-LM/RWKV-v1 model's training process. It also sets important parameters like epoch_save_frequency, epoch_save_path, batch_size, and their values. These settings influence how the model is trained and saved during the training process.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/train.py\":22-42",
            "content": "# datafile = u\"V:\\\\NLP\\\\text8\"\n# datafile = u\"V:\\\\NLP\\\\enwik8\"\ndatafile = u\"V:\\\\NLP\\\\simplebooks\\\\simplebooks-92-raw\\\\train.txt\"\ndatafile_encoding = 'utf-8'\n# datafile = u\"D:\\\\NLP-Data\\\\ww100M.txt\"\n# datafile = u\"D:\\\\NLP-Data\\\\__2019.txt\"\n# datafile = u\"Y:\\\\BlinkNLP\\\\_txt_\\\\txt\\\\_all.txt\"\n# datafile = u\"V:\\\\NLP\\\\enwik8-shift-300.bpe\"\n# datafile_encoding = 'utf-16'\n# datafile = u\"V:\\\\NLP\\\\simplebooks-shift-utf32.word\"\n# datafile_encoding = 'utf-32'\ndatafile_type = 0 # use 0 for char-level english. use 1 for chinese. only affects some RWKV hyperparametrs \n#################################### VERY IMPORTANT ####################################\nepoch_save_frequency = 10                            # 0 = never, 1 = every 'epoch', 2 = every two 'epoch', etc.\nepoch_save_path = 'trained-'\nbatch_size = 32                                      # if you see \"CUDA out of memory\", reduce this.\n                                                     # if you have good GPU, increase this.\n                                                     # use GPU-Z to find the highest value for your VRAM."
        },
        {
            "comment": "This code snippet sets various hyperparameters for the RWKV-v1 model. It defines the number of epochs, model type, and specific layer and head configurations. The context length is adjustable, and there are special hyperparameters for RWKV models such as embedding scale.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/train.py\":44-67",
            "content": "n_epoch = 100                                        # the 'epoch' here is actually very short (and of fixed length)\n########################################################################################\nmodel_level = 'character' # 'character' (recommended) or 'word'\nctx_len = 256 # context length, try 512 or 1024 if you have good GPU\nn_layer = 6   # try 12 for 100M, 24 for 300M\nn_head = 8    # try 12 for 100M, 16 for 300M\nn_embd = n_head * 64\nn_attn = n_embd\nn_ffn = n_embd\nlr_init = 6e-4 if model_type == 'RWKV' else 4e-4    # RWKV can use higher lr.  8e-4 = 0.0008   4e-4 = 0.0004\nlr_final = 4e-5\nbetas = (0.9, 0.99) if model_type == 'RWKV' else (0.9, 0.99)\neps = 4e-9\nweight_decay = 0 if model_type == 'RWKV' else 0.01  # wd is not useful when we have enough data\nepoch_length_fixed = 10000                          # make an 'epoch' very short, so we can see the training progress\n######## special hyperparameters for RWKV model ########\nrwkv_emb_scale = 0.4                                # scale of initial embedding. 0.4 is a good choice"
        },
        {
            "comment": "This code snippet initializes the RWKV-LM/RWKV-v1's train.py file and includes several parameters for model configuration and data processing. It sets various attention dimensions, side projection sizes, and handles different types of input data. The code also creates a Dataset class to handle the loading and tokenization of the given datafile. Overall, it prepares the necessary components for training the RWKV language model.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/train.py\":68-86",
            "content": "rwkv_tiny_attn = 0#64 if (datafile_type == 0 and ctx_len > 600) else 0 # extra tiny attention dim, useful for long ctx char-level english\nrwkv_tiny_head = 1                                  # 1 is good enough. 8 is slow\n# n_side_proj = 512                                 # extra 'side projection', quite useful for BPE models \n########################################################################################################\n# Load data\n########################################################################################################\nprint('loading data... ' + datafile)\nclass Dataset(Dataset):\n    def __init__(self, data, model_level, ctx_len):\n        print('building token list...', end=' ')\n        if model_level == 'word':\n            import re\n            data = re.sub(r'(\\n|\\.|\\,|\\?|\\!|\\:|\\;|\\-|\\\u2014|\\||\\'|\\\"|\\`|\\(|\\)|[0-9]|\\[|\\]|\\{|\\}|\\=|\\+|\\*|\\\\|\\/|\\~|\\&|\\$|\\#|\\%)', r' \\g<0> ', data)\n            data = re.sub(' +',' ',data)\n            print('splitting token...')\n            data = data.lower().split(' ')"
        },
        {
            "comment": "The code trains the RWKV-LM model and writes the vocabulary to a JSON file named 'vocab.json'. It then defines two dictionaries, `stoi` and `itos`, which map characters to their unique indices and vice versa. The function `__len__` returns the epoch length and `__getitem__` retrieves a chunk of data with context length from the dataset given an index.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/train.py\":87-116",
            "content": "        unique = sorted(list(set(data)))\n        # print()\n        # for u in unique:\n        #     print(u, end=' ')\n        # print('\\n\\n')\n        xx = 0\n        xxObj = {}\n        for u in unique:\n            xxObj[xx] = u\n            xx += 1\n        with open('vocab.json', \"w\", encoding=\"utf-16\") as vocab_file:\n            vocab_file.write(json.dumps(xxObj, ensure_ascii=False))\n        data_size, vocab_size = len(data), len(unique)\n        print('data has %d %ss, %d unique.' % (data_size, model_level, vocab_size))\n        self.stoi = { ch:i for i,ch in enumerate(unique) }\n        self.itos = { i:ch for i,ch in enumerate(unique) }\n        self.ctx_len = ctx_len\n        self.vocab_size = vocab_size\n        self.data = data\n    def __len__(self):\n        return epoch_length_fixed\n    def __getitem__(self, idx):\n        i = np.random.randint(0, len(self.data) - (self.ctx_len + 1)) # cheat: pick a random spot in dataset\n        chunk = self.data[i:i+self.ctx_len+1]\n        dix = [self.stoi[s] for s in chunk]\n        x = torch.tensor(dix[:-1], dtype=torch.long)"
        },
        {
            "comment": "This code snippet initializes a GPT model with the specified configuration and train dataset. It then loads a pre-trained model's state dictionary or prints out some details about the model, trainer, and training process setup.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/train.py\":117-134",
            "content": "        y = torch.tensor(dix[1:], dtype=torch.long)\n        return x, y\ntrain_dataset = Dataset(open(datafile, \"r\", encoding=datafile_encoding).read(), model_level, ctx_len)\n########################################################################################################\n# Train model\n########################################################################################################\nmodel = GPT(GPTConfig(train_dataset.vocab_size, train_dataset.ctx_len, model_type=model_type,\n                rwkv_emb_scale=rwkv_emb_scale, rwkv_tiny_attn=rwkv_tiny_attn, rwkv_tiny_head=rwkv_tiny_head,\n                n_layer=n_layer, n_head=n_head, n_embd=n_embd, n_attn=n_attn, n_ffn=n_ffn))\n# load a trained model\n# model.load_state_dict(torch.load('trained-xxx.pth').state_dict())\nprint('model', model_type, 'epoch', n_epoch, 'batchsz', batch_size, 'betas', betas, 'eps', eps, 'wd', weight_decay, 'ctx', ctx_len, 'layer', n_layer, 'head', n_head, 'embd', n_embd, 'attn', n_attn, 'ffn', n_ffn)\ntconf = TrainerConfig(model_type=model_type, max_epochs=n_epoch, batch_size=batch_size, weight_decay=weight_decay,"
        },
        {
            "comment": "This code initializes a trainer with learning rate, decay, and final values for training the model on the provided train dataset. The trainer is then used to train the model, and after training, the trained model is saved in a file named \"trained-[run_name]-[current_date_time].pth\".",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/train.py\":135-141",
            "content": "                        learning_rate=lr_init, lr_decay=True, lr_final=lr_final, betas=betas, eps=eps,\n                        warmup_tokens=0, final_tokens=n_epoch*len(train_dataset)*ctx_len, num_workers=0, epoch_save_frequency=epoch_save_frequency, epoch_save_path=epoch_save_path)\ntrainer = Trainer(model, train_dataset, None, tconf)\ntrainer.train()\ntorch.save(model, 'trained-' + trainer.get_run_name() + '-' + datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S') + '.pth')"
        }
    ]
}