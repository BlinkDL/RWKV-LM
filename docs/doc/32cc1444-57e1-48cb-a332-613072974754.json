{
    "summary": "The code imports libraries, defines a Dataset class for data handling, sets vocabulary size, generates unique tokens, maps characters to integers, prints data and token sizes, initializes a tokenizer, calculates dataset length, samples logits, applies softmax with soft constraints on newlines, includes \"probs_sample\" function, converts tensor to float value, and sets random seeds for Python, numpy, and PyTorch.",
    "details": [
        {
            "comment": "The code is importing necessary libraries and defining a class Dataset for handling data. It checks the number of GPUs, reads input data, and determines the vocabulary size. It prints current vocabulary size and data token count for verification.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/utils.py\":0-28",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport os\ntry:\n    NUM_GPUS = int(os.environ['RWKV_NUM_GPUS'])\nexcept:\n    NUM_GPUS = 1\nimport json\nimport random\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset\nclass Dataset(Dataset):\n    def __init__(self, data, ctx_len, epoch_length_fixed):\n        self.ctx_len = ctx_len\n        self.epoch_length_fixed = epoch_length_fixed\n        self.data = data\n        if 'MMapIndexedDataset' in str(type(self.data)):\n            self.vocab_size = int(os.environ['VOCAB_SIZE'])\n            print('current vocab size =', self.vocab_size, \"(make sure it's correct)\")\n            self.data_size = len(self.data._bin_buffer) // 2\n            print(f'data has {self.data_size} tokens.')\n        elif 'numpy' in str(type(self.data)):"
        },
        {
            "comment": "This code sets the vocabulary size based on environment variable 'VOCAB_SIZE'. If the size is not specified, it generates a unique token list from data and stores it in 'vocab.json', then calculates the vocabulary and data sizes. It also maps characters to unique integers and inverse mapping. Finally, it prints the data size and number of unique tokens.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/utils.py\":29-52",
            "content": "            self.vocab_size = int(os.environ['VOCAB_SIZE'])\n            print('current vocab size =', self.vocab_size, \"(make sure it's correct)\")\n            self.data_size = len(self.data)\n            print(f'data has {self.data_size} tokens.')\n        else:\n            print('building token list...', end=' ')\n            unique = sorted(list(set(data)))\n            self.vocab_size = len(unique)\n            # print()\n            # for u in unique:\n            #     print(u, end=' ')\n            # print('\\n\\n')\n            xx = 0\n            xxObj = {}\n            for u in unique:\n                xxObj[xx] = u\n                xx += 1\n            with open('vocab.json', \"w\", encoding=\"utf-16\") as vocab_file:\n                vocab_file.write(json.dumps(xxObj, ensure_ascii=False))\n            self.data_size = len(self.data)\n            print('data has %d tokens, %d unique.' % (self.data_size, self.vocab_size))\n            self.stoi = {ch: i for i, ch in enumerate(unique)}\n            self.itos = {i: ch for i, ch in enumerate(unique)}"
        },
        {
            "comment": "This code defines a class that initializes a tokenizer using pre-trained word embeddings. It also includes methods to calculate the length and retrieve items from the dataset, handling different data types like MMapIndexedDataset or numpy arrays. The tokenizer is initialized with a list of words or a PreTrainedTokenizerFast instance based on the input argument.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/utils.py\":54-80",
            "content": "    def __len__(self):\n        return self.epoch_length_fixed // NUM_GPUS\n    def __getitem__(self, idx):\n        #\n        # we are cheating: pick a random spot in dataset\n        #\n        i = np.random.randint(0, self.data_size - (self.ctx_len + 1))\n        if 'MMapIndexedDataset' in str(type(self.data)):\n            dix = self.data.get(idx=0, offset=i, length=self.ctx_len + 1).astype(int)\n        elif 'numpy' in str(type(self.data)):\n            dix = self.data[i:i+self.ctx_len+1]\n        else:\n            dix = [self.stoi[s] for s in self.data[i:i+self.ctx_len+1]]\n        x = torch.tensor(dix[:-1], dtype=torch.long)\n        y = torch.tensor(dix[1:], dtype=torch.long)\n        return x, y\nclass TOKENIZER():\n    def __init__(self, WORD_NAME, UNKNOWN_CHAR='\\ue083'):\n        if 'list' in str(type(WORD_NAME)):\n            self.charMode = False\n            if WORD_NAME[0] == WORD_NAME[1]:\n                from transformers import PreTrainedTokenizerFast\n                self.tokenizer = PreTrainedTokenizerFast(tokenizer_file=WORD_NAME[0])"
        },
        {
            "comment": "This code checks if a tokenizer or word table is provided, initializes them accordingly and sets the vocabulary size. It also refines the context input by stripping unnecessary characters and returning an empty string if no content is found.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/utils.py\":81-105",
            "content": "            else:\n                from transformers import GPT2TokenizerFast\n                self.tokenizer = GPT2TokenizerFast(WORD_NAME[0], WORD_NAME[1])\n            self.vocab_size = len(self.tokenizer)\n        else:\n            self.charMode = True\n            with open(WORD_NAME + '.json', \"r\", encoding=\"utf-16\") as result_file:\n                self.word_table = json.load(result_file)\n            self.vocab_size = len(self.word_table)\n            self.stoi = {v: int(k) for k, v in self.word_table.items()}\n            self.itos = {int(k): v for k, v in self.word_table.items()}\n            self.UNKNOWN_CHAR = self.stoi[UNKNOWN_CHAR]\n    def refine_context(self, context):\n        context = context.strip().split('\\n')\n        for c in range(len(context)):\n            context[c] = context[c].strip().strip('\\u3000').strip('\\r')\n        context = list(filter(lambda c: c != '', context))\n        context = '\\n' + ('\\n'.join(context)).strip()\n        if context == '':\n            context = '\\n'\n        return context"
        },
        {
            "comment": "This function samples logits from the output of the model and applies softmax to obtain probabilities. It handles newlines by changing the top_p value when last character is a newline, otherwise it uses top_p_usual. It then sorts probabilities in descending order, finds the cutoff for sampling, and sets probabilities below this cutoff to 0.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/utils.py\":107-136",
            "content": "    def sample_logits(self, out, x, ctx_len, temperature=1.0, top_p_usual=None, top_p_newline=None):\n        # out[self.UNKNOWN_CHAR] = -float('Inf')\n        lastChar = int(x[-1])\n        probs = F.softmax(torch.tensor(out), dim=-1)\n        if self.charMode:\n            if self.itos[lastChar] == '\\n':\n                top_p = top_p_newline\n            else:\n                top_p = top_p_usual\n        else:\n            top_p = top_p_usual\n        sorted_probs, s_index = torch.sort(probs, descending=True)\n        # for j in range(30):\n        #     pp = sorted_probs[j].item()\n        #     if pp < 0.005:\n        #         break\n        #     ss = self.itos[int(s_index[j])].replace('\\n','_')\n        #     print(f'{math.floor(pp*100):>3.0f}{ss}', end='')\n        # print('')\n        cumulative_probs = torch.cumsum(sorted_probs, dim=-1).numpy()\n        cutoff = float(sorted_probs[np.argmax(cumulative_probs > top_p)])\n        probs[probs < cutoff] = 0\n        # print(\"[\" + str(round(cutoff,4)) + ' ' + str(round(to_float(sum(probs)),3)) + \"]\", end = \"\")"
        },
        {
            "comment": "This code snippet contains three functions: \"probs_sample\" which samples one sample from the multinomial distribution if temperature is not 1, \"to_float\" that converts a tensor to a float value, and \"set_seed\" for setting random seeds in Python, numpy, and PyTorch.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/utils.py\":138-152",
            "content": "        if temperature != 1.0:\n            probs = probs.pow(1.0 / temperature)\n        return torch.multinomial(probs, num_samples=1)[0]\ndef to_float(x):\n    return x.cpu().detach().numpy().flatten()[0].astype(float)\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)"
        }
    ]
}