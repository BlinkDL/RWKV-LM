{
    "summary": "This code utilizes the RWKV Language Model for text generation, setting up parameters and initializing the model. It tokenizes prompts, performs multiple trials, calculates logits, handles special cases, prints tokens, and saves/loads model states.",
    "details": [
        {
            "comment": "This code is importing necessary libraries and modules for the RWKV Language Model. It sets the TOKEN_MODE variable to either 'char', 'bpe', or 'pile' depending on whether the model is trained from scratch, pre-trained pile models are being tested, etc. This helps set up the appropriate configuration for running the model.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/run.py\":0-25",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport numpy as np\nimport math, os\nimport time\nimport types\nimport copy\nimport torch\nfrom torch.nn import functional as F\nfrom src.utils import TOKENIZER, Dataset\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cuda.matmul.allow_tf32 = True\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)\n########################################################################################################\n# Step 1: set model\n# \n# Set TOKEN_MODE to 'char' or 'bpe' if the model is trained by 'train.py' from scratch.\n#\n# Set TOKEN_MODE to 'pile' if you want to test pre-trained pile models.\n########################################################################################################\nTOKEN_MODE = 'char' # char / bpe / pile"
        },
        {
            "comment": "This code is responsible for setting the necessary parameters and model name based on the tokenization mode. The modes include character, byte-pair encoding (BPE), and Pile. If using a pre-trained RWKV model, it allows specifying a fine-tuned model for better performance. Different model architectures like BERT, RoBERTa, and GPT are used depending on the mode. The parameters n_layer, n_embd, and ctx_len define the number of layers, embedding dimensions, and context length respectively for the chosen model.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/run.py\":27-59",
            "content": "n_layer = 6\nn_embd = 512\nctx_len = 1024\nif TOKEN_MODE == 'char':\n    MODEL_NAME = 'trained-500'  # your trained model\n    WORD_NAME = 'vocab'         # the .json vocab (generated by train.py)\n    # set UNKNOWN_CHAR to the rarest token in your vocab.json, and all unknown tokens in your prompt will be denoted by it\n    UNKNOWN_CHAR = ' '          # here we just set it to ' ' for simplicity\nelif TOKEN_MODE == 'bpe':\n    MODEL_NAME = 'trained-500'  # your trained model\n    WORD_NAME = ['model-vocab.json', 'model-merges.txt'] # [vocab, merge] for your BPE model\n    UNKNOWN_CHAR = None\nelif TOKEN_MODE == 'pile':\n    WORD_NAME = ['20B_tokenizer.json', '20B_tokenizer.json']\n    UNKNOWN_CHAR = None\n    #---> you can set MODEL_NAME to your fine-tuned model <---\n    MODEL_NAME = 'RWKV-4-Pile-169M-20220807-8023'\n    # MODEL_NAME = 'trained-11'\n    n_layer = 12\n    n_embd = 768\n    ctx_len = 1024\n    # MODEL_NAME = 'RWKV-4-Pile-430M-20220808-8066'\n    # n_layer = 24\n    # n_embd = 1024\n    # ctx_len = 1024\n    # MODEL_NAME = 'RWKV-4-Pile-1B5-20220903-8040'"
        },
        {
            "comment": "The code is setting the environment variables for the RWKV model, specifying the model type, and providing the prompt and sampling parameters. The model is currently using fp32 (floating-point arithmetic) and running on the CPU. The prompt provided is a text describing the discovery of Chinese-speaking dragons in Tibet. The code will perform 999 trials, each producing output of length 333, with temperature set to 1.0 for sampling and top_p set to 0.7 and 0.9 for softmax output.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/run.py\":60-86",
            "content": "    # n_layer = 24\n    # n_embd = 2048\n    # ctx_len = 1024    \nos.environ['RWKV_FLOAT_MODE'] = 'fp32'  # 'bf16' / 'fp16' / 'fp32' (note: only using fp32 at this moment)\nos.environ['RWKV_RUN_DEVICE'] = 'cpu'   # 'cpu' (already very fast) or 'cuda'\nmodel_type = 'RWKV' # 'RWKV' or 'RWKV-ffnPre'\n########################################################################################################\n# Step 2: set prompt & sampling stuffs\n########################################################################################################\n# context = 'A'\n# context = \"\\nIn the\"\n# context = '\\nSugar:'\ncontext = '\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.'\nNUM_TRIALS = 999\nLENGTH_PER_TRIAL = 333\nTEMPERATURE = 1.0\ntop_p = 0.7\ntop_p_newline = 0.9 # only used in TOKEN_MODE = char\nDEBUG_DEBUG = False  # True False --> show softmax output\n####"
        },
        {
            "comment": "Loading the specified RWKV model, creating a tokenizer for text processing, refining context if in character mode, converting context to tokens, printing the number of tokens in prompt, informing about the time taken to process long prompts with RNN or using GPT for faster speed, and starting a loop for multiple trials.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/run.py\":86-108",
            "content": "####################################################################################################\nprint(f'Loading {MODEL_NAME}...')\nfrom src.model_run import RWKV_RNN\nmodel = RWKV_RNN(MODEL_NAME, os.environ['RWKV_RUN_DEVICE'], model_type, n_layer, n_embd, ctx_len)\ntokenizer = TOKENIZER(WORD_NAME, UNKNOWN_CHAR=UNKNOWN_CHAR)\n########################################################################################################\nif tokenizer.charMode:\n    context = tokenizer.refine_context(context)\n    ctx = [tokenizer.stoi.get(s, tokenizer.UNKNOWN_CHAR) for s in context]\nelse:\n    ctx = tokenizer.tokenizer.encode(context)\nsrc_len = len(ctx)\nsrc_ctx = ctx.copy()\nprint('\\nYour prompt has ' + str(src_len) + ' tokens.')\nprint('\\n--> Currently the first run takes a while if your prompt is long, as we are using RNN to process the prompt. Use GPT to build the hidden state for better speed. <--\\n')\nfor TRIAL in range(1 if DEBUG_DEBUG else NUM_TRIALS):\n    t_begin = time.time_ns()\n    print(('-' * 30) + context, end='')"
        },
        {
            "comment": "This code initializes a model and its state, then generates text based on the input sequence. It saves and loads the model's state for subsequent trials, calculates logits for character sampling, and handles special cases like disabling <|endoftext|>. The DEBUG_DEBUG print statement displays output statistics if enabled.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/run.py\":109-141",
            "content": "    ctx = src_ctx.copy()\n    model.clear()\n    if TRIAL == 0:\n        init_state = types.SimpleNamespace()\n        for i in range(src_len):\n            x = ctx[:i+1]\n            if i == src_len - 1:\n                init_state.out = model.run(x)\n            else:\n                model.run(x)\n        model.save(init_state)\n    else:\n        model.load(init_state)\n    for i in range(src_len, src_len + (1 if DEBUG_DEBUG else LENGTH_PER_TRIAL)):\n        x = ctx[:i+1]\n        x = x[-ctx_len:]\n        if i == src_len:\n            out = copy.deepcopy(init_state.out)\n        else:\n            out = model.run(x)\n        if DEBUG_DEBUG:\n            print('model', np.array(x), '==>', np.array(\n                out), np.max(out), np.min(out))\n        if TOKEN_MODE == 'pile':\n            out[0] = -999999999  # disable <|endoftext|>\n        char = tokenizer.sample_logits(out, x, ctx_len, temperature=TEMPERATURE,\n                                       top_p_usual=top_p, top_p_newline=top_p_newline)\n        char = char.item()\n        if tokenizer.charMode:"
        },
        {
            "comment": "The code is printing each token in a sequence using either the integer representation or the decoded version from the tokenizer, and storing each character in the ctx list. It also measures the time taken for this process and prints it in seconds at the end.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/run.py\":142-148",
            "content": "            print(tokenizer.itos[int(char)], end='', flush=True)\n        else:\n            print(tokenizer.tokenizer.decode(int(char)), end='', flush=True)\n        ctx += [char]\n    t_end = time.time_ns()\n    print(\"\\n----------\", round((t_end - t_begin) / (10 ** 9), 2), end='s ')"
        }
    ]
}