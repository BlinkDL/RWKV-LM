{
    "summary": "This code initializes and trains a RWKV language model, sets layers and context length, creates tokenizer, loads model file, and prints select RWKV-RNN outputs.",
    "details": [
        {
            "comment": "The code imports necessary libraries and sets environment variables for running the RWKV Language Model, which verifies results from different models to ensure consistency. It also specifies the device (CPU or GPU) to run the model and tokenization method (Pile).",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/verify.py\":0-26",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\n# this is for verifying the results of different models and make sure they agree with each other\nimport os, sys, types\nimport numpy as np\nimport torch\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)\ntry:\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = sys.argv[1]\nexcept:\n    pass\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.allow_tf32 = False\ntorch.backends.cuda.matmul.allow_tf32 = False\nos.environ['RWKV_FLOAT_MODE'] = 'bf16' # bf16 or fp32\nos.environ['RWKV_RUN_DEVICE'] = 'cuda' # currently model_train requires CUDA\nRUN_DEVICE = os.environ['RWKV_RUN_DEVICE']\nTOKEN_MODE = 'pile'\nif TOKEN_MODE == 'pile':\n    WORD_NAME = ['20B_tokenizer.json', '20B_tokenizer.json']\n    MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-3b/RWKV-4-Pile-3B-20221003-6783'"
        },
        {
            "comment": "This code initializes a RWKV model for training. It sets the number of layers, embedding dimension, and context length. The tokenizer is created based on the given word name and unknown character. The environment variables are set to define the maximum context length and enable JIT compilation. The model is then loaded from the specified file using appropriate float mode.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/verify.py\":27-65",
            "content": "    n_layer = 32\n    n_embd = 2560\n    ctx_len = 1024\n    UNKNOWN_CHAR = None\nfrom src.utils import TOKENIZER\ntokenizer = TOKENIZER(WORD_NAME, UNKNOWN_CHAR=UNKNOWN_CHAR)\nif TOKEN_MODE == 'pile':\n    tokenizer.vocab_size = 50277\n########################################################################################################\nos.environ[\"RWKV_JIT_ON\"] = \"1\"\nos.environ[\"RWKV_T_MAX\"] = str(ctx_len)\nfrom src.model_run import RWKV_RNN\nfrom src.model import RWKV\nargs = types.SimpleNamespace()\nargs.vocab_size = tokenizer.vocab_size\nargs.ctx_len = ctx_len\nargs.n_embd = n_embd\nargs.n_layer = n_layer\nargs.head_qk = 0\nargs.pre_ffn = 0\nargs.grad_cp = 0\nargs.my_pos_emb = 0\nmodel_train = RWKV(args).to(RUN_DEVICE)\nif os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n    model_train = model_train.half()\nelif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n    model_train = model_train.bfloat16()\nprint('loading ' + MODEL_NAME)\nm2 = torch.load(MODEL_NAME + '.pth', map_location='cpu')\nmodel_train.load_state_dict(m2)\nif os.environ['RWKV_FLOAT_MODE'] == 'fp16':"
        },
        {
            "comment": "This code checks the RWKV_FLOAT_MODE environment variable and sets the appropriate float mode for the model_train. It then verifies the device and float mode, encodes a context string into tokens using the tokenizer, and generates output from the model_train in forward pass with no gradient calculation (torch.no_grad()).",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/verify.py\":66-90",
            "content": "    model_train = model_train.half()\nelif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n    model_train = model_train.bfloat16()\nargs.MODEL_NAME = MODEL_NAME\nargs.RUN_DEVICE = RUN_DEVICE\nargs.FLOAT_MODE = os.environ['RWKV_FLOAT_MODE']\nmodel_rnn = RWKV_RNN(args)\n########################################################################################################\nprint(f\"\\nVerifying {os.environ['RWKV_RUN_DEVICE']} {os.environ['RWKV_FLOAT_MODE']}\")\n# context = '\\nIn a'\ncontext = '\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.'\nif TOKEN_MODE == 'pile':\n    ctx = tokenizer.tokenizer.encode(context)\nprint(f'input len {len(ctx)} data {ctx}')\n########################################################################################################\nwith torch.no_grad():\n    print('\\nRWKV-train output')\n    out = model_train.forward(torch.tensor([ctx]).to(RUN_DEVICE))[0].detach().cpu().float().numpy()"
        },
        {
            "comment": "This code prints RWKV-RNN output at certain positions in the sequence. It uses a for loop to iterate through the context, calling the forward function of model_rnn. The first three outputs and the third one are printed using `print(out.detach().cpu().numpy())`, with ellipsis ('...') printed after the second output.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/verify.py\":91-103",
            "content": "    print(out, '\\n')\n    print('\\nRWKV-RNN output')\n    state = None\n    out = None\n    src_len = len(ctx)\n    for i in range(src_len):\n        x = ctx[:i+1]\n        out, state = model_rnn.forward(x, state)\n        if i < 3 or i >= src_len - 3:\n            print(out.detach().cpu().numpy())\n        if i == 2:\n            print('...')"
        }
    ]
}