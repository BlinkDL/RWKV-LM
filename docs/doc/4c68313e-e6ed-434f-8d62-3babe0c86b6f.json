{
    "summary": "This code performs a forward pass of a neural network on GPU with BFloat16 data type and softmax cross-entropy loss, calculating gradients using fixed-point arrays in CUDA. It defines two functions, cuda_forward and cuda_backward, which use CUDA to perform matrix operations on GPU.",
    "details": [
        {
            "comment": "This function implements the forward pass of a neural network operation using BFloat16 (bf16) data type on GPU. It takes input dimensions and pointers to weights, inputs, keys, and values arrays as parameters. The function then performs element-wise multiplications, accumulates results, and stores the result in an output array.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/cuda/wkv_cuda_bf16.cu\":0-25",
            "content": "#include <stdio.h>\n#include <assert.h>\n#include \"ATen/ATen.h\"\n#define MIN_VALUE (-1e38)\ntypedef at::BFloat16 bf16;\n__global__ void kernel_forward(const int B, const int T, const int C,\n                               const float *__restrict__ const _w, const bf16 *__restrict__ const _u, const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v,\n                               bf16 *__restrict__ const _y) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int _b = idx / C;\n    const int _c = idx % C;\n    const int _offset = _b * T * C + _c;\n    float u = float(_u[_c]);\n    float w = _w[_c];\n    const bf16 *__restrict__ const k = _k + _offset;\n    const bf16 *__restrict__ const v = _v + _offset;\n    bf16 *__restrict__ const y = _y + _offset;\n    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n    float aa = 0, bb = 0, pp = MIN_VALUE;\n    for (int i = 0; i < T; i++) {\n        const int ii = i * C;\n        const float kk = float(k[ii]);\n        const float vv = float(v[ii]);"
        },
        {
            "comment": "Calculates gradients for weight, input, and kernel arrays using backward pass with softmax cross-entropy loss. Batch size, sequence length, number of channels, weight, input, kernel arrays, output gradients, and gradients for each array are passed as arguments to the kernel function. Gradient computation is performed per element in the arrays.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/cuda/wkv_cuda_bf16.cu\":27-52",
            "content": "        float ww = u + kk;\n        float p = max(pp, ww);\n        float e1 = exp(pp - p);\n        float e2 = exp(ww - p);\n        y[ii] = bf16((e1 * aa + e2 * vv) / (e1 * bb + e2));\n        ww = w + pp;\n        p = max(ww, kk);\n        e1 = exp(ww - p);\n        e2 = exp(kk - p);\n        aa = e1 * aa + e2 * vv;\n        bb = e1 * bb + e2;\n        pp = p;\n    }\n}\n__global__ void kernel_backward(const int B, const int T, const int C,\n                                const float *__restrict__ const _w, const bf16 *__restrict__ const _u, const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v,\n                                const bf16 *__restrict__ const _y, const bf16 *__restrict__ const _gy,\n                                bf16 *__restrict__ const _gw, bf16 *__restrict__ const _gu, bf16 *__restrict__ const _gk, bf16 *__restrict__ const _gv) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int _b = idx / C;\n    const int _c = idx % C;\n    const int _offset = _b * T * C + _c;\n    float u = float(_u[_c]);"
        },
        {
            "comment": "The code is calculating the softmax and element-wise multiplication of input vectors for matrix multiplication using bf16 data type in CUDA. It initializes variables, performs calculations using exp() and max(), stores results in q and r arrays, and updates gw and gu variables.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/cuda/wkv_cuda_bf16.cu\":53-84",
            "content": "    float w = _w[_c];\n    const bf16 *__restrict__ const k = _k + _offset;\n    const bf16 *__restrict__ const v = _v + _offset;\n    const bf16 *__restrict__ const y = _y + _offset;\n    const bf16 *__restrict__ const gy = _gy + _offset;\n    bf16 *__restrict__ const gk = _gk + _offset;\n    bf16 *__restrict__ const gv = _gv + _offset;\n    float q[Tmax], r[Tmax];\n    float gw = 0, gu = 0, aa = 0, bb = 0, ga = 0, gb = 0, pp = MIN_VALUE;\n    for (int i = 0; i < T; i++) {\n        const int ii = i * C;\n        const float kk = float(k[ii]);\n        const float vv = float(v[ii]);\n        const float yy = float(y[ii]);\n        float ww = u + kk;\n        float p = max(pp, ww);\n        float e1 = exp(pp - p);\n        float e2 = exp(ww - p);\n        const float qq = float(gy[ii]) / (e1 * bb + e2);\n        gw += (ga - gb * yy) * e1 * qq;\n        gu += (vv - yy) * e2 * qq;\n        q[i] = qq;\n        r[i] = ww - p;\n        ww = w + pp;\n        p = max(ww, kk);\n        e1 = exp(ww - p);\n        e2 = exp(kk - p);\n        ga = e1 * (aa + ga);"
        },
        {
            "comment": "This code computes the forward pass of a neural network using CUDA for efficient GPU computation. The input includes batch size B, time steps T, channels C, and floating-point w parameter, along with fixed-point u, k, v, and y arrays. It initializes gk and gv arrays and performs element-wise computations to calculate the gradients.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/cuda/wkv_cuda_bf16.cu\":85-119",
            "content": "        gb = e1 * (bb + gb);\n        aa = e1 * aa + e2 * vv;\n        bb = e1 * bb + e2;\n        pp = p;\n    }\n    const int _offsetBC = _b * C + _c;\n    _gw[_offsetBC] = bf16(gw * _w[_c]); // multiply by w because of w -> -exp(w) in python forward()\n    _gu[_offsetBC] = bf16(gu);\n    aa = 0, bb = 0, pp = MIN_VALUE;\n    for (int i = T - 1; i >= 0; i--) {\n        const int ii = i * C;\n        const float kk = float(k[ii]);\n        const float vv = float(v[ii]);\n        const float yy = float(y[ii]);\n        const float qq = q[i];\n        const float rr = r[i];\n        float e1 = qq * exp(rr);\n        float e2 = exp(kk + pp);\n        gk[ii] = bf16(e1 * (vv - yy) + e2 * (aa * vv + bb));\n        gv[ii] = bf16(e1 + e2 * aa);\n        const float ww = w + pp;\n        const float www = rr - u - kk;\n        const float p = max(ww, www);\n        e1 = exp(ww - p);\n        e2 = qq * exp(www - p);\n        aa = e1 * aa + e2;\n        bb = e1 * bb - e2 * yy;\n        pp = p;\n    }\n}\nvoid cuda_forward(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y) {"
        },
        {
            "comment": "This code defines two functions, cuda_forward and cuda_backward, which use CUDA to perform a matrix operation on a GPU. The function sets the number of threads per block based on the C dimension and ensures optimal performance by setting --maxrregcount 60. It also asserts that B * C is divisible by threadsPerBlock.x to ensure even distribution of tasks among blocks, then assigns the total number of blocks accordingly. Finally, it calls a kernel function with these parameters.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/cuda/wkv_cuda_bf16.cu\":120-131",
            "content": "    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n    assert(B * C % threadsPerBlock.x == 0);\n    dim3 numBlocks(B * C / threadsPerBlock.x);\n    kernel_forward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y);\n}\nvoid cuda_backward(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, bf16 *gy, bf16 *gw, bf16 *gu, bf16 *gk, bf16 *gv) {\n    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n    assert(B * C % threadsPerBlock.x == 0);\n    dim3 numBlocks(B * C / threadsPerBlock.x);\n    kernel_backward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, gy, gw, gu, gk, gv);\n}"
        }
    ]
}