{
    "summary": "This code defines a custom dataset class and tokenizer for RWKV language model, initializing objects with dictionaries and functions. It handles text generation using top-p sampling with temperature parameter and includes conversion functions for input values.",
    "details": [
        {
            "comment": "The code defines a custom dataset class for the RWKV language model, which reads in data and builds a token list. It then stores the unique tokens in a dictionary-like format and writes it to a 'vocab.json' file. The function prints the total number of tokens in the data and the number of unique tokens.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/utils.py\":0-33",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport json\nimport random\nimport time\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset\nclass Dataset(Dataset):\n    def __init__(self, data, ctx_len, epoch_length_fixed):\n        print('building token list...', end=' ')\n        unique = sorted(list(set(data)))\n        # print()\n        # for u in unique:\n        #     print(u, end=' ')\n        # print('\\n\\n')\n        xx = 0\n        xxObj = {}\n        for u in unique:\n            xxObj[xx] = u\n            xx += 1\n        with open('vocab.json', \"w\", encoding=\"utf-16\") as vocab_file:\n            vocab_file.write(json.dumps(xxObj, ensure_ascii=False))\n        data_size, vocab_size = len(data), len(unique)\n        print('data has %d tokens, %d unique.' % (data_size, vocab_size))"
        },
        {
            "comment": "The code above initializes an object for a tokenizer that converts text data into numerical representations. The object contains dictionaries mapping characters to indices (stoi) and indices to characters (itos), context length, fixed epoch length, vocabulary size, and the actual text data. It also provides functions for getting elements at specific index and calculating lengths of the tokenizer instance. The tokenizer class is initialized with a Word Name file and an optional Unknown Character.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/utils.py\":34-61",
            "content": "        self.stoi = {ch: i for i, ch in enumerate(unique)}\n        self.itos = {i: ch for i, ch in enumerate(unique)}\n        self.ctx_len = ctx_len\n        self.epoch_length_fixed = epoch_length_fixed\n        self.vocab_size = vocab_size\n        self.data = data\n    def __len__(self):\n        return self.epoch_length_fixed\n    def __getitem__(self, idx):\n        # cheat: pick a random spot in dataset\n        i = np.random.randint(0, len(self.data) - (self.ctx_len + 1))\n        chunk = self.data[i:i+self.ctx_len+1]\n        dix = [self.stoi[s] for s in chunk]\n        x = torch.tensor(dix[:-1], dtype=torch.long,\n                         device=torch.device('cuda'))\n        y = torch.tensor(dix[1:], dtype=torch.long,\n                         device=torch.device('cuda'))\n        return x, y\nclass TOKENIZER():\n    def __init__(self, WORD_NAME, UNKNOWN_CHAR='\\ue083'):\n        with open(WORD_NAME + '.json', \"r\", encoding=\"utf-16\") as result_file:\n            self.word_table = json.load(result_file)\n        self.vocab_size = len(self.word_table)"
        },
        {
            "comment": "Function `refine_context` strips and filters context strings.\n\"sample\\_logits\" calculates softmax probs, applies top\\_p if last char is newline, sorts probs, then...",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/utils.py\":63-94",
            "content": "        self.stoi = {v: int(k) for k, v in self.word_table.items()}\n        self.itos = {int(k): v for k, v in self.word_table.items()}\n        self.UNKNOWN_CHAR = self.stoi[UNKNOWN_CHAR]\n    def refine_context(self, context):\n        context = context.strip().split('\\n')\n        for c in range(len(context)):\n            context[c] = context[c].strip().strip('\\u3000').strip('\\r')\n        context = list(filter(lambda c: c != '', context))\n        context = '\\n' + ('\\n'.join(context)).strip()\n        if context == '':\n            context = '\\n'\n        return context\n    def sample_logits(self, out, x, ctx_len, temperature=1.0, top_p_usual=None, top_p_newline=None):\n        # out[self.UNKNOWN_CHAR] = -float('Inf')\n        lastChar = int(x[-1])\n        probs = F.softmax(torch.tensor(out), dim=-1)\n        if self.itos[lastChar] == '\\n':\n            top_p = top_p_newline\n        else:\n            top_p = top_p_usual\n        sorted_probs, s_index = torch.sort(probs, descending=True)\n        # for j in range(30):\n        #     pp = sorted_probs[j].item()"
        },
        {
            "comment": "This code snippet is from the RWKV-LM project's \"utils.py\" file, and it appears to handle text generation using top-p sampling with a temperature parameter. The function generates a single token based on the given input and calculates cumulative probabilities. It then determines a cutoff value for the probabilities and sets any values below that cutoff to zero. If the temperature is not set to 1.0, it applies power normalization to the probabilities. Finally, it uses torch.multinomial to select one token based on the modified probabilities and returns it. The code also includes a to_float function for converting input values to floats and a set_seed function for setting random number generator seeds.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/utils.py\":95-121",
            "content": "        #     if pp < 0.005:\n        #         break\n        #     ss = self.itos[int(s_index[j])].replace('\\n','_')\n        #     print(f'{math.floor(pp*100):>3.0f}{ss}', end='')\n        # print('')\n        cumulative_probs = torch.cumsum(sorted_probs, dim=-1).numpy()\n        cutoff = float(sorted_probs[np.argmax(cumulative_probs > top_p)])\n        probs[probs < cutoff] = 0\n        # print(\"[\" + str(round(cutoff,4)) + ' ' + str(round(to_float(sum(probs)),3)) + \"]\", end = \"\")\n        if temperature != 1.0:\n            probs = probs.pow(1.0 / temperature)\n        return torch.multinomial(probs, num_samples=1)[0]\ndef to_float(x):\n    return x.cpu().detach().numpy().flatten()[0].astype(float)\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)"
        }
    ]
}