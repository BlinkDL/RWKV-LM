{
    "summary": "The code establishes the RWKV Language Model trainer, trains a model with specified parameters, updates optimizer, handles multi-GPU, averages losses, logs progress, and saves states at each epoch.",
    "details": [
        {
            "comment": "This code sets up the trainer configuration for the RWKV Language Model. It defines the batch size, learning rate, optimizer parameters, gradient norm clip, and warmup tokens. The code also ensures proper CUDA backend configurations based on the environment variables.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/trainer.py\":0-33",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport os\nNUM_GPUS = int(os.environ['RWKV_NUM_GPUS'])\nUSE_WANDB = (int(os.environ['USE_WANDB']) == 1)\nfrom torch.utils.data.dataloader import DataLoader\nimport torch\nfrom tqdm.auto import tqdm\nimport logging\nimport datetime\nimport math\nfrom pytorch_lightning.lite import LightningLite\nimport gc\nlogger = logging.getLogger(__name__)\ntorch.backends.cudnn.benchmark = True\nif os.environ['RWKV_FLOAT_MODE'] == 'fp32':\n    torch.backends.cudnn.allow_tf32 = False\n    torch.backends.cuda.matmul.allow_tf32 = False\nelse:\n    torch.backends.cudnn.allow_tf32 = True\n    torch.backends.cuda.matmul.allow_tf32 = True\nclass TrainerConfig:\n    batch_size = 64\n    learning_rate = 4e-4\n    betas = (0.9, 0.99)\n    eps = 1e-8\n    grad_norm_clip = 1.0\n    warmup_tokens = 0"
        },
        {
            "comment": "The code defines a Trainer class that initializes various variables and contains methods for model training. The `get_run_name` method generates the run name based on the model's configuration, `run` method prepares the model, and in this snippet, it checks if a pre-trained model should be loaded.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/trainer.py\":34-62",
            "content": "    final_tokens = 0\n    epoch_save_frequency = 0\n    epoch_save_path = 'trained-'\n    num_workers = 0  # for DataLoader\n    def __init__(self, **kwargs):\n        for k, v in kwargs.items():\n            setattr(self, k, v)\nfrom src.model import GPT, GPTConfig\nclass Trainer(LightningLite):\n    def get_run_name(self):\n        raw_model = self.model.module if hasattr(\n            self.model, \"module\") else self.model\n        cfg = raw_model.config\n        run_name = str(cfg.vocab_size) + '-' + str(cfg.ctx_len) + '-' + \\\n            cfg.model_type + '-' + str(cfg.n_layer) + '-' + str(cfg.n_embd)\n        return run_name\n    def run(self, m_cfg, train_dataset, test_dataset, config):\n        self.cuda_id = int(str(self.device).strip('cuda:'))\n        print('[0]')\n        model = GPT(GPTConfig(train_dataset.vocab_size, train_dataset.ctx_len, model_type=m_cfg.model_type,\n                        n_layer=m_cfg.n_layer, n_embd=m_cfg.n_embd))\n        print('[1]')\n        with torch.no_grad():\n            if m_cfg.LOAD_MODEL:"
        },
        {
            "comment": "Loading model, transferring it to GPU, and initializing logging for training.\nThe code loads the model from a specified file path, transfers it to the device's GPU, and opens a log file if necessary. If WandB is enabled, it initializes WandB with project details and a unique run name based on the current date and time. The configuration is combined and saved without saving the code itself.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/trainer.py\":63-86",
            "content": "                print('loading', m_cfg.MODEL_NAME)\n                m2 = torch.load(m_cfg.MODEL_NAME + '.pth', map_location='cpu')\n                model.load_state_dict(m2)\n                del m2\n        model.to(self.device)\n        self.model = model\n        self.train_dataset = train_dataset\n        self.test_dataset = test_dataset\n        self.config = config\n        self.avg_loss = -1\n        self.EPOCH_BEGIN = m_cfg.EPOCH_BEGIN\n        self.steps = self.EPOCH_BEGIN * (len(self.train_dataset) // (config.batch_size // NUM_GPUS))\n        if self.cuda_id == 0:\n            log_file = open(\"mylog.txt\", \"a\")\n            if USE_WANDB:\n                print('logging to wandb... (comment it if you don\\'t have wandb)')\n                import wandb # comment this if you don't have wandb\n                cfg = model.config\n                for k in config.__dict__:\n                    setattr(cfg, k, config.__dict__[k]) # combine cfg\n                wandb.init(project=\"RWKV-LM\", name=self.get_run_name() + '-' + datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S'), config=cfg, save_code=False)"
        },
        {
            "comment": "This code defines a function \"run_epoch\" that takes in a split (train or test) and performs the necessary configurations for training or testing. It sets the model to train mode if split is 'train'. Then, it assigns the corresponding dataset (train or test) to the variable data. The data's idx_begin is set to the current steps multiplied by config.batch_size + 1, and cuda_id is set to self.cuda_id. Finally, it creates a DataLoader for the dataset with specified batch size and number of workers based on config settings.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/trainer.py\":88-108",
            "content": "        model, config = self.model, self.config\n        raw_model = model.module if hasattr(self.model, \"module\") else model\n        optimizer = raw_model.configure_optimizers(config)\n        model, optimizer = self.setup(model, optimizer)\n        print('[3]')\n        def run_epoch(split):\n            is_train = split == 'train'\n            model.train(is_train)\n            data = self.train_dataset if is_train else self.test_dataset\n            data.idx_begin = self.steps * config.batch_size + 1\n            data.cuda_id = self.cuda_id\n            if config.num_workers > 0:\n                loader = DataLoader(data, shuffle=False, pin_memory=True,\n                                    batch_size=config.batch_size // NUM_GPUS,\n                                    num_workers=config.num_workers)\n            else:\n                loader = DataLoader(data, shuffle=False,\n                                    batch_size=config.batch_size // NUM_GPUS,\n                                    num_workers=config.num_workers)"
        },
        {
            "comment": "This code sets up a dataloader and trains an RWKV model. It iterates over the dataloader, forwards data through the model, calculates loss, and performs backpropagation if training. Depending on the DEEPSPEED environment variable, it handles all_gather for multi-GPU scenarios. Finally, it updates the optimizer and decay the learning rate based on progress.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/trainer.py\":110-135",
            "content": "            pbar = tqdm(enumerate(loader), total=len(\n                loader), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}') if is_train else enumerate(loader)\n            loader = self.setup_dataloaders(loader)\n            gc.collect()\n            torch.cuda.empty_cache()\n            for it, (x, y) in pbar:\n                with torch.set_grad_enabled(is_train):\n                    loss = model(x, y) # forward the model\n                if os.environ['RWKV_DEEPSPEED'] == '0':\n                    all_loss = [loss.clone()]\n                else:\n                    all_loss = [loss.clone() for _ in range(NUM_GPUS)]\n                    torch.distributed.all_gather(all_loss, loss)\n                if is_train:  # backprop and update the parameters\n                    model.zero_grad()\n                    self.backward(loss)\n                    # deepspeed will handle gradient_clipping\n                    optimizer.step()\n                    # decay the learning rate based on our progress\n                    self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)"
        },
        {
            "comment": "This code determines the learning rate (lr) for training a model using the RWKV algorithm. It uses config parameters such as lr_final, learning_rate, warmup_tokens, and final_tokens to calculate the learning rate based on whether the current token count is in the warm-up phase or not. If in the warm-up phase (tokens < warmup_tokens), it performs linear interpolation. If past warm-up phase, it does exponential decay. The calculated lr is then applied to optimizer's param_groups and stored in self.lr.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/trainer.py\":136-155",
            "content": "                    lr_final_factor = config.lr_final / config.learning_rate\n                    if self.tokens < config.warmup_tokens:\n                        # linear warmup\n                        lr_mult = lr_final_factor + \\\n                            (1 - lr_final_factor) * float(self.tokens) / \\\n                            float(config.warmup_tokens)\n                        progress = 0\n                    else:\n                        # exponential learning rate decay\n                        progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n                        if progress >= 1:\n                            lr_mult = lr_final_factor\n                        else:\n                            lr_mult = math.exp(math.log(lr_final_factor) * pow(progress, 1))\n                    lr = config.learning_rate * lr_mult\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = lr\n                    self.lr = lr"
        },
        {
            "comment": "The code calculates the average loss over multiple GPUs, updates a moving average of the loss, logs the current loss to Wandb (if applicable), and sets the progress description. It also resets the tokens counter for learning rate decay and runs an epoch.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/trainer.py\":156-178",
            "content": "                    self.steps += 1\n                    now_loss = 0\n                    for gg in range(NUM_GPUS):\n                        now_loss += all_loss[gg].item()\n                    now_loss = now_loss / NUM_GPUS # report progress                    \n                    if USE_WANDB and self.cuda_id == 0:\n                        wandb.log({\"loss\": now_loss}, step = self.steps)\n                    if self.avg_loss < 0:\n                        self.avg_loss = now_loss\n                    else:\n                        factor = 1 / (it + 1)\n                        self.avg_loss = self.avg_loss * (1.0 - factor) + now_loss * factor\n                    pbar.set_description(f\"miniE {epoch+1+self.EPOCH_BEGIN} s {self.steps} prog {progress*100.0:.2f}% : ppl {math.exp(self.avg_loss):.6f} loss {self.avg_loss:.6f} lr {lr:e}\")\n        self.tokens = 0  # counter used for learning rate decay\n        for epoch in range(99999999):\n            run_epoch('train')\n            if math.isnan(self.avg_loss):\n                exit(0)"
        },
        {
            "comment": "This code snippet saves the model's state every time an epoch ends, or if the current epoch is a multiple of `config.epoch_save_frequency`. If using GPU, it saves the model's state dict as a .pth file with the epoch number in the filename and path specified by `config.epoch_save_path`. Additionally, it logs loss values during each epoch.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/trainer.py\":180-186",
            "content": "            if self.cuda_id == 0:\n                log_file.write(f'{epoch+1+self.EPOCH_BEGIN} {self.avg_loss:.6f} {math.exp(self.avg_loss):.4f} {self.lr:.8f} {datetime.datetime.now()} {epoch+1} \\n')\n                log_file.flush()\n                if (self.config.epoch_save_frequency > 0 and epoch % self.config.epoch_save_frequency == 0) or (epoch == config.max_epochs - 1):\n                    raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n                    torch.save(raw_model.state_dict(), self.config.epoch_save_path + str(epoch+1+self.EPOCH_BEGIN) + '.pth')"
        }
    ]
}