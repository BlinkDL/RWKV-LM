{
    "summary": "This code initializes and compares the outputs of RWKV-GPT and RWKV-RNN models, tokenizing input context and training model_train on GPU with tensor data. It sets environment variables for hardware mode and floating-point precision.",
    "details": [
        {
            "comment": "This code verifies the results of different models to ensure consistency. It imports necessary libraries, sets environment variables for hardware mode and floating-point precision, defines the model to run (RWKV_RNN or GPT), specifies token mode as either character or pile, and defines variables for model name, vocabulary file, context length, and number of layers.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/verify.py\":0-25",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\n# this is for verifying the results of different models and make sure they agree with each other\nimport numpy as np\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ['RWKV_FLOAT_MODE'] = 'bf16' # 'bf16' (stable) or 'fp16' (will overflow after training a large model for very long. can be solved in the future)\nos.environ['RWKV_RUN_DEVICE'] = 'cuda'\nRUN_DEVICE = os.environ['RWKV_RUN_DEVICE']\nimport torch\nfrom src.model_run import RWKV_RNN, RWKV_GPT\nfrom src.model import GPT, GPTConfig\nTOKEN_MODE = 'pile' # char / pile\nif TOKEN_MODE == 'char':\n    MODEL_NAME = 'trained-1'\n    WORD_NAME = 'vocab'         # the .json vocab (generated by train.py)\n    ctx_len = 1024\n    n_layer = 6"
        },
        {
            "comment": "The code is initializing a model for the RWKV language model, specifically the \"RWKV-v4\" variant. It checks the TOKEN_MODE and sets up the tokenizer and model accordingly. The tokenizer's vocab_size is set to 50277 if in 'pile' mode, and the model is loaded from a specific .pth file. The model is also converted to fp16 or bf16 depending on the environment variable RWKV_FLOAT_MODE.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/verify.py\":26-56",
            "content": "    n_embd = 512\n    UNKNOWN_CHAR = ' '   # here we just set it to [space] for simplicity\nelif TOKEN_MODE == 'pile':\n    WORD_NAME = ['20B_tokenizer.json', '20B_tokenizer.json']\n    MODEL_NAME = 'RWKV-4-Pile-169M-20220807-8023'\n    ctx_len = 1024\n    n_layer = 12\n    n_embd = 768\n    UNKNOWN_CHAR = None\nmodel_type = 'RWKV'\nfrom src.utils import TOKENIZER\ntokenizer = TOKENIZER(WORD_NAME, UNKNOWN_CHAR=UNKNOWN_CHAR)\nif TOKEN_MODE == 'pile':\n    tokenizer.vocab_size = 50277\n########################################################################################################\nmodel_train = GPT(GPTConfig(tokenizer.vocab_size, ctx_len, model_type=model_type, n_layer=n_layer, n_embd=n_embd)).cuda()\nif os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n    model_train = model_train.half()\nelif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n    model_train = model_train.bfloat16()\nprint('loading ' + MODEL_NAME)\nm2 = torch.load(MODEL_NAME + '.pth', map_location=RUN_DEVICE)\nmodel_train.load_state_dict(m2)\nmodel_rnn = RWKV_RNN(MODEL_NAME, RUN_DEVICE, model_type, n_layer, n_embd, ctx_len)"
        },
        {
            "comment": "The code initializes an RWKV-GPT model, tokenizes input context in either character or pile mode, and prints the output of both RWKV-GPT and RWKV-RNN models for the given input.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/verify.py\":57-80",
            "content": "model_gpt = RWKV_GPT(MODEL_NAME, RUN_DEVICE, model_type, tokenizer.vocab_size, n_layer, n_embd, ctx_len).cuda()\n########################################################################################################\n# context = '\\nIn a'\ncontext = '\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.'\nif TOKEN_MODE == 'char':\n    ctx = [tokenizer.stoi.get(s, tokenizer.UNKNOWN_CHAR) for s in context]\nelif TOKEN_MODE == 'pile':\n    ctx = tokenizer.tokenizer.encode(context)\nprint(f'input len {len(ctx)} data {ctx}')\n########################################################################################################\nprint('\\nRWKV-GPT output')\nout = model_gpt.forward(torch.tensor(ctx).unsqueeze(0).cuda())[0].detach().cpu().numpy()\nprint(out)\nprint('\\nRWKV-RNN output')\nmodel_rnn.clear()\nsrc_len = len(ctx)\nfor i in range(src_len):\n    x = ctx[:i+1]"
        },
        {
            "comment": "This code snippet is checking the output of RWKV-v4 model at specific indices and then prints the RWKV-train output. It first runs the model_rnn on input x and checks if i (index) is less than 3 or greater than src\\_len - 3, printing the detached output to CPU numpy array. If i equals 2, it prints '...'. Then, it trains the model_train on cuda with ctx tensor, gets the forward output and prints it as float numpy array.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/verify.py\":81-89",
            "content": "    out = model_rnn.run(x)\n    if i < 3 or i >= src_len - 3:\n        print(torch.tensor(out).detach().cpu().numpy())\n    if i == 2:\n        print('...')\nprint('\\nRWKV-train output')\nout = model_train.forward(torch.tensor([ctx]).cuda())[0][0].detach().cpu().float().numpy()\nprint(out, '\\n')"
        }
    ]
}