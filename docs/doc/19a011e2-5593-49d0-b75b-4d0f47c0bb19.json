{
    "summary": "The RWKV Language Model employs CUDA support, time-shifted operations, and forward passes for efficient execution. It trains a transformer model with head-multihead attention using RWKV-v4, organizes weights in namespaced structure, and performs layer normalization, feed-forward operations, and applies layers like LN and FFN.",
    "details": [
        {
            "comment": "This code snippet is from the RWKV Language Model. It begins with importing necessary libraries and defines some constants like RWKV_HEAD_QK_DIM, DEBUG_TIME, and T_MAX. The code checks if the execution environment is CUDA and imports a CUDA kernel if it is. This language model is designed to perform natural language processing tasks with potential VRAM usage limitations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":0-24",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport types\nimport copy\nimport torch\nimport math, os\nfrom torch.nn import functional as F\nimport torch.nn as nn\nRWKV_HEAD_QK_DIM = 0\nprint(f'\\nRWKV_HEAD_QK_DIM {RWKV_HEAD_QK_DIM}\\n')\nDEBUG_TIME = False   # True False - show trained time-coeffs\n########################################################################################################\n# CUDA Kernel\n########################################################################################################\nif os.environ['RWKV_RUN_DEVICE'] == 'cuda':\n    T_MAX = 1024 # increase this if your ctx_len is long [NOTE: TAKES LOTS OF VRAM!]\n    # it's possible to go beyond CUDA limitations if you slice the ctx and pass the hidden state in each slice\n    from torch.utils.cpp_extension import load"
        },
        {
            "comment": "This code initializes a WKV object, loads the model from CUDA, and handles float mode conversion. It asserts certain conditions, such as T being less than or equal to a maximum value, and B*C being divisible by min(C, 1024). The code also checks the environment variable 'RWKV_FLOAT_MODE' and converts the float types accordingly for compatibility. The model parameters are stored in the ctx object for backward propagation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":25-47",
            "content": "    wkv_cuda = load(name=\"wkv\", sources=[\"cuda/wkv_op.cpp\", \"cuda/wkv_cuda.cu\"],\n                    verbose=True, extra_cuda_cflags=['-res-usage', '--maxrregcount 60', '--use_fast_math', '-O3', '-Xptxas -O3', f'-DTmax={T_MAX}'])\n    class WKV(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, B, T, C, w, u, k, v):\n            ctx.B = B\n            ctx.T = T\n            ctx.C = C\n            assert T <= T_MAX\n            assert B * C % min(C, 1024) == 0\n            if '32' in os.environ['RWKV_FLOAT_MODE']:\n                w = -torch.exp(w.contiguous())\n                u = u.contiguous()\n                k = k.contiguous()\n                v = v.contiguous()\n            else:\n                w = -torch.exp(w.float().contiguous())\n                u = u.float().contiguous()\n                k = k.float().contiguous()\n                v = v.float().contiguous()\n            ctx.save_for_backward(w, u, k, v)\n            y = torch.empty((B, T, C), device='cuda', memory_format=torch.contiguous_format)"
        },
        {
            "comment": "This code defines a function `forward` for a model that performs operations on input tensors B, T, C, w, u, k, and v. It also includes a backward function for gradient calculations using saved tensors. The `forward` function returns the output y, which is modified based on the environment variable RWKV_FLOAT_MODE. The backward function performs gradient calculations based on the input tensor gy and saves gradients in gw, gu, gk, and gv.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":48-71",
            "content": "            wkv_cuda.forward(B, T, C, w, u, k, v, y)\n            if '32' in os.environ['RWKV_FLOAT_MODE']:\n                return y\n            elif os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n                return y.half()\n            elif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n                return y.bfloat16()\n        @staticmethod\n        def backward(ctx, gy):\n            B = ctx.B\n            T = ctx.T\n            C = ctx.C\n            assert T <= T_MAX\n            assert B * C % min(C, 1024) == 0\n            w, u, k, v = ctx.saved_tensors\n            gw = torch.zeros((B, C), device='cuda').contiguous()\n            gu = torch.zeros((B, C), device='cuda').contiguous()\n            gk = torch.zeros((B, T, C), device='cuda').contiguous()\n            gv = torch.zeros((B, T, C), device='cuda').contiguous()\n            if '32' in os.environ['RWKV_FLOAT_MODE']:\n                wkv_cuda.backward(B, T, C, w, u, k, v, gy.contiguous(), gw, gu, gk, gv)\n            else:\n                wkv_cuda.backward(B, T, C, w, u, k, v, gy.float().contiguous(), gw, gu, gk, gv)"
        },
        {
            "comment": "This code snippet is part of a model training and inference process. It defines a function `RUN_CUDA` for running the model on CUDA devices, and initializes a module `RWKV_ChannelMix`. The code also sets up various tensor operations such as summations, and environment variable checks for floating point precision modes. The module is part of the RWKV language model framework.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":72-95",
            "content": "            gw = torch.sum(gw, dim=0)\n            gu = torch.sum(gu, dim=0)\n            if '32' in os.environ['RWKV_FLOAT_MODE']:\n                return (None, None, None, gw, gu, gk, gv)\n            elif os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n                return (None, None, None, gw.half(), gu.half(), gk.half(), gv.half())\n            elif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n                return (None, None, None, gw.bfloat16(), gu.bfloat16(), gk.bfloat16(), gv.bfloat16())\n    def RUN_CUDA(B, T, C, w, u, k, v):\n        return WKV.apply(B, T, C, w.cuda(), u.cuda(), k.cuda(), v.cuda())\n############################################################################################################\nRWKV_CFG = types.SimpleNamespace()\nclass RWKV_ChannelMix(nn.Module):\n    def __init__(self, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n        self.time_shift = nn.ZeroPad2d((0,0,1,-1))\n        self.time_mix_k = nn.Parameter(torch.ones(1, 1, RWKV_CFG.n_embd))\n        self.time_mix_r = nn.Parameter(torch.ones(1, 1, RWKV_CFG.n_embd))"
        },
        {
            "comment": "Class \"RWKV_TimeMix\" initializes with layer id, and contains parameters for time decay, first position correction, shift operation, key mix, and value mix. The forward function applies time shifting, mixes with key and value mix parameters, applies square and relu operations on keys, multiplies by sigmoid-transformed values, and returns the result.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":97-123",
            "content": "        hidden_sz = 4 * RWKV_CFG.n_embd\n        self.key = nn.Linear(RWKV_CFG.n_embd, hidden_sz, bias=False)\n        self.receptance = nn.Linear(RWKV_CFG.n_embd, RWKV_CFG.n_embd, bias=False)\n        self.value = nn.Linear(hidden_sz, RWKV_CFG.n_embd, bias=False)\n    def forward(self, x):\n        xx = self.time_shift(x)\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        k = self.key(xk)\n        k = torch.square(torch.relu(k))\n        kv = self.value(k)\n        rkv = torch.sigmoid(self.receptance(xr)) * kv\n        return rkv\nclass RWKV_TimeMix(nn.Module):\n    def __init__(self, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n        self.time_decay = nn.Parameter(torch.ones(RWKV_CFG.n_embd))\n        self.time_first = nn.Parameter(torch.ones(RWKV_CFG.n_embd) * math.log(0.3))\n        self.time_shift = nn.ZeroPad2d((0,0,1,-1))\n        self.time_mix_k = nn.Parameter(torch.ones(1,1,RWKV_CFG.n_embd))\n        self.time_mix_v = nn.Parameter(torch.ones(1,1,RWKV_CFG.n_embd))"
        },
        {
            "comment": "The code initializes a module with time-based mixing parameters and applies a series of linear transformations to the input. The forward function performs time-shifted operations, then multiplies with learned coefficients for key, value, and receptance components. The results are passed through a sigmoid activation, another layer normalization, and a final linear transformation before returning the final output. This block is part of the RWKV model implementation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":124-154",
            "content": "        self.time_mix_r = nn.Parameter(torch.ones(1,1,RWKV_CFG.n_embd))\n        self.key = nn.Linear(RWKV_CFG.n_embd, RWKV_CFG.n_embd, bias=False)\n        self.value = nn.Linear(RWKV_CFG.n_embd, RWKV_CFG.n_embd, bias=False)\n        self.receptance = nn.Linear(RWKV_CFG.n_embd, RWKV_CFG.n_embd, bias=False)\n        self.output = nn.Linear(RWKV_CFG.n_embd, RWKV_CFG.n_embd, bias=False)\n    def forward(self, x):\n        B, T, C = x.size()\n        xx = self.time_shift(x)\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        k = self.key(xk)\n        v = self.value(xv)\n        r = self.receptance(xr)\n        rwkv = torch.sigmoid(r) * RUN_CUDA(B, T, C, self.time_decay, self.time_first, k, v)\n        rwkv = self.output(rwkv)\n        return rwkv\nclass Block(nn.Module):\n    def __init__(self, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n        self.ln1 = nn.LayerNorm(RWKV_CFG.n_embd)"
        },
        {
            "comment": "This code initializes a RWKV-GPT model instance with specified configuration, and includes layer normalization and different forward pass depending on the layer ID and model type. The forward method performs layer normalization and adds the outputs of specific layers or modules, resulting in the final output.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":155-185",
            "content": "        self.ln2 = nn.LayerNorm(RWKV_CFG.n_embd)\n        if self.layer_id == 0:\n            self.ln0 = nn.LayerNorm(RWKV_CFG.n_embd)\n        if self.layer_id == 0 and RWKV_CFG.model_type == 'RWKV-ffnPre':\n            self.ffnPre = RWKV_ChannelMix(layer_id+1000)\n        else:\n            self.att = RWKV_TimeMix(layer_id)\n        self.ffn = RWKV_ChannelMix(layer_id)\n    def forward(self, x):\n        if self.layer_id == 0:\n            x = self.ln0(x)\n        if self.layer_id == 0 and RWKV_CFG.model_type == 'RWKV-ffnPre':\n            x = x + self.ffnPre(self.ln1(x))\n        else:\n            x = x + self.att(self.ln1(x))\n        x = x + self.ffn(self.ln2(x))\n        return x\nclass RWKV_GPT(nn.Module):\n    def __init__(self, MODEL_NAME, RUN_DEVICE, model_type, vocab_size, n_layer, n_embd, ctx_len):\n        global RWKV_CFG\n        super().__init__()\n        RWKV_CFG.RUN_DEVICE = RUN_DEVICE\n        RWKV_CFG.model_type = model_type\n        RWKV_CFG.vocab_size = vocab_size\n        RWKV_CFG.n_layer = n_layer\n        RWKV_CFG.n_embd = n_embd"
        },
        {
            "comment": "This code initializes an RWKV model, sets its context length, and loads the state dictionary from a saved file. It also includes optional head layers for QK vectors and a copy mechanism. The forward pass performs embedding, passes through blocks, applies layer normalization, and returns the output.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":186-216",
            "content": "        RWKV_CFG.ctx_len = ctx_len\n        print('\\nloading RWKV-GPT', MODEL_NAME)\n        self.emb = nn.Embedding(vocab_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(i) for i in range(n_layer)])\n        self.ln_out = nn.LayerNorm(n_embd)\n        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n        if RWKV_HEAD_QK_DIM > 0:\n            self.head_q = nn.Linear(n_embd, RWKV_HEAD_QK_DIM, bias=False)\n            self.head_q.scale_init = 0\n            self.head_k = nn.Linear(n_embd, RWKV_HEAD_QK_DIM, bias=False)\n            self.head_k.scale_init = 0.1\n            self.register_buffer(\"copy_mask\", torch.tril(\n                torch.ones(ctx_len, ctx_len)))\n        self.ctx_len = ctx_len\n        self.eval()\n        self.load_state_dict(torch.load(MODEL_NAME + '.pth'))\n        self.eval()\n    def forward(self, idx):\n        B, T = idx.size()\n        assert T <= self.ctx_len, \"Cannot forward, because len(input) > model ctx_len.\"\n        x = self.emb(idx)\n        x = self.blocks(x)\n        x = self.ln_out(x)"
        },
        {
            "comment": "This code segment is part of a model training process. It performs a head-multihead attention mechanism and then applies the result to the input. If RWKV_HEAD_QK_DIM is greater than 0, it calculates and applies the attention matrix, otherwise it simply passes through the input. The floating point mode is set based on the environment variable, with options for FP32, FP16, or BF16 precision. The class RWKV_RNN initializes a model with specified parameters like device, model type, number of layers, embedding size, and context length.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":218-241",
            "content": "        if RWKV_HEAD_QK_DIM > 0:\n            q = self.head_q(x)[:, :T, :]\n            k = self.head_k(x)[:, :T, :]\n            c = (q @ k.transpose(-2, -1)) * (1.0 / RWKV_HEAD_QK_DIM)\n            c = c.masked_fill(self.copy_mask[:T, :T] == 0, 0)\n            if '32' in os.environ['RWKV_FLOAT_MODE']:\n                c = c @ F.one_hot(idx, num_classes=RWKV_CFG.vocab_size)\n            elif os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n                c = c @ F.one_hot(idx, num_classes=RWKV_CFG.vocab_size).half()\n            elif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n                c = c @ F.one_hot(idx, num_classes=RWKV_CFG.vocab_size).bfloat16()\n            x = self.head(x) + c\n        else:\n            x = self.head(x)        \n        return x\n############################################################################################################\nclass RWKV_RNN(): # this is running in FP32 at this moment\n    def __init__(self, MODEL_NAME, RUN_DEVICE, model_type, n_layer, n_embd, ctx_len):\n        self.RUN_DEVICE = RUN_DEVICE"
        },
        {
            "comment": "This code loads a pre-trained RWKV-v4 model from a file and assigns the weights to the corresponding layers. It also performs some processing on time-related parameters, such as squeezing them and applying exponential decay. The code then organizes the loaded weights into a nested namespace structure based on their names, following a hierarchical pattern with layer numbers and layer types.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":242-271",
            "content": "        self.model_type = model_type\n        self.n_layer = n_layer\n        self.n_embd = n_embd\n        self.ctx_len = ctx_len\n        self.w = types.SimpleNamespace()\n        w = torch.load(MODEL_NAME + '.pth',\n                       map_location=torch.device(RUN_DEVICE))\n        for x in w.keys():\n            w[x] = w[x].float()\n            if '.time_' in x:\n                w[x] = w[x].squeeze()\n            if '.time_decay' in x:\n                w[x] = -torch.exp(w[x])\n            if DEBUG_TIME and '.time_' in x:\n                print(x, w[x].squeeze().cpu().numpy())\n            xx = x.split('.')\n            here = self.w\n            for i in range(len(xx)):\n                if xx[i].isdigit():\n                    ii = int(xx[i])\n                    if ii not in here:\n                        here[ii] = types.SimpleNamespace()\n                    here = here[ii]\n                else:\n                    if i == len(xx) - 1:\n                        setattr(here, xx[i], w[x])\n                    elif not hasattr(here, xx[i]):"
        },
        {
            "comment": "This code represents a class that can load, clear, and save various variables (xx, aa, bb, pp). It also contains methods to perform layer normalization (LN) and feed-forward operations (FF). The code uses the `getattr` function to dynamically access attributes based on input, and it initializes certain attributes as SimpleNamespace or empty dictionaries. The `clear`, `save`, and `load` functions are used to manage the state of the class variables.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":272-305",
            "content": "                        if xx[i+1].isdigit():\n                            setattr(here, xx[i], {})\n                        else:\n                            setattr(here, xx[i], types.SimpleNamespace())\n                    here = getattr(here, xx[i])\n        self.clear()\n    def clear(self):\n        self.xx = {}\n        self.aa = {}\n        self.bb = {}\n        self.pp = {}\n        self.hk = None\n    def save(self, target):\n        target.xx = copy.deepcopy(self.xx)\n        target.aa = copy.deepcopy(self.aa)\n        target.bb = copy.deepcopy(self.bb)\n        target.pp = copy.deepcopy(self.pp)\n        target.hk = copy.deepcopy(self.hk)\n    def load(self, target):\n        self.xx = copy.deepcopy(target.xx)\n        self.aa = copy.deepcopy(target.aa)\n        self.bb = copy.deepcopy(target.bb)\n        self.pp = copy.deepcopy(target.pp)\n        self.hk = copy.deepcopy(target.hk)\n    def LN(self, xx, w):\n        return F.layer_norm(xx, (self.n_embd,), weight=w.weight, bias=w.bias)\n    def FF(self, xx, w, name):\n        if name not in self.xx:"
        },
        {
            "comment": "Code is a part of an attention mechanism in a transformer model. It calculates the key, value and returns a weighted sum. The SA function initializes variables for each name.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":306-329",
            "content": "            self.xx[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n        xk = xx * w.time_mix_k + self.xx[name] * (1 - w.time_mix_k)\n        xr = xx * w.time_mix_r + self.xx[name] * (1 - w.time_mix_r)\n        self.xx[name] = xx\n        r = torch.sigmoid(w.receptance.weight @ xr)\n        k = torch.square(torch.relu(w.key.weight @ xk))\n        kv = w.value.weight @ k\n        return r * kv\n    def SA(self, xx, w, name):\n        if name not in self.xx:\n            self.xx[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n            self.aa[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n            self.bb[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n            self.pp[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE) - 1e30\n        xk = xx * w.time_mix_k + self.xx[name] * (1 - w.time_mix_k)\n        xv = xx * w.time_mix_v + self.xx[name] * (1 - w.time_mix_v)\n        xr = xx * w.time_mix_r + self.xx[name] * (1 - w.time_mix_r)\n        self.xx[name] = xx\n        r = torch.sigmoid(w.receptance.weight @ xr)"
        },
        {
            "comment": "Function defines the operation of a RWKV model. It applies layers like LN, FFN, and SA in a loop to transform input x. The function uses variables pp, aa, bb, and ww for intermediate calculations related to time-decaying weights and exponential operations. Output is the weighted sum of input x transformed by the applied layers.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":331-365",
            "content": "        k = w.key.weight @ xk\n        v = w.value.weight @ xv\n        pp = self.pp[name]\n        aa = self.aa[name]\n        bb = self.bb[name]\n        ww = w.time_first + k\n        p = torch.maximum(pp, ww)\n        e1 = torch.exp(pp - p)\n        e2 = torch.exp(ww - p)\n        a = e1 * aa + e2 * v\n        b = e1 * bb + e2\n        ww = pp + w.time_decay\n        p = torch.maximum(ww, k)\n        e1 = torch.exp(ww - p)\n        e2 = torch.exp(k - p)\n        self.aa[name] = e1 * aa + e2 * v\n        self.bb[name] = e1 * bb + e2\n        self.pp[name] = p\n        rwkv = r * a / b\n        return w.output.weight @ rwkv\n    def run(self, ctx):\n        w = self.w\n        x = w.emb.weight[ctx[-1]]\n        for i in range(self.n_layer):\n            if i == 0:\n                x = self.LN(x, w.blocks[i].ln0)\n            if i == 0 and self.model_type == 'RWKV-ffnPre':\n                x = x + self.FF(self.LN(x, w.blocks[i].ln1), w.blocks[i].ffnPre, f'ffnPre.{i}')\n            else:\n                x = x + self.SA(self.LN(x, w.blocks[i].ln1), w.blocks[i].att, f'att.{i}')"
        },
        {
            "comment": "This code applies a feed-forward network (FFN) and layer normalization (LN) to the input 'x' and updates it based on the context length ('ctx_len'). It also handles the case when RWKV_HEAD_QK_DIM is greater than 0, calculating the head key matrix ('hk') and updating 'x' accordingly. Finally, it returns the updated 'x'.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model_run.py\":366-391",
            "content": "            x = x + self.FF(self.LN(x, w.blocks[i].ln2), w.blocks[i].ffn, f'ffn.{i}')\n        x = self.LN(x, w.ln_out)\n        if RWKV_HEAD_QK_DIM > 0:\n            if self.hk == None:\n                self.hk = (w.head_k.weight @ x).unsqueeze(0)\n            else:\n                self.hk = torch.cat(\n                    [self.hk, (w.head_k.weight @ x).unsqueeze(0)], dim=0)\n            if self.hk.shape[0] > self.ctx_len:\n                self.hk = self.hk[-self.ctx_len:, :]\n            q = w.head_q.weight @ x\n            x = w.head.weight @ x\n            x = x.cpu().numpy().tolist()\n            c = (self.hk @ q) / RWKV_HEAD_QK_DIM\n            for i in range(len(c)):\n                x[ctx[i]] += c[i]\n        else:\n            x = w.head.weight @ x\n            x = x.cpu().numpy().tolist()\n        return x"
        }
    ]
}