{
    "summary": "The code initializes RWKV Language Model, uses Pytorch Lightning for training and handles command line arguments. It optimizes performance by loading checkpoints, handling exceptions, setting trainer parameters and using Deepspeed optimization.",
    "details": [
        {
            "comment": "This code initializes the RWKV Language Model, sets up logging, and parses command-line arguments for loading a model, using Wandb, project directory, random seed, and data file. It uses Pytorch Lightning framework for training.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":0-22",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport logging\nlogging.basicConfig(level=logging.INFO)\nif __name__ == \"__main__\":\n    from argparse import ArgumentParser\n    from pytorch_lightning import Trainer\n    from pytorch_lightning.utilities import rank_zero_info, rank_zero_only\n    import pytorch_lightning as pl\n    rank_zero_info(\"########## work in progress ##########\")\n    parser = ArgumentParser()\n    parser.add_argument(\"--load_model\", default=\"\", type=str)  # full path, with .pth\n    parser.add_argument(\"--wandb\", default=\"\", type=str)  # wandb project name. if \"\" then don't use wandb\n    parser.add_argument(\"--proj_dir\", default=\"out\", type=str)\n    parser.add_argument(\"--random_seed\", default=\"-1\", type=int)\n    parser.add_argument(\"--data_file\", default=\"\", type=str)"
        },
        {
            "comment": "This code snippet is for argument parsing in the RWKV-LM/RWKV-v5/train.py file. It sets default values and types for various training parameters such as data type, vocab size, context length, epoch steps, number of epochs, initial epoch, epoch save frequency, micro batch size, number of layers, embedding dimension, and attention dimensionality.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":23-35",
            "content": "    parser.add_argument(\"--data_type\", default=\"utf-8\", type=str)\n    parser.add_argument(\"--vocab_size\", default=0, type=int)  # vocab_size = 0 means auto (for char-level LM and .txt data)\n    parser.add_argument(\"--ctx_len\", default=1024, type=int)\n    parser.add_argument(\"--epoch_steps\", default=1000, type=int)  # a mini \"epoch\" has [epoch_steps] steps\n    parser.add_argument(\"--epoch_count\", default=500, type=int)  # train for this many \"epochs\". will continue afterwards with lr = lr_final\n    parser.add_argument(\"--epoch_begin\", default=0, type=int)  # if you load a model trained for x \"epochs\", set epoch_begin = x\n    parser.add_argument(\"--epoch_save\", default=5, type=int)  # save the model every [epoch_save] \"epochs\"\n    parser.add_argument(\"--micro_bsz\", default=12, type=int)  # micro batch size (batch size per GPU)\n    parser.add_argument(\"--n_layer\", default=6, type=int)\n    parser.add_argument(\"--n_embd\", default=512, type=int)\n    parser.add_argument(\"--dim_att\", default=0, type=int)"
        },
        {
            "comment": "This code snippet is from the RWKV-LM's \"train.py\" file and it sets various arguments for model training, such as dimensionality of feedforward network layers (dim_ffn), replacing first attention layer by a feedforward network (pre_ffn), and tricks like my headQK trick (head_qk). The code also specifies parameters for the tiny attention dimension (tiny_att_dim) and layer (tiny_att_layer), learning rate initialization and final values (lr_init, lr_final), warm-up steps, optimizer parameters (beta1, beta2), and Adam epsilon (adam_eps). There's also an argument for gradient checkpointing to save VRAM at the cost of increased training time (grad_cp).",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":36-48",
            "content": "    parser.add_argument(\"--dim_ffn\", default=0, type=int)\n    parser.add_argument(\"--pre_ffn\", default=0, type=int)  # replace first att layer by ffn (sometimes better)\n    parser.add_argument(\"--head_qk\", default=0, type=int)  # my headQK trick\n    parser.add_argument(\"--tiny_att_dim\", default=0, type=int)  # tiny attention dim\n    parser.add_argument(\"--tiny_att_layer\", default=-999, type=int)  # tiny attention @ which layer\n    parser.add_argument(\"--lr_init\", default=6e-4, type=float)  # 6e-4 for L12-D768, 4e-4 for L24-D1024, 3e-4 for L24-D2048\n    parser.add_argument(\"--lr_final\", default=1e-5, type=float)\n    parser.add_argument(\"--warmup_steps\", default=-1, type=int)  # try 50 if you load a model\n    parser.add_argument(\"--beta1\", default=0.9, type=float)\n    parser.add_argument(\"--beta2\", default=0.99, type=float)  # use 0.999 when your model is close to convergence\n    parser.add_argument(\"--adam_eps\", default=1e-8, type=float)\n    parser.add_argument(\"--grad_cp\", default=0, type=int)  # gradient checkpt: saves VRAM, but slower"
        },
        {
            "comment": "This code snippet is using argparse in Python to define various command-line arguments for a machine learning model. These arguments control features such as dropout rate, weight decay, and additional customizable settings like text shift, sample length, and more. Different values can be tried to optimize the performance of the model during training.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":49-62",
            "content": "    parser.add_argument(\"--dropout\", default=0, type=float) # try 0.01 / 0.02 / 0.05 / 0.1\n    parser.add_argument(\"--weight_decay\", default=0, type=float) # try 0.1 / 0.01 / 0.001\n    parser.add_argument(\"--weight_decay_final\", default=-1, type=float)\n    parser.add_argument(\"--my_pile_version\", default=1, type=int)  # my special pile version\n    parser.add_argument(\"--my_pile_stage\", default=0, type=int)  # my special pile mode\n    parser.add_argument(\"--my_pile_shift\", default=-1, type=int)  # my special pile mode - text shift\n    parser.add_argument(\"--my_pile_edecay\", default=0, type=int)\n    parser.add_argument(\"--layerwise_lr\", default=1, type=int)  # layerwise lr for faster convergence (but slower it/s)\n    parser.add_argument(\"--ds_bucket_mb\", default=200, type=int)  # deepspeed bucket size in MB. 200 seems enough\n    # parser.add_argument(\"--cuda_cleanup\", default=0, type=int)  # extra cuda cleanup (sometimes helpful)\n    parser.add_argument(\"--my_sample_len\", default=0, type=int)\n    parser.add_argument(\"--my_ffn_shift\", default=1, type=int)"
        },
        {
            "comment": "This code snippet adds command line arguments to a parser for various settings and configurations in the RWKV-v5 model training. It includes options like my_att_shift, head_size_a, head_size_divisor, my_pos_emb, load_partial, magic_prime, my_qa_mask, my_random_steps, my_testing, my_exit, my_exit_tokens, accelerator, strategy, devices and num_nodes. The code also checks if the Python version is 2, in which case it adds arguments for accelerator, strategy, devices and num_nodes.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":63-79",
            "content": "    parser.add_argument(\"--my_att_shift\", default=1, type=int)\n    parser.add_argument(\"--head_size_a\", default=64, type=int) # can try larger values for larger models\n    parser.add_argument(\"--head_size_divisor\", default=8, type=int)\n    parser.add_argument(\"--my_pos_emb\", default=0, type=int)\n    parser.add_argument(\"--load_partial\", default=0, type=int)\n    parser.add_argument(\"--magic_prime\", default=0, type=int)\n    parser.add_argument(\"--my_qa_mask\", default=0, type=int)\n    parser.add_argument(\"--my_random_steps\", default=0, type=int)\n    parser.add_argument(\"--my_testing\", default='', type=str)\n    parser.add_argument(\"--my_exit\", default=99999999, type=int)\n    parser.add_argument(\"--my_exit_tokens\", default=0, type=int)\n    if pl.__version__[0]=='2':\n        parser.add_argument(\"--accelerator\", default=\"gpu\", type=str)\n        parser.add_argument(\"--strategy\", default=\"auto\", type=str)\n        parser.add_argument(\"--devices\", default=1, type=int)\n        parser.add_argument(\"--num_nodes\", default=1, type=int)"
        },
        {
            "comment": "This code is adding arguments to the argument parser, handling global seed, setting numpy print options, and filtering warnings. The \"--precision\" argument sets the precision type to \"fp16\", and \"--accumulate_grad_batches\" determines the number of gradient accumulation batches. The code also imports necessary libraries and handles warnings related to workers and metric tracking.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":80-102",
            "content": "        parser.add_argument(\"--precision\", default=\"fp16\", type=str)\n        parser.add_argument(\"--accumulate_grad_batches\", default=1, type=int)\n    else:\n        parser = Trainer.add_argparse_args(parser)\n    args = parser.parse_args()\n    ########################################################################################################\n    import os, warnings, math, datetime, sys, time\n    import numpy as np\n    import torch\n    from torch.utils.data import DataLoader\n    if \"deepspeed\" in args.strategy:\n        import deepspeed\n    from pytorch_lightning import seed_everything\n    if args.random_seed >= 0:\n        print(f\"########## WARNING: GLOBAL SEED {args.random_seed} THIS WILL AFFECT MULTIGPU SAMPLING ##########\\n\" * 3)\n        seed_everything(args.random_seed)\n    np.set_printoptions(precision=4, suppress=True, linewidth=200)\n    warnings.filterwarnings(\"ignore\", \".*Consider increasing the value of the `num_workers` argument*\")\n    warnings.filterwarnings(\"ignore\", \".*The progress bar already tracks a metric with the*\")"
        },
        {
            "comment": "This code sets various arguments for a training script. It enables continuous training (`args.max_epochs = -1`), disables checkpointing and logging, and adjusts the batch size based on the number of nodes and devices. It also configures the dimensions of certain layers and sets specific environment variables for the training process.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":103-125",
            "content": "    # os.environ[\"WDS_SHOW_SEED\"] = \"1\"\n    args.my_timestamp = datetime.datetime.today().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    args.enable_checkpointing = False\n    args.replace_sampler_ddp = False\n    args.logger = False\n    args.gradient_clip_val = 1.0\n    args.num_sanity_val_steps = 0\n    args.check_val_every_n_epoch = int(1e20)\n    args.log_every_n_steps = int(1e20)\n    args.max_epochs = -1  # continue forever\n    args.betas = (args.beta1, args.beta2)\n    args.real_bsz = int(args.num_nodes) * int(args.devices) * args.micro_bsz\n    os.environ[\"RWKV_MY_TESTING\"] = args.my_testing\n    os.environ[\"RWKV_HEAD_SIZE_A\"] = str(args.head_size_a)\n    if args.dim_att <= 0:\n        args.dim_att = args.n_embd\n    if args.dim_ffn <= 0:\n        args.dim_ffn = int((args.n_embd * 3.5) // 32 * 32) # default = 3.5x emb size\n    if args.data_type == \"wds_img\":\n        args.run_name = f\"v{args.my_img_version}-{args.my_img_size}-{args.my_img_bit}bit-{args.my_img_clip}x{args.my_img_clip_scale}\"\n        args.proj_dir = f\"{args.proj_dir}-{args.run_name}\""
        },
        {
            "comment": "This code sets the run name based on certain parameters, creates a project directory if it doesn't exist, and adjusts the magic prime and epoch count for specific stages. It also ensures that the number of epoch steps and batch size are correctly set, and finds the latest saved model in the specified project directory.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":126-153",
            "content": "    else:\n        args.run_name = f\"{args.vocab_size} ctx{args.ctx_len} L{args.n_layer} D{args.n_embd}\"\n    if not os.path.exists(args.proj_dir):\n        os.makedirs(args.proj_dir)\n    if args.my_pile_stage > 0:\n        magic_prime_bak = args.magic_prime\n        if args.my_pile_shift < 0:\n            args.my_pile_shift = 0\n        if magic_prime_bak > 0:\n            args.magic_prime = magic_prime_bak\n        if args.my_qa_mask == 2:\n            args.epoch_count = 2 * args.magic_prime // 40320\n        else:\n            args.epoch_count = args.magic_prime // 40320\n        args.epoch_steps = 40320 // args.real_bsz\n        assert args.epoch_steps * args.real_bsz == 40320\n        # if args.my_pile_stage == 2:\n        #     assert args.lr_final == args.lr_init\n        if args.my_pile_stage >= 2:  # find latest saved model\n            list_p = []\n            for p in os.listdir(args.proj_dir):\n                if p.startswith(\"rwkv\") and p.endswith(\".pth\"):\n                    p = ((p.split(\"-\"))[1].split(\".\"))[0]\n                    if p != \"final\":"
        },
        {
            "comment": "The code retrieves the maximum value from a list of integers and uses it to determine which model checkpoint file (rwkv-{max_p}.pth or rwkv-init.pth) to load, depending on whether the max value is -1 or not. It also calculates the number of samples and tokens for an epoch based on the provided arguments. The code attempts to retrieve the DeepSpeed version but handles exceptions if it fails.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":154-181",
            "content": "                        if p == \"init\":\n                            p = -1\n                        else:\n                            p = int(p)\n                        list_p += [p]\n            list_p.sort()\n            max_p = list_p[-1]\n            if len(list_p) > 1:\n                args.my_pile_prev_p = list_p[-2]  # in case max_p is corrupted\n            if max_p == -1:\n                args.load_model = f\"{args.proj_dir}/rwkv-init.pth\"\n            else:\n                args.load_model = f\"{args.proj_dir}/rwkv-{max_p}.pth\"\n                if args.warmup_steps < 0:\n                    if args.my_pile_stage == 2:\n                        args.warmup_steps = 10\n                    else:\n                        args.warmup_steps = 30\n            args.epoch_begin = max_p + 1\n    samples_per_epoch = args.epoch_steps * args.real_bsz\n    tokens_per_epoch = samples_per_epoch * args.ctx_len\n    try:\n        deepspeed_version = deepspeed.__version__\n    except:\n        deepspeed_version = None\n        pass\n    rank_zero_info("
        },
        {
            "comment": "This code block is displaying various configuration details of the RWKV-5 model, including the precision, number of nodes and devices used, batch size, data file and project directory. It also mentions the epoch range, saving frequency, steps per epoch, model architecture, learning rate schedule, Adam optimizer settings, and version information for Torch, Deepspeed, and PyTorch Lightning. The recommendation section advises using specific versions of these libraries for optimal performance.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":182-199",
            "content": "        f\"\"\"\n############################################################################\n#\n# RWKV-5 {args.precision.upper()} on {args.num_nodes}x{args.devices} {args.accelerator.upper()}, bsz {args.num_nodes}x{args.devices}x{args.micro_bsz}={args.real_bsz}, {args.strategy} {'with grad_cp' if args.grad_cp > 0 else ''}\n#\n# Data = {args.data_file} ({args.data_type}), ProjDir = {args.proj_dir}\n#\n# Epoch = {args.epoch_begin} to {args.epoch_begin + args.epoch_count - 1} (will continue afterwards), save every {args.epoch_save} epoch\n#\n# Each \"epoch\" = {args.epoch_steps} steps, {samples_per_epoch} samples, {tokens_per_epoch} tokens\n#\n# Model = {args.n_layer} n_layer, {args.n_embd} n_embd, {args.ctx_len} ctx_len\n#\n# Adam = lr {args.lr_init} to {args.lr_final}, warmup {args.warmup_steps} steps, beta {args.betas}, eps {args.adam_eps}\n#\n# Found torch {torch.__version__}, recommend 1.13.1+cu117 or newer\n# Found deepspeed {deepspeed_version}, recommend 0.7.0 (faster than newer versions)\n# Found pytorch_lightning {pl.__version__}, recommend 1.9.5"
        },
        {
            "comment": "This code segment sets up the training environment for the RWKV-v5 model. It checks the arguments provided, ensures correct data type and precision, handles special learning rate cases, and sets up appropriate configurations for faster and stable training. It also provides informative notes if using potentially slower or less stable precisions. Additionally, it enables CUDA features for improved performance.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":200-225",
            "content": "#\n############################################################################\n\"\"\"\n    )\n    rank_zero_info(str(vars(args)) + \"\\n\")\n    assert args.data_type in [\"utf-8\", \"utf-16le\", \"numpy\", \"binidx\", \"dummy\", \"uint16\"]\n    if args.lr_final == 0 or args.lr_init == 0:\n        rank_zero_info(\"\\n\\nNote: lr_final = 0 or lr_init = 0. Using linear LR schedule instead.\\n\\n\")\n    assert args.precision in [\"fp32\", \"tf32\", \"fp16\", \"bf16\"]\n    os.environ[\"RWKV_FLOAT_MODE\"] = args.precision\n    if args.precision == \"fp32\":\n        for i in range(10):\n            rank_zero_info(\"\\n\\nNote: you are using fp32 (very slow). Try bf16 / tf32 for faster training.\\n\\n\")\n    if args.precision == \"fp16\":\n        rank_zero_info(\"\\n\\nNote: you are using fp16 (might overflow). Try bf16 / tf32 for stable training.\\n\\n\")\n    os.environ[\"RWKV_JIT_ON\"] = \"1\"\n    if \"deepspeed_stage_3\" in args.strategy:\n        os.environ[\"RWKV_JIT_ON\"] = \"0\"\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.enabled = True\n    if args.precision == \"fp32\":"
        },
        {
            "comment": "This code snippet is setting up the model training environment. It sets the CUDA backend allowances for TF32 and checks the precision argument (32, fp16, or bf16). It imports necessary modules like `train_callback`, `MyDataset` and `RWKV`. The code initializes a dataset instance, sets the vocab size based on it. Then it creates an RWKV model instance. If there's no pre-existing load model or if it's at the first stage of MyPile, it generates initial weights using `generate_init_weight`, saves them to a file and uses that file as the load model. Finally, it prints a status message about loading the specified model.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":226-255",
            "content": "        torch.backends.cudnn.allow_tf32 = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n    else:\n        torch.backends.cudnn.allow_tf32 = True\n        torch.backends.cuda.matmul.allow_tf32 = True\n    if \"32\" in args.precision:\n        args.precision = 32\n    elif args.precision == \"fp16\":\n        args.precision = 16\n    else:\n        args.precision = \"bf16\"\n    ########################################################################################################\n    from src.trainer import train_callback, generate_init_weight\n    from src.dataset import MyDataset\n    train_data = MyDataset(args)\n    args.vocab_size = train_data.vocab_size\n    from src.model import RWKV\n    model = RWKV(args)\n    if len(args.load_model) == 0 or args.my_pile_stage == 1:  # shall we build the initial weights?\n        init_weight_name = f\"{args.proj_dir}/rwkv-init.pth\"\n        generate_init_weight(model, init_weight_name)  # save initial weights\n        args.load_model = init_weight_name\n    rank_zero_info(f\"########## Loading {args.load_model}... ##########\")"
        },
        {
            "comment": "This code attempts to load a checkpoint model from the specified file. It handles exceptions if the checkpoint is invalid and allows for loading partial models. If an issue occurs, it provides information about the bad checkpoint and tries again with a different one. The code also removes unnecessary keys starting with \"_forward_module.\"",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":256-279",
            "content": "    try:\n        load_dict = torch.load(args.load_model, map_location=\"cpu\")\n        load_keys = list(load_dict.keys())\n        for k in load_keys:\n            if k.startswith('_forward_module.'):\n                load_dict[k.replace('_forward_module.','')] = load_dict[k]\n                del load_dict[k]\n    except:\n        rank_zero_info(f\"Bad checkpoint {args.load_model}\")\n        if args.my_pile_stage >= 2:  # try again using another checkpoint\n            max_p = args.my_pile_prev_p\n            if max_p == -1:\n                args.load_model = f\"{args.proj_dir}/rwkv-init.pth\"\n            else:\n                args.load_model = f\"{args.proj_dir}/rwkv-{max_p}.pth\"\n            args.epoch_begin = max_p + 1\n            rank_zero_info(f\"Trying {args.load_model}\")\n            load_dict = torch.load(args.load_model, map_location=\"cpu\")\n    if args.load_partial == 1:\n        load_keys = load_dict.keys()\n        for k in model.state_dict():\n            if k not in load_keys:\n                load_dict[k] = model.state_dict()[k]"
        },
        {
            "comment": "The code is creating a trainer object for PyTorch Lightning, depending on the version of PyTorch. It loads the state dictionary into the model and initializes the trainer with provided arguments like accelerator, strategy, devices, etc. The code also checks the shape of parameters in the model's state_dict and prints them if the shape has more than one element.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":280-298",
            "content": "    model.load_state_dict(load_dict)\n    if pl.__version__[0]=='2':\n        trainer = Trainer(accelerator=args.accelerator,strategy=args.strategy,devices=args.devices,num_nodes=args.num_nodes,precision=args.precision,\n        logger=args.logger,callbacks=[train_callback(args)],max_epochs=args.max_epochs,check_val_every_n_epoch=args.check_val_every_n_epoch,num_sanity_val_steps=args.num_sanity_val_steps,\n        log_every_n_steps=args.log_every_n_steps,enable_checkpointing=args.enable_checkpointing,accumulate_grad_batches=args.accumulate_grad_batches,gradient_clip_val=args.gradient_clip_val)\n    else:\n        trainer = Trainer.from_argparse_args(\n            args,\n            callbacks=[train_callback(args)],\n        )\n    if trainer.global_rank == 0:\n        for n in model.state_dict():\n            shape = model.state_dict()[n].shape\n            shape = [i for i in shape if i != 1]\n            if len(shape) > 1:\n                print(f\"{str(shape[0]).ljust(5)} {str(shape[1]).ljust(5)} {n}\")\n            else:"
        },
        {
            "comment": "This code prints the shape and number of training samples, configures Deepspeed optimization settings, sets up a DataLoader with specified parameters, and trains the model using the Deepspeed trainer.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/train.py\":299-308",
            "content": "                print(f\"{str(shape[0]).ljust(5)}       {n}\")\n    if \"deepspeed\" in args.strategy:\n        trainer.strategy.config[\"zero_optimization\"][\"allgather_bucket_size\"] = args.ds_bucket_mb * 1000 * 1000\n        trainer.strategy.config[\"zero_optimization\"][\"reduce_bucket_size\"] = args.ds_bucket_mb * 1000 * 1000\n    # must set shuffle=False, persistent_workers=False (because worker is in another thread)\n    data_loader = DataLoader(train_data, shuffle=False, pin_memory=True, batch_size=args.micro_bsz, num_workers=1, persistent_workers=False, drop_last=True)\n    trainer.fit(model, data_loader)"
        }
    ]
}