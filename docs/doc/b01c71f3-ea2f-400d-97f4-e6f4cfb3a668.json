{
    "summary": "The code introduces RWKV, a deep learning NLP model with tokenization classes and RWKV_RNN functions. It initializes the model, loads checkpoints, optimizes performance using layer norm, time-dependent mixing, and sigmoid activation functions, generates context, predicts next character based on model output, and uses layer norm and time-mixing operations for processing.",
    "details": [
        {
            "comment": "This code is a part of the RWKV Language Model, which is a deep learning model for natural language processing. The code snippet defines an equation and tokenizes it using a word table to map numbers and symbols to corresponding strings. It also initializes an object with a word_table dictionary and sets up arguments such as MODEL_NAME, n_layer, and n_embd.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/math_demo/run.py\":0-24",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport numpy as np\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)\nimport types, torch\nfrom torch.nn import functional as F\n# only + - *\nequation = \"4.2379*564.778-1209.01\" # 1184.4626862\n# equation = \"32731423*2189286\" # 71658446133978\n# equation = \"18239.715*9.728263\" # 177440.744565045\n# equation = \"2067*9832*4549\" # 92448162456\n########################################################################################################\nargs = types.SimpleNamespace()\nargs.MODEL_NAME = 'rwkv-200'\nargs.n_layer = 6\nargs.n_embd = 192\nclass TOKENIZER():\n    def __init__(self):\n        self.word_table = {\"0\": \"\\n\", \"1\": \" \", \"2\": \"(\", \"3\": \")\", \"4\": \"*\", \"5\": \"+\", \"6\": \"-\", \"7\": \".\", \"8\": \"0\", \"9\": \"1\", \"10\": \"2\", \"11\": \"3\", \"12\": \"4\", \"13\": \"5\", \"14\": \"6\", \"15\": \"7\", \"16\": \"8\", \"17\": \"9\", \"18\": \"=\", \"19\": \"e\", \"20\": \"f\"}"
        },
        {
            "comment": "This code defines a class for RWKV tokenization and another class for RWKV_RNN. The RWKV_RNN class initializes with arguments, loads the model from a checkpoint file, and adjusts some parameters as needed. It sets the model to inference mode using eval() function and converts certain parameters to floating point type. The code also includes functions for tokenization: encode and decode which convert tokens to integers and integers back to tokens respectively.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/math_demo/run.py\":25-52",
            "content": "        self.vocab_size = len(self.word_table)\n        self.stoi = {v: int(k) for k, v in self.word_table.items()}\n        self.itos = {int(k): v for k, v in self.word_table.items()}\n    def encode(self, x):\n        return [self.stoi[t] for t in x]\n    def decode(self, x):\n        return ''.join([self.itos[t] for t in x])\ntokenizer = TOKENIZER()\n########################################################################################################\nclass RWKV_RNN(torch.jit.ScriptModule):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.eval() # set torch to inference mode\n        w = torch.load(args.MODEL_NAME + '.pth', map_location='cpu')\n        for k in w.keys():\n            if      '.time_' in k: w[k] = w[k].squeeze()\n            if '.time_decay' in k: w[k] = -torch.exp(w[k].float()) # the real time decay is like e^{-e^x}\n            else: w[k] = w[k].float() # convert to f32 type\n        self.w = types.SimpleNamespace() # set self.w from w\n        self.w.blocks = {}"
        },
        {
            "comment": "This code is defining a function called \"channel_mixing\" which performs channel-wise mixing and normalization on input tensor \"x\". It uses layer norm, time-wise mixing, and sigmoid activation functions. The function also updates the state variable for future iterations. The code organizes the weights in a specific way and utilizes torch script method to optimize performance.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/math_demo/run.py\":53-76",
            "content": "        for k in w.keys(): # example: \"blocks.0.att.time_first\" => self.w.blocks[0].att.time_first\n            parts = k.split('.')\n            last = parts.pop()\n            here = self.w\n            for p in parts:\n                if p.isdigit():\n                    p = int(p)\n                    if p not in here: here[p] = types.SimpleNamespace()\n                    here = here[p]\n                else:\n                    if not hasattr(here, p): setattr(here, p, types.SimpleNamespace())\n                    here = getattr(here, p)\n            setattr(here, last, w[k])\n    def layer_norm(self, x, w):\n        return F.layer_norm(x, (self.args.n_embd,), weight=w.weight, bias=w.bias)\n    @torch.jit.script_method\n    def channel_mixing(self, x, state, i:int, time_mix_k, time_mix_r, kw, vw, rw):\n        xk = x * time_mix_k + state[5*i+0] * (1 - time_mix_k)\n        xr = x * time_mix_r + state[5*i+0] * (1 - time_mix_r)\n        state[5*i+0] = x\n        r = torch.sigmoid(rw @ xr)\n        k = torch.square(torch.relu(kw @ xk)) # square relu, primer paper"
        },
        {
            "comment": "This code performs time-dependent mixing and computes the output for each step of a recurrent neural network. The `time_mixing` method updates hidden states, applies transformations based on time constants, and calculates the weighted sum of inputs using dot products. The `forward` method takes input tokens and current state as arguments to perform forward pass calculations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/math_demo/run.py\":77-108",
            "content": "        return r * (vw @ k)\n    @torch.jit.script_method\n    def time_mixing(self, x, state, i:int, time_mix_k, time_mix_v, time_mix_r, time_first, time_decay, kw, vw, rw, ow):\n        xk = x * time_mix_k + state[5*i+1] * (1 - time_mix_k)\n        xv = x * time_mix_v + state[5*i+1] * (1 - time_mix_v)\n        xr = x * time_mix_r + state[5*i+1] * (1 - time_mix_r)\n        state[5*i+1] = x\n        r = torch.sigmoid(rw @ xr)\n        k = kw @ xk\n        v = vw @ xv\n        aa = state[5*i+2]\n        bb = state[5*i+3]\n        pp = state[5*i+4]\n        ww = time_first + k\n        qq = torch.maximum(pp, ww)\n        e1 = torch.exp(pp - qq)\n        e2 = torch.exp(ww - qq)\n        a = e1 * aa + e2 * v\n        b = e1 * bb + e2\n        wkv = a / b\n        ww = pp + time_decay\n        qq = torch.maximum(ww, k)\n        e1 = torch.exp(ww - qq)\n        e2 = torch.exp(k - qq)\n        state[5*i+2] = e1 * aa + e2 * v\n        state[5*i+3] = e1 * bb + e2\n        state[5*i+4] = qq\n        return ow @ (r * wkv)\n    def forward(self, token, state):"
        },
        {
            "comment": "Iterates over layers, applies time-mixing and channel-mixing operations, layer norm, and final weighted operation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/math_demo/run.py\":109-126",
            "content": "        with torch.no_grad():\n            if state == None:\n                state = torch.zeros(self.args.n_layer * 5, self.args.n_embd)\n                for i in range(self.args.n_layer): state[5*i+4] = -1e30 # -infinity\n            x = self.w.emb.weight[token]\n            x = self.layer_norm(x, self.w.blocks[0].ln0)\n            for i in range(self.args.n_layer):\n                att = self.w.blocks[i].att\n                x = x + self.time_mixing(self.layer_norm(x, self.w.blocks[i].ln1), state, i, \n                    att.time_mix_k, att.time_mix_v, att.time_mix_r, att.time_first, att.time_decay, \n                    att.key.weight, att.value.weight, att.receptance.weight, att.output.weight)\n                ffn = self.w.blocks[i].ffn\n                x = x + self.channel_mixing(self.layer_norm(x, self.w.blocks[i].ln2), state, i, \n                    ffn.time_mix_k, ffn.time_mix_r, \n                    ffn.key.weight, ffn.value.weight, ffn.receptance.weight)\n            x = self.w.head.weight @ self.layer_norm(x, self.w.ln_out)"
        },
        {
            "comment": "This code loads an RWKV_RNN model, generates context using provided equation and tokenizes it. It then iterates through tokens, predicting the next character based on the model's output, until a newline is predicted.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/math_demo/run.py\":127-149",
            "content": "            return x.float(), state\n##########################################################################################################\nprint(f'\\nUsing CPU. Loading {args.MODEL_NAME} ...')\nmodel = RWKV_RNN(args)\ncontext = \"\\n\" + equation.strip().replace(' ','') + \"=\"\nprint(context, f'(python answer {eval(equation)})')\nstate = None\nfor token in tokenizer.encode(context):\n    out, state = model.forward(token, state)\nfor i in range(4096):\n    token = int(torch.argmax(out))\n    tmp = tokenizer.decode([token])\n    print(tmp, end=\"\", flush=True)\n    if tmp == '\\n':\n        break\n    out, state = model.forward(token, state)   \nprint()"
        }
    ]
}