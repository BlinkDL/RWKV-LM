{
    "summary": "This code imports libraries and defines a function ToBinary for converting images to binary format. It includes an R_ENCODER class with layers for image processing and backward operation, as well as forward pass and neural network class for image decoding. The code also defines a neural network model for image generation using convolution layers and Mish activation.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines a function called ToBinary for converting input images to binary format. It uses RWKV Language Model and provides model_prefix and input_img variables for further processing.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/img_demoAE.py\":0-24",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport torch, types, os\nimport numpy as np\nfrom PIL import Image\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torchvision as vision\nimport torchvision.transforms as transforms\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)\nprint(f'loading...')\n########################################################################################################\nmodel_prefix = 'test/image_trained/out-v7c_d8_256-224-13bit-OB32x0.5-201'\ninput_img = 'test/img_ae_test/test0.png'\n########################################################################################################\nclass ToBinary(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        return torch.floor(x + 0.5) # no need for noise when we have plenty of data"
        },
        {
            "comment": "This code defines a class named R_ENCODER inheriting from nn.Module, which includes several convolutional and batch normalization layers for image processing or feature extraction. The backward function is defined as a pass-through operation for gradient computation during backpropagation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/img_demoAE.py\":26-51",
            "content": "    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.clone() # pass-through\nclass R_ENCODER(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        dd = 8\n        self.Bxx = nn.BatchNorm2d(dd*64)\n        self.CIN = nn.Conv2d(3, dd, kernel_size=3, padding=1)\n        self.Cx0 = nn.Conv2d(dd, 32, kernel_size=3, padding=1)\n        self.Cx1 = nn.Conv2d(32, dd, kernel_size=3, padding=1)\n        self.B00 = nn.BatchNorm2d(dd*4)\n        self.C00 = nn.Conv2d(dd*4, 256, kernel_size=3, padding=1)\n        self.C01 = nn.Conv2d(256, dd*4, kernel_size=3, padding=1)\n        self.C02 = nn.Conv2d(dd*4, 256, kernel_size=3, padding=1)\n        self.C03 = nn.Conv2d(256, dd*4, kernel_size=3, padding=1)\n        self.B10 = nn.BatchNorm2d(dd*16)\n        self.C10 = nn.Conv2d(dd*16, 256, kernel_size=3, padding=1)\n        self.C11 = nn.Conv2d(256, dd*16, kernel_size=3, padding=1)\n        self.C12 = nn.Conv2d(dd*16, 256, kernel_size=3, padding=1)\n        self.C13 = nn.Conv2d(256, dd*16, kernel_size=3, padding=1)"
        },
        {
            "comment": "This code defines a forward pass function for a neural network layer. It applies various convolutions and batch normalization to input image 'img' after passing it through several activation functions, including Mish. The final result is passed through a sigmoid function before being returned.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/img_demoAE.py\":53-81",
            "content": "        self.B20 = nn.BatchNorm2d(dd*64)\n        self.C20 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        self.C21 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)\n        self.C22 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        self.C23 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)\n        self.COUT = nn.Conv2d(dd*64, args.my_img_bit, kernel_size=3, padding=1)\n    def forward(self, img):\n        ACT = F.mish\n        x = self.CIN(img)\n        xx = self.Bxx(F.pixel_unshuffle(x, 8))\n        x = x + self.Cx1(ACT(self.Cx0(x)))\n        x = F.pixel_unshuffle(x, 2)\n        x = x + self.C01(ACT(self.C00(ACT(self.B00(x)))))\n        x = x + self.C03(ACT(self.C02(x)))\n        x = F.pixel_unshuffle(x, 2)\n        x = x + self.C11(ACT(self.C10(ACT(self.B10(x)))))\n        x = x + self.C13(ACT(self.C12(x)))\n        x = F.pixel_unshuffle(x, 2)\n        x = x + self.C21(ACT(self.C20(ACT(self.B20(x)))))\n        x = x + self.C23(ACT(self.C22(x)))\n        x = self.COUT(x + xx)\n        return torch.sigmoid(x)"
        },
        {
            "comment": "This code defines a class \"R_DECODER\" that inherits from the PyTorch's `nn.Module` and consists of multiple convolutional layers and batch normalization layers for image decoding. The class takes an argument \"args\", which contains information such as the number of image channels, and the kernel size of convolutions.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/img_demoAE.py\":83-105",
            "content": "class R_DECODER(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        dd = 8\n        self.CIN = nn.Conv2d(args.my_img_bit, dd*64, kernel_size=3, padding=1)\n        self.B00 = nn.BatchNorm2d(dd*64)\n        self.C00 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        self.C01 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)\n        self.C02 = nn.Conv2d(dd*64, 256, kernel_size=3, padding=1)\n        self.C03 = nn.Conv2d(256, dd*64, kernel_size=3, padding=1)\n        self.B10 = nn.BatchNorm2d(dd*16)\n        self.C10 = nn.Conv2d(dd*16, 256, kernel_size=3, padding=1)\n        self.C11 = nn.Conv2d(256, dd*16, kernel_size=3, padding=1)\n        self.C12 = nn.Conv2d(dd*16, 256, kernel_size=3, padding=1)\n        self.C13 = nn.Conv2d(256, dd*16, kernel_size=3, padding=1)\n        self.B20 = nn.BatchNorm2d(dd*4)\n        self.C20 = nn.Conv2d(dd*4, 256, kernel_size=3, padding=1)\n        self.C21 = nn.Conv2d(256, dd*4, kernel_size=3, padding=1)\n        self.C22 = nn.Conv2d(dd*4, 256, kernel_size=3, padding=1)"
        },
        {
            "comment": "This code defines a neural network model for image generation. It has multiple convolution layers and uses the Mish activation function. The model takes an input code, performs several operations with different convolution layers and pixel shuffling, and outputs a final tensor. The code also builds the model using given arguments.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/img_demoAE.py\":106-138",
            "content": "        self.C23 = nn.Conv2d(256, dd*4, kernel_size=3, padding=1)\n        self.Cx0 = nn.Conv2d(dd, 32, kernel_size=3, padding=1)\n        self.Cx1 = nn.Conv2d(32, dd, kernel_size=3, padding=1)\n        self.COUT = nn.Conv2d(dd, 3, kernel_size=3, padding=1)\n    def forward(self, code):\n        ACT = F.mish\n        x = self.CIN(code)\n        x = x + self.C01(ACT(self.C00(ACT(self.B00(x)))))\n        x = x + self.C03(ACT(self.C02(x)))\n        x = F.pixel_shuffle(x, 2)\n        x = x + self.C11(ACT(self.C10(ACT(self.B10(x)))))\n        x = x + self.C13(ACT(self.C12(x)))\n        x = F.pixel_shuffle(x, 2)\n        x = x + self.C21(ACT(self.C20(ACT(self.B20(x)))))\n        x = x + self.C23(ACT(self.C22(x)))\n        x = F.pixel_shuffle(x, 2)\n        x = x + self.Cx1(ACT(self.Cx0(x)))\n        x = self.COUT(x)\n        return torch.sigmoid(x)\n########################################################################################################\nprint(f'building model...')\nargs = types.SimpleNamespace()\nargs.my_img_bit = 13\nencoder = R_ENCODER(args).eval().cuda()"
        },
        {
            "comment": "The code is loading a pre-trained encoder and decoder model, applying image transformations, and generating an output image using the decoder. It then saves the output image in a specific format. The code also prints the shape and values of a tensor after performing some operations on it.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/img_demoAE.py\":139-164",
            "content": "decoder = R_DECODER(args).eval().cuda()\nzpow = torch.tensor([2**i for i in range(0,13)]).reshape(13,1,1).cuda().long()\nencoder.load_state_dict(torch.load(f'{model_prefix}-E.pth'))\ndecoder.load_state_dict(torch.load(f'{model_prefix}-D.pth'))\n########################################################################################################\nprint(f'test image...')\nimg_transform = transforms.Compose([\n    transforms.PILToTensor(),\n    transforms.ConvertImageDtype(torch.float),\n    transforms.Resize((224, 224))\n])\nwith torch.no_grad():\n    img = img_transform(Image.open(input_img)).unsqueeze(0).cuda()\n    z = encoder(img)\n    z = ToBinary.apply(z)\n    zz = torch.sum(z.squeeze().long() * zpow, dim=0)\n    print(f'Code shape = {zz.shape}\\n{zz.cpu().numpy()}\\n')\n    out = decoder(z)\n    vision.utils.save_image(out, f\"{input_img.split('.')[0]}-out-13bit.jpg\")"
        }
    ]
}