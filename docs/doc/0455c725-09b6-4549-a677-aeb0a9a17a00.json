{
    "summary": "The code initializes a language generation model class using layer normalization and attention, applying iterative layers with time decay to generate future sequence tokens. A context vector is calculated via matrix multiplications and added to the input list at corresponding indices.",
    "details": [
        {
            "comment": "The code initializes a RWKV_RNN class. It takes input parameters such as MODEL_NAME, RUN_DEVICE, model_type, n_layer, n_embd, and ctx_len. The class loads the weights from a .pth file located at MODEL_NAME+'.pth' using map_location set to RUN_DEVICE. It performs certain operations on the loaded weights for time-coefficients and assigns them to corresponding attributes within the class. If DEBUG_TIME is True, it prints the updated time coefficients.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model_run.py\":0-36",
            "content": "import types\nimport copy\nimport torch\nfrom torch.nn import functional as F\nRWKV_K_CLAMP = 60\nRWKV_K_EPS = 1e-16\nRWKV_HEAD_QK_DIM = 256\nDEBUG_TIME = False   # True False - show trained time-coeffs\nclass RWKV_RNN():\n    def __init__(self, MODEL_NAME, RUN_DEVICE, model_type, n_layer, n_embd, ctx_len):\n        self.RUN_DEVICE = RUN_DEVICE\n        self.model_type = model_type\n        self.n_layer = n_layer\n        self.n_embd = n_embd\n        self.ctx_len = ctx_len\n        self.w = types.SimpleNamespace()\n        w = torch.load(MODEL_NAME + '.pth',\n                       map_location=torch.device(RUN_DEVICE))\n        for x in w.keys():\n            if '.time_' in x:\n                w[x] = w[x].squeeze()\n            if '.time_decay' in x:\n                w[x] = torch.exp(-torch.exp(w[x]))\n            if '.time_first' in x:\n                w[x] = torch.exp(w[x])\n            if DEBUG_TIME and '.time_' in x:\n                print(x, w[x].squeeze().cpu().numpy())\n            xx = x.split('.')\n            here = self.w\n            for i in range(len(xx)):"
        },
        {
            "comment": "This code represents a data structure that allows for nested dictionary-like storage with optional object types. It includes functions to clear, save, and load the data structure. The `clear` function resets all stored variables to empty states, while `save` and `load` allow for copying state between instances of this data structure. The code uses a combination of dictionaries and simple namespaces to handle various types of values within the storage.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model_run.py\":37-68",
            "content": "                if xx[i].isdigit():\n                    ii = int(xx[i])\n                    if ii not in here:\n                        here[ii] = types.SimpleNamespace()\n                    here = here[ii]\n                else:\n                    if i == len(xx) - 1:\n                        setattr(here, xx[i], w[x])\n                    elif not hasattr(here, xx[i]):\n                        if xx[i+1].isdigit():\n                            setattr(here, xx[i], {})\n                        else:\n                            setattr(here, xx[i], types.SimpleNamespace())\n                    here = getattr(here, xx[i])\n        self.clear()\n    def clear(self):\n        self.xx = {}\n        self.aa = {}\n        self.bb = {}\n        self.hk = None\n    def save(self, target):\n        target.xx = copy.deepcopy(self.xx)\n        target.aa = copy.deepcopy(self.aa)\n        target.bb = copy.deepcopy(self.bb)\n        target.hk = copy.deepcopy(self.hk)\n    def load(self, target):\n        self.xx = copy.deepcopy(target.xx)\n        self.aa = copy.deepcopy(target.aa)"
        },
        {
            "comment": "This code defines methods for a neural network model. It includes deep copying of variables, layer normalization, feed-forward operation, and scaled attention operation. The LN method performs layer normalization on input tensor xx using the weights w. The FF method applies a feed-forward operation to the input tensor xx using the weights w and stores intermediate results. The SA method applies the scaled attention operation to the input tensor xx using the weights w and stores intermediate results.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model_run.py\":69-97",
            "content": "        self.bb = copy.deepcopy(target.bb)\n        self.hk = copy.deepcopy(target.hk)\n    def LN(self, xx, w):\n        return F.layer_norm(xx, (self.n_embd,), weight=w.weight, bias=w.bias)\n    def FF(self, xx, w, name):\n        if name not in self.xx:\n            self.xx[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n        x = xx * w.time_mix + self.xx[name] * (1 - w.time_mix)\n        self.xx[name] = xx\n        r = torch.sigmoid(w.receptance.weight @ x)\n        k = torch.square(torch.relu(w.key.weight @ x))\n        kv = w.value.weight @ k\n        return r * kv\n    def SA(self, xx, w, name):\n        if name not in self.xx:\n            self.xx[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n            self.aa[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n            self.bb[name] = torch.zeros(self.n_embd, device=self.RUN_DEVICE)\n        x = xx * w.time_mix + self.xx[name] * (1 - w.time_mix)\n        self.xx[name] = xx\n        r = torch.sigmoid(w.receptance.weight @ x)\n        k = torch.exp(torch.clamp(w.key.weight @ x, max=RWKV_K_CLAMP))"
        },
        {
            "comment": "This code is implementing the RWKV model for language generation. It applies layer normalization, self-attention, and feed-forward layers iteratively to generate output. The time decay mechanism is used to update the internal states of the model. Additionally, the code initializes the head keys for generating future sequence tokens.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model_run.py\":98-129",
            "content": "        v = w.value.weight @ x\n        kv = k * v\n        a = self.aa[name] + w.time_first * kv\n        b = self.bb[name] + w.time_first * k\n        self.aa[name] = w.time_decay * self.aa[name] + kv\n        self.bb[name] = w.time_decay * self.bb[name] + k\n        rwkv = r * a / (b + RWKV_K_EPS)\n        return w.output.weight @ rwkv\n    def run(self, ctx):\n        w = self.w\n        x = w.emb.weight[ctx[-1]]\n        for i in range(self.n_layer):\n            x = self.LN(x, w.blocks[i].ln1)\n            if i == 0 and self.model_type == 'RWKV-ffnPre':\n                x = x + self.FF(x, w.blocks[i].ffnPre, f'ffnPre.{i}')\n            else:\n                x = x + self.SA(x, w.blocks[i].att, f'att.{i}')\n            x = self.LN(x, w.blocks[i].ln2)\n            x = x + self.FF(x, w.blocks[i].ffn, f'ffn.{i}')\n        x = self.LN(x, w.ln_out)\n        if self.hk == None:\n            self.hk = (w.head_k.weight @ x).unsqueeze(0)\n        else:\n            self.hk = torch.cat(\n                [self.hk, (w.head_k.weight @ x).unsqueeze(0)], dim=0)"
        },
        {
            "comment": "This code snippet reshapes the `hk` variable if its shape exceeds the context length, then performs matrix multiplications to calculate a context vector, and finally adds this vector to the input list at corresponding indices.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model_run.py\":130-142",
            "content": "        if self.hk.shape[0] > self.ctx_len:\n            self.hk = self.hk[-self.ctx_len:, :]\n        q = w.head_q.weight @ x\n        x = w.head.weight @ x\n        x = x.cpu().numpy().tolist()\n        c = (self.hk @ q) / RWKV_HEAD_QK_DIM\n        for i in range(len(c)):\n            x[ctx[i]] += c[i]\n        return x"
        }
    ]
}