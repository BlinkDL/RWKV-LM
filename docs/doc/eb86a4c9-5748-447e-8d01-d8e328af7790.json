{
    "summary": "This code imports libraries, defines dataset classes for RWKV Language Model, handles data types, initializes vocabulary and dictionaries, and trains the model. It initializes a context (z) for a dataset, checks if context sum is zero, randomly selects an index from data pile, converts indices to tensors, and returns x, y, or just x, y depending on my_qa_mask.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines a class for a dataset, specifically for the RWKV Language Model. The dataset supports different data types like \"binidx\", and if the type is binidx, it loads the data from the specified file into memory. It also checks if the current vocabulary size is correct and provides information about the loaded data size in tokens.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/dataset.py\":0-24",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport json, math, random, os, sys\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom pytorch_lightning.utilities import rank_zero_info\nfrom .binidx import MMapIndexedDataset\nfrom .utils import MaybeIsPrime\nclass MyDataset(Dataset):\n    def __init__(self, args):\n        self.args = args\n        if args.data_type == \"binidx\":\n            self.vocab_size = args.vocab_size\n            rank_zero_info(f\"Current vocab size = {self.vocab_size} (make sure it's correct)\")\n            if args.my_pile_version == 1:\n                self.data = MMapIndexedDataset(args.data_file)\n                self.data_size = len(self.data._bin_buffer) // self.data._index._dtype_size\n                rank_zero_info(f\"Data has {self.data_size} tokens.\")"
        },
        {
            "comment": "This code reads a dataset file and processes it based on the specified version. If the my_pile_version is 2, it splits the data into chunks and creates MMapIndexedDataset objects for each chunk with corresponding sizes. It also checks and asserts the size differences. If my_qa_mask is greater than 0, it sets a specific data pile for use.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/dataset.py\":25-41",
            "content": "            elif args.my_pile_version == 2:\n                data_list = open(args.data_file, \"r\", encoding='utf-8').read().strip().split('\\n')\n                data_list = [i.strip().split(' ') for i in data_list]\n                self.data = []\n                self.data_size = int(data_list[-1][-1])\n                rank_zero_info(f\"Data has {self.data_size} chunks.\")\n                for d in data_list:\n                    data = MMapIndexedDataset(d[0])\n                    data_size = len(data._bin_buffer) // data._index._dtype_size\n                    assert (data_size - args.ctx_len) == int(d[1])\n                    self.data += [[int(d[-1]), int(d[1]), data]]\n                # rank_zero_info(self.data)\n            if args.my_qa_mask > 0:\n                # self.data_pile = MMapIndexedDataset('/fsx/pile/pile_20B_tokenizer_text_document')\n                self.data_pile = MMapIndexedDataset('/fsx/pile_deduped/pile_0.87_deduped_text_document')\n                self.data_pile_size = len(self.data_pile._bin_buffer) // self.data._index._dtype_size"
        },
        {
            "comment": "The code is defining a dataset class. If data_size and vocab_size are not provided, the data pile is set to None. If my_pile_stage is greater than 0, it asserts that the data size is correct, calculates samples per epoch based on epoch_steps and real_bsz, checks if magic_prime is prime and within certain range, or loads data from file if data type is numpy. The code also provides information about current vocab size for the user.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/dataset.py\":42-59",
            "content": "            else:\n                self.data_pile = None\n                self.data_pile_size = 0\n            if args.my_pile_stage > 0:\n                # assert self.data_size == 332115325534 and self.vocab_size == 50277\n                self.samples_per_epoch = args.epoch_steps * args.real_bsz\n                assert self.samples_per_epoch == 40320\n                rank_zero_info(f\"########## Pile 20b-tokenized stage {args.my_pile_stage} ##########\")\n                dataset_slot = self.data_size // args.ctx_len\n                if args.my_pile_stage != 4:\n                    assert MaybeIsPrime(args.magic_prime)\n                    assert args.magic_prime % 3 == 2\n                    assert args.magic_prime / dataset_slot > 0.99 and args.magic_prime / dataset_slot <= 1\n        elif args.data_type == \"numpy\":\n            self.data = np.load(args.data_file).astype(\"int\")\n            self.vocab_size = args.vocab_size\n            rank_zero_info(f\"Current vocab size = {self.vocab_size} (make sure it's correct)\")"
        },
        {
            "comment": "The code reads data from a file based on the provided data type and builds an array of tokens. It checks if the data_type is \"RWKV-LM/RWKV-v5/src/dataset.py\":60-79\n\"RWKV-LM/RWKV-v5/src/dataset.py\":60-79",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/dataset.py\":60-79",
            "content": "            self.data_size = len(self.data)\n            rank_zero_info(f\"Data has {self.data_size} tokens.\")\n        elif args.data_type == \"uint16\":\n            self.data = np.fromfile(args.data_file, dtype=np.uint16).astype(\"int32\").reshape(-1, args.my_sample_len)\n            self.vocab_size = args.vocab_size\n            rank_zero_info(f\"Current vocab size = {self.vocab_size} (make sure it's correct)\")\n            self.data_size = self.data.shape[0]\n            rank_zero_info(f\"Data has {self.data_size} samples.\")\n        else:\n            if args.data_type == \"dummy\":\n                rank_zero_info(\"Building dummy data...\")\n                self.data = \"\"\n                for i in range(100000):\n                    aa = (i) % 10000\n                    bb = (i * i) % 10000\n                    cc = aa + bb\n                    self.data += f\".{aa}+{bb}={cc}.\"\n            else:\n                self.data = open(args.data_file, \"r\", encoding=args.data_type).read()\n            rank_zero_info(\"Building token list...\")"
        },
        {
            "comment": "The code snippet initializes a vocabulary and associated dictionaries, writes the vocabulary to a JSON file, sets the data size and vocab size, and defines two helper methods for converting between indices and tokens. It also includes a print statement that displays information about the current epoch, index, and rank in case of distributed training.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/dataset.py\":80-106",
            "content": "            unique = sorted(list(set(self.data)))\n            self.vocab_size = len(unique)\n            # rank_zero_info()\n            # for u in unique:\n            #     print(u, end=' ')\n            # rank_zero_info('\\n\\n')\n            xx = 0\n            xxObj = {}\n            for u in unique:\n                xxObj[xx] = u\n                xx += 1\n            with open(f\"{args.proj_dir}/vocab.json\", \"w\", encoding=\"utf-8\") as vocab_file:\n                vocab_file.write(json.dumps(xxObj, ensure_ascii=False))\n            self.data_size = len(self.data)\n            rank_zero_info(f\"Data has {self.data_size} tokens, {self.vocab_size} vocab size.\")\n            self.stoi = {ch: i for i, ch in enumerate(unique)}\n            self.itos = {i: ch for i, ch in enumerate(unique)}\n    def __len__(self):\n        return self.args.epoch_steps * self.args.micro_bsz\n    def __getitem__(self, idx):\n        args = self.args\n        rank = self.global_rank\n        epoch = self.real_epoch\n        world_size = self.world_size\n        # print(f\"epoch {epoch} idx {idx} rank {rank}/{world_size}\")"
        },
        {
            "comment": "This code retrieves data from a dataset based on the specified data type. If the data type is uint16, it selects a random index and uses the sliced data for training. Otherwise, it determines the context length and required data length based on arguments, then picks a random spot in the dataset or cheats by picking a random location when specified.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/dataset.py\":108-133",
            "content": "        if args.data_type == \"uint16\":\n            i = np.random.randint(0, self.data_size-1)\n            dix = self.data[i]\n            x = torch.tensor(dix[:-1], dtype=torch.long)\n            y = torch.tensor(dix[1:], dtype=torch.long)\n        else:\n            ctx_len = args.ctx_len\n            req_len = ctx_len + 1\n            magic_prime = args.magic_prime\n            data = self.data\n            if args.my_pile_stage > 0:\n                ii = 1 + epoch * self.samples_per_epoch + (idx * world_size) + rank\n                if args.my_qa_mask > 0:\n                    ii_orig = ii\n                    if ii % 2 == 0:\n                        ii = -1\n                        data = self.data_pile\n                    else:\n                        ii = ii // 2\n                if data == self.data_pile:\n                    i = np.random.randint(0, self.data_pile_size - req_len)\n                else:\n                    if args.my_pile_stage == 4 or ii < args.my_random_steps:\n                        # cheat: pick a random spot in dataset"
        },
        {
            "comment": "This code randomly selects a chunk of data from a dataset. If the `args.my_pile_version` is 1, it picks a random position within the remaining data after subtracting `req_len`. For other cases, it calculates the index using a complex formula involving `ii`, `magic_prime`, and `ctx_len`. If `args.data_type` is \"binidx\" and `args.my_pile_version` is 1, it retrieves data from the `data` object at the specified offset and length. Otherwise, it simply picks a random position within the remaining data after subtracting `req_len`.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/dataset.py\":134-153",
            "content": "                        if args.my_pile_version == 1:\n                            i = np.random.randint(0, self.data_size - req_len)\n                        else:\n                            i = np.random.randint(0, self.data_size)\n                    else:\n                        ii = ii - args.my_random_steps\n                        factor = (math.sqrt(5) - 1) / 2\n                        factor = int(magic_prime * factor)\n                        i = ((factor * ii * ii * ii) % magic_prime) * ctx_len\n                        i = i + args.my_pile_shift\n                # print(f\"epoch {epoch} idx {idx} rank {rank}/{world_size} ii {ii} pos {round(i / self.data_size, 3)}\")\n            else:\n                # cheat: pick a random spot in dataset\n                i = np.random.randint(0, self.data_size - req_len)\n            if args.data_type == \"binidx\":\n                if args.my_pile_version == 1:\n                    dix = data.get(idx=0, offset=i, length=req_len).astype(int)\n                else:\n                    # self.data : cutoff, chunk_count, data"
        },
        {
            "comment": "Code is slicing data from a list based on the provided index 'i' and length 'req_len'. If the data type is \"numpy\", it directly assigns the slice to dix. Otherwise, it converts the slice into indices using self.stoi. If args.my_qa_mask == 1, it checks for a specific sequence of tokens in the sliced data and assigns either [1] * ctx_len or [0] * ctx_len to 'z'.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/dataset.py\":154-176",
            "content": "                    for j in range(len(data)):\n                        if i < data[j][0]:\n                            ii = i\n                            i = (i - (data[j-1][0] if j > 0 else 0)) % data[j][1]\n                            dix = data[j][2].get(idx=0, offset=i, length=req_len).astype(int)\n                            # print(ii, j, i)\n                            break\n            elif args.data_type == \"numpy\":\n                dix = data[i : i + req_len]\n            else:\n                dix = [self.stoi[s] for s in data[i : i + req_len]]\n            if args.my_qa_mask == 1:\n                if data == self.data_pile:\n                    z = [1] * ctx_len\n                else:\n                    z = [0] * ctx_len\n                    z_sum = 0\n                    isGood = False\n                    for i in range(3, ctx_len):\n                        if dix[i] == 27 and dix[i-1] == 34 and dix[i-2] == 187 and dix[i-3] == 187:\n                            isGood = True\n                        if dix[i] == 0:"
        },
        {
            "comment": "This code is initializing a context (z) for a dataset. It checks if the context sum is zero and if so, sets all elements of z to 1 randomly selects an index from the data pile, converts indices to tensors, and returns the input (x), output (y), and context (z) unless my_qa_mask is set to 1. In that case, it only returns x, y.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/src/dataset.py\":177-199",
            "content": "                            isGood = False\n                        if isGood:\n                            z[i] = 1\n                            z_sum += 1\n                    if z_sum == 0:\n                        z = [1] * ctx_len\n                        i = np.random.randint(0, self.data_pile_size - req_len)\n                        dix = self.data_pile.get(idx=0, offset=i, length=req_len).astype(int)\n                z = torch.tensor(z, dtype=torch.bfloat16)\n            x = torch.tensor(dix[:-1], dtype=torch.long)\n            y = torch.tensor(dix[1:], dtype=torch.long)\n            # if ii_orig < 50:\n            #     # if rank == 1:\n            #     print('rank', rank, 'i', ii_orig, ii, i, 'x', x[:5], '...', x[-5:])\n            # else:\n            #     exit(0)\n            if args.my_qa_mask == 1:\n                return x, y, z\n            return x, y"
        }
    ]
}