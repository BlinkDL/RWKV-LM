{
    "summary": "RWKV-v4's CUDA kernel performs RNN operations, gradient calculation, and avoids overflow with matrix multiplication. Efficient parallel execution techniques are used in cuda_forward and cuda_backward functions, optimizing register usage and ensuring proper block and thread configurations for GPU execution.",
    "details": [
        {
            "comment": "This code snippet defines a CUDA kernel function for the forward pass of a recurrent neural network (RNN). The function takes input and weight tensors as arguments, performs matrix multiplications, and accumulates results using running sums to avoid overflows. The result is stored in output tensor y.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/cuda/wkv_cuda.cu\":0-28",
            "content": "#include <stdio.h>\n#include <assert.h>\n#define MIN_VALUE (-1e38)\ntemplate <typename F>\n__global__ void kernel_forward(const int B, const int T, const int C,\n                               const F *__restrict__ const _w, const F *__restrict__ const _u, const F *__restrict__ const _k, const F *__restrict__ const _v,\n                               F *__restrict__ const _y) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int _b = idx / C;\n    const int _c = idx % C;\n    const int _offset = _b * T * C + _c;\n    F u = _u[_c];\n    F w = _w[_c];\n    const F *__restrict__ const k = _k + _offset;\n    const F *__restrict__ const v = _v + _offset;\n    F *__restrict__ const y = _y + _offset;\n    F p = 0, q = 0, o = MIN_VALUE;\n    // p and q are running sums divided by exp(o) (to avoid overflows)\n    for (int i = 0; i < T; i++) {\n        const int ii = i * C;\n        F no = max(o, u + k[ii]);\n        F A = exp(o - no);\n        F B = exp(u + k[ii] - no);\n        y[ii] = (A * p + B * v[ii]) / (A * q + B);"
        },
        {
            "comment": "Kernel function for backward propagation in RWKV-v4. Calculates gradients for weight matrix w, input u and key matrix k. Uses blockIdx and threadIdx to calculate offsets for accessing data.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/cuda/wkv_cuda.cu\":30-55",
            "content": "        no = max(w + o, k[ii]);\n        A = exp(w + o - no);\n        B = exp(k[ii] - no);\n        p = A * p + B * v[ii];\n        q = A * q + B;\n        o = no;\n    }\n}\ntemplate <typename F>\n__global__ void kernel_backward(const int B, const int T, const int C,\n                                const F *__restrict__ const _w, const F *__restrict__ const _u, const F *__restrict__ const _k, const F *__restrict__ const _v, const F *__restrict__ const _gy,\n                                F *__restrict__ const _gw, F *__restrict__ const _gu, F *__restrict__ const _gk, F *__restrict__ const _gv) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int _b = idx / C;\n    const int _c = idx % C;\n    const int _offset = _b * T * C + _c;\n    F u = _u[_c];\n    F w = _w[_c];\n    const F *__restrict__ const k = _k + _offset;\n    const F *__restrict__ const v = _v + _offset;\n    const F *__restrict__ const gy = _gy + _offset;\n    F *__restrict__ const gk = _gk + _offset;\n    F *__restrict__ const gv = _gv + _offset;"
        },
        {
            "comment": "This code calculates the gradients of model parameters by iterating through a dataset, updating intermediate variables, and storing gradients in gk and gv arrays. It uses matrix multiplication and exponential operations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/cuda/wkv_cuda.cu\":57-96",
            "content": "    F y[Tmax], z[Tmax], zexp[Tmax];\n    F gw = 0, gu = 0;\n    F p = 0, q = 0;\n    F dpdw = 0, dqdw = 0;\n    F o = MIN_VALUE;\n    for (int i = 0; i < T; i++) {\n        const int ii = i * C;\n        F no = max(o, k[ii] + u);\n        F A = exp(o - no);\n        F B = exp(k[ii] + u - no);\n        F num = A * p + B * v[ii];\n        F iden = 1 / (A * q + B);\n        y[i] = num * iden;\n        z[i] = iden;\n        zexp[i] = k[ii] + u - no;\n        gw += gy[ii] * (dpdw - dqdw * y[i]) * iden * A;\n        gu += gy[ii] * (v[ii] - y[i]) * B * iden;\n        no = max(w + o, k[ii]);\n        A = exp(w + o - no);\n        B = exp(k[ii] - no);\n        dpdw = A * (p + dpdw);\n        dqdw = A * (q + dqdw);\n        p = A * p + B * v[ii];\n        q = A * q + B;\n        o = no;\n    }\n    F gp = 0, gq = 0;\n    o = MIN_VALUE;\n    for (int i = T - 1; i >= 0; i--) {\n        const int ii = i * C;\n        F A = gy[ii] * z[i] * exp(zexp[i]);\n        F B = exp(k[ii] + o);\n        gk[ii] = A * (v[ii] - y[i]) + B * (gp * v[ii] + gq);\n        gv[ii] = A + B * gp;"
        },
        {
            "comment": "This code is a part of the RWKV-v4 library, specifically the cuda_forward and cuda_backward functions for CUDA implementation. It involves setting up blocks and threads based on the given dimensions B, T, and C. The kernels kernel_forward and kernel_backward are launched with appropriate block and thread configurations to perform matrix operations in parallel. This code utilizes an optimal number of registers and asserts that B * C is divisible by the chosen number of threads per block for efficient execution.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/cuda/wkv_cuda.cu\":98-120",
            "content": "        F no = max(w + o, zexp[i] - k[ii] - u);\n        A = exp(w + o - no);\n        B = gy[ii] * z[i] * exp(zexp[i] - k[ii] - u - no);\n        gp = A * gp + B;\n        gq = A * gq - B * y[i];\n        o = no;\n    }\n    // Multiply by w because the w -> -exp(w) preprocessing is halfway in the backwards pass, even though it's not in the forward pass\n    const int _offsetBC = _b * C + _c;\n    _gw[_offsetBC] += gw * _w[_c];\n    _gu[_offsetBC] += gu;\n}\nvoid cuda_forward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y) {\n    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n    assert(B * C % threadsPerBlock.x == 0);\n    dim3 numBlocks(B * C / threadsPerBlock.x);\n    kernel_forward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y);\n}\nvoid cuda_backward(int B, int T, int C, float *w, float *u, float *k, float *v, float *gy, float *gw, float *gu, float *gk, float *gv) {\n    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance"
        },
        {
            "comment": "This code ensures that the number of blocks is an integer multiple of threads per block, then launches a kernel function for backward computation. It uses B, C, and threadsPerBlock parameters to determine the number of blocks and threads per block for efficient GPU execution.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/cuda/wkv_cuda.cu\":121-124",
            "content": "    assert(B * C % threadsPerBlock.x == 0);\n    dim3 numBlocks(B * C / threadsPerBlock.x);\n    kernel_backward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, gy, gw, gu, gk, gv);\n}"
        }
    ]
}