{
    "summary": "TrainerConfig class establishes training parameters, GPU support, Wandb setup, initializes device, defines epoch function, creates optimizer, sets DataLoader and trains model. Code handles learning rate multiplier update, progress logging, and moving averages for loss calculation in specified epochs loop.",
    "details": [
        {
            "comment": "TrainerConfig class initializes model training parameters, setting maximum epochs to 10, batch size to 64, learning rate to 4e-4, gradient norm clipping threshold to 1.0, and optimizer betas and eps for Adam optimizer. The code also sets up environment for efficient GPU usage with CUDA and TF32 support.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/trainer.py\":0-35",
            "content": "########################################################################################################\n# The RWKV v2-RNN Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.nn import functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport logging\nimport os\nimport datetime\nimport sys\nimport math\n# import wandb  # comment this if you don't have wandb\n# print('logging to wandb... (comment it if you don\\'t have wandb)')\nlogger = logging.getLogger(__name__)\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cuda.matmul.allow_tf32 = True\nlog_file = open(\"mylog.txt\", \"a\")\nclass TrainerConfig:\n    max_epochs = 10\n    batch_size = 64\n    learning_rate = 4e-4\n    betas = (0.9, 0.99)\n    eps = 1e-8\n    grad_norm_clip = 1.0"
        },
        {
            "comment": "The code snippet represents the initialisation of a Trainer class. It accepts a model, train and test datasets, and a configuration. If wandb (a popular ML experiment tracking tool) is available, it combines the config with the model's config and initiates an experiment with a specific project name and timestamped run name. The device is set to 'cpu', but if GPUs are available, they will be used.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/trainer.py\":36-66",
            "content": "    lr_decay = True  # linear warmup followed by cosine decay\n    warmup_tokens = 0\n    final_tokens = 0\n    epoch_save_frequency = 0\n    epoch_save_path = 'trained-'\n    num_workers = 0  # for DataLoader\n    def __init__(self, **kwargs):\n        for k, v in kwargs.items():\n            setattr(self, k, v)\nclass Trainer:\n    def __init__(self, model, train_dataset, test_dataset, config):\n        self.model = model\n        self.train_dataset = train_dataset\n        self.test_dataset = test_dataset\n        self.config = config\n        self.avg_loss = -1\n        self.steps = 0\n        if 'wandb' in sys.modules:\n            cfg = model.config\n            for k in config.__dict__:\n                setattr(cfg, k, config.__dict__[k])  # combine cfg\n            wandb.init(project=\"RWKV-LM\", name=self.get_run_name() + '-' +\n                       datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S'), config=cfg, save_code=False)\n        self.device = 'cpu'\n        if torch.cuda.is_available():  # take over whatever gpus are on the system"
        },
        {
            "comment": "The code initializes the device, retrieves the run name based on model configuration, and defines a function to run an epoch. It also creates an optimizer for the model and sets up a data loader if necessary, all within the context of training the model.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/trainer.py\":67-91",
            "content": "            self.device = torch.cuda.current_device()\n    def get_run_name(self):\n        raw_model = self.model.module if hasattr(\n            self.model, \"module\") else self.model\n        cfg = raw_model.config\n        run_name = str(cfg.vocab_size) + '-' + str(cfg.ctx_len) + '-' + \\\n            cfg.model_type + '-' + str(cfg.n_layer) + '-' + str(cfg.n_embd)\n        return run_name\n    def train(self):\n        model, config = self.model, self.config\n        raw_model = model.module if hasattr(self.model, \"module\") else model\n        optimizer = raw_model.configure_optimizers(config)\n        def run_epoch(split):\n            is_train = split == 'train'\n            model.train(is_train)\n            data = self.train_dataset if is_train else self.test_dataset\n            if config.num_workers > 0:\n                loader = DataLoader(data, shuffle=False, pin_memory=True,\n                                    batch_size=config.batch_size,\n                                    num_workers=config.num_workers)\n            else:"
        },
        {
            "comment": "This code creates a DataLoader for iterating through data with shuffling disabled, and sets the batch size and number of workers according to config settings. It then initializes a progress bar (pbar) to track the progress through the loader. In training mode, it loops through each iteration (it, x, y), moves tensors to the device, forwards model, zeroes gradients, backprops, applies gradient clipping if specified, and steps the optimizer. If learning rate decay is enabled, it also decays the learning rate based on progress.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/trainer.py\":92-116",
            "content": "                loader = DataLoader(data, shuffle=False,\n                                    batch_size=config.batch_size,\n                                    num_workers=config.num_workers)\n            pbar = tqdm(enumerate(loader), total=len(\n                loader), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}') if is_train else enumerate(loader)\n            for it, (x, y) in pbar:\n                x = x.to(self.device)  # place data on the correct device\n                y = y.to(self.device)\n                with torch.set_grad_enabled(is_train):\n                    _, loss = model(x, y)  # forward the model\n                if is_train:  # backprop and update the parameters\n                    model.zero_grad()\n                    loss.backward()\n                    if config.grad_norm_clip > 0:\n                        torch.nn.utils.clip_grad_norm_(\n                            model.parameters(), config.grad_norm_clip)\n                    optimizer.step()\n                    if config.lr_decay:  # decay the learning rate based on our progress"
        },
        {
            "comment": "This code calculates the learning rate multiplier based on the number of tokens processed. If fewer than warmup_tokens, uses linear warmup; otherwise, applies cosine learning rate decay. The learning rate is adjusted according to the current token count and configuration parameters.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/trainer.py\":117-131",
            "content": "                        # number of tokens processed this step (i.e. label is not -100)\n                        self.tokens += (y >= 0).sum()\n                        lr_final_factor = config.lr_final / config.learning_rate\n                        if self.tokens < config.warmup_tokens:\n                            # linear warmup\n                            lr_mult = lr_final_factor + \\\n                                (1 - lr_final_factor) * float(self.tokens) / \\\n                                float(config.warmup_tokens)\n                            progress = 0\n                        else:\n                            # cosine learning rate decay\n                            progress = float(self.tokens - config.warmup_tokens) / float(\n                                max(1, config.final_tokens - config.warmup_tokens))\n                            lr_mult = (0.5 + lr_final_factor / 2) + (0.5 - lr_final_factor /\n                                                                     2) * math.cos(math.pi * progress)  # better 1.0 ~ 0.1"
        },
        {
            "comment": "Updates learning rate based on config. Sets the learning rate for optimizer parameter groups and logs loss, steps, average loss, progress, and learning rate using WandB. Calculates and updates average loss using a moving average factor. Updates progress bar description with mini-epoch, progress percentage, perplexity, current loss, and learning rate in exponential format.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/trainer.py\":132-153",
            "content": "                        lr = config.learning_rate * lr_mult\n                        for param_group in optimizer.param_groups:\n                            param_group['lr'] = lr\n                    else:\n                        lr = config.learning_rate\n                    now_loss = loss.item()  # report progress\n                    self.lr = lr\n                    if 'wandb' in sys.modules:\n                        wandb.log({\"loss\": now_loss},\n                                  step=self.steps * self.config.batch_size)\n                    self.steps += 1\n                    if self.avg_loss < 0:\n                        self.avg_loss = now_loss\n                    else:\n                        factor = 1 / (it + 1)\n                        self.avg_loss = self.avg_loss * \\\n                            (1.0 - factor) + now_loss * factor\n                    pbar.set_description(\n                        f\"mini-epoch {epoch+1} prog {progress*100.0:.2f}% iter {it}: ppl {math.exp(self.avg_loss):.2f} loss {self.avg_loss:.4f} lr {lr:e}\")"
        },
        {
            "comment": "The code initializes a tokens counter and loops over the specified number of epochs. For each epoch, it runs the 'train' function, logs information to a file, saves the model's state if necessary (based on config), and flushes the log file.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/trainer.py\":155-169",
            "content": "        self.tokens = 0  # counter used for learning rate decay\n        for epoch in range(config.max_epochs):\n            run_epoch('train')\n            log_file.write(\n                f'{epoch+1} {self.avg_loss:.6f} {math.exp(self.avg_loss):.4f} {self.lr:.8f} {datetime.datetime.now()} \\n')\n            log_file.flush()\n            if (self.config.epoch_save_frequency > 0 and epoch % self.config.epoch_save_frequency == 0) or (epoch == config.max_epochs - 1):\n                # DataParallel wrappers keep raw model object in .module\n                raw_model = self.model.module if hasattr(\n                    self.model, \"module\") else self.model\n                torch.save(raw_model.state_dict(),\n                           self.config.epoch_save_path + str(epoch+1) + '.pth')"
        }
    ]
}