{
    "summary": "This code trains RWKV v2-RNN and GPT models, adjusting parameters such as batch size, learning rate, and optimizing Torch usage. It saves models at fixed intervals and provides configurations for both models.",
    "details": [
        {
            "comment": "Code for RWKV v2-RNN language model training. Sets up training data, model size, and provides options for encoding format. Imports necessary modules, optimizes Torch usage, and offers different model configurations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/train.py\":0-28",
            "content": "########################################################################################################\n# The RWKV v2-RNN Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport logging\nimport datetime\nimport json\nfrom src.model import GPT, GPTConfig\nfrom src.trainer import Trainer, TrainerConfig\nfrom src.utils import Dataset\nimport torch\nimport numpy as np\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cuda.matmul.allow_tf32 = True\n### Step 1: set training data ##########################################################################\ndatafile = \"enwik8\"\ndatafile_encoding = 'utf-8'\n# datafile_encoding = 'utf-16le'\n### Step 2: set model size #############################################################################\nctx_len = 1024        # ===> increase T_MAX in model.py if your ctx_len > 1024\nn_layer = 6\nn_embd = 512\n# 'RWKV' (better for char-level English) or 'RWKV-ffnPre' (better in some cases)"
        },
        {
            "comment": "This code sets the batch size, learning rate, training mini-epochs, and other parameters for the RWKV model. The batch size should be divisible by forward and backward group sizes in model.py. If encountering \"CUDA out of memory,\" reduce it within GPU limitations. Mini-epochs have a fixed length with saved models saved every 30 mini-epochs.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/train.py\":29-55",
            "content": "model_type = 'RWKV'\n### Step 3: set batch size #############################################################################\n# ===> batch_size must be divisible by B_GROUP_FORWARD and B_GROUP_BACKWARD in model.py\n# For example, if your batch_size = 20, you can set B_GROUP_FORWARD = 4, B_GROUP_BACKWARD = 2\n# If you see \"CUDA out of memory\", reduce it. Use GPU-Z to find the highest value for your VRAM.\nbatch_size = 12\n### Step 4: set learning rate, training mini-epochs #######################################################\nlr_init = 6e-4\nlr_final = 1e-5\n# the mini-epoch is very short and of fixed length (ctx_len * epoch_length_fixed tokens)\nn_epoch = 500\n# 0 = never, 1 = every mini-epoch, 2 = every two mini-epochs, etc.\nepoch_save_frequency = 30\nepoch_save_path = 'trained-'\nepoch_length_fixed = 10000\n########################################################################################################\n# import src.utils\n# src.utils.set_seed(42) # remember to change seed if you load a model\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)"
        },
        {
            "comment": "Loading data for training GPT model...\nTraining the GPT model with specified configuration and parameters.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/train.py\":56-81",
            "content": "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n                    datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO,)\ngrad_norm_clip = 1.0\nwarmup_tokens = 0\nbetas = (0.9, 0.99)\neps = 4e-9\nnum_workers = 0\n########################################################################################################\n# Load data\n########################################################################################################\nprint('loading data... ' + datafile)\ntrain_dataset = Dataset(open(\n    datafile, \"r\", encoding=datafile_encoding).read(), ctx_len, epoch_length_fixed)\n########################################################################################################\n# Train model\n########################################################################################################\nif __name__ == '__main__':\n    model = GPT(GPTConfig(train_dataset.vocab_size, train_dataset.ctx_len, model_type=model_type,\n                          n_layer=n_layer, n_embd=n_embd)).cuda()"
        },
        {
            "comment": "This code loads a trained model, sets up the Trainer and trains it, then saves the final model. It also prints out various parameters for the training session such as epochs, batch size, etc.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/train.py\":83-97",
            "content": "    # # # load a trained model. remember to change random seed\n    # m2 = torch.load('trained-61.pth')\n    # model.load_state_dict(m2)\n    print('model', model_type, 'epoch', n_epoch, 'batchsz', batch_size, 'betas',\n          betas, 'eps', eps, 'ctx', ctx_len, 'layer', n_layer, 'embd', n_embd, )\n    tconf = TrainerConfig(model_type=model_type, max_epochs=n_epoch, batch_size=batch_size,\n                          learning_rate=lr_init, lr_decay=True, lr_final=lr_final, betas=betas, eps=eps, grad_norm_clip=grad_norm_clip,\n                          warmup_tokens=warmup_tokens, final_tokens=n_epoch*len(train_dataset)*ctx_len, num_workers=num_workers, epoch_save_frequency=epoch_save_frequency, epoch_save_path=epoch_save_path)\n    trainer = Trainer(model, train_dataset, None, tconf)\n    trainer.train()\n    torch.save(model.state_dict(), 'trained-' + str(n_epoch) + '-' + trainer.get_run_name() +\n               '-' + datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S') + '.pth')"
        }
    ]
}