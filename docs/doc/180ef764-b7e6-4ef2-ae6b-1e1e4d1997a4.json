{
    "summary": "This code utilizes PyTorch and CUDA to implement RWKV Language Model and GPT model with time-based mixing, layer normalization, attention heads, Adam optimizer, weight decay, and efficient attention retrieval for forward computations.",
    "details": [
        {
            "comment": "The code is for the RWKV Language Model, implemented in PyTorch with CUDA kernel. It defines constants such as `RWKV_K_CLAMP`, `RWKV_K_EPS`, and `RWKV_HEAD_QK_DIM`. The CUDA kernel limits the maximum context length to 1024, allows grouping for forward and backward passes.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":0-24",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nfrom torch.utils.cpp_extension import load\nimport math\nimport numpy as np\nimport logging\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nlogger = logging.getLogger(__name__)\nRWKV_K_CLAMP = 60  # e^60 = 1e26\nRWKV_K_EPS = 1e-8\nRWKV_HEAD_QK_DIM = 256\nprint(f'\\nRWKV_K_CLAMP {RWKV_K_CLAMP} RWKV_K_EPS {RWKV_K_EPS} RWKV_HEAD_QK_DIM {RWKV_HEAD_QK_DIM}\\n')\n########################################################################################################\n# CUDA Kernel\n########################################################################################################\nT_MAX = 1024          # increase this if your ctx_len > 1024\nB_GROUP_FORWARD = 4   # set to 8 for best performance\nB_GROUP_BACKWARD = 2  # set to 2 for best performance (sometimes 8 is faster)"
        },
        {
            "comment": "This code defines a TimeX class that extends torch.autograd.Function for the TimeX operation. It includes a forward method to calculate the output and a backward method for gradient computation. The class initializes context variables (B, C, T) based on input arguments, performs assertions on T, B, and checks contiguity of input tensors. It then calls the timex_cuda.forward function with saved tensors, an empty tensor for wk output, and other input arguments. The backward method performs similar assertions and uses saved tensors and gradient input gwk to calculate gradients for w and k. T_MAX, B_GROUP_FORWARD, and B_GROUP_BACKWARD are constants used in assertions.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":26-49",
            "content": "timex_cuda = load(name=\"timex\", sources=[\"cuda/timex_op.cpp\", \"cuda/timex_cuda.cu\"],\n                  verbose=True, extra_cuda_cflags=['--use_fast_math', '--extra-device-vectorization', f'-DTmax={T_MAX}', f'-DBF={B_GROUP_FORWARD}', f'-DBB={B_GROUP_BACKWARD}'])\nclass TimeX(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, w, k, B, C, T, eps):\n        ctx.B = B\n        ctx.C = C\n        ctx.T = T\n        assert ctx.T % 4 == 0 and ctx.T <= T_MAX and ctx.B % B_GROUP_FORWARD == 0 and ctx.B % B_GROUP_BACKWARD == 0\n        w = w.contiguous()\n        k = k.contiguous()\n        ctx.save_for_backward(w, k)\n        wk = torch.empty((B, C, T), device='cuda',\n                         memory_format=torch.contiguous_format)\n        timex_cuda.forward(w, k, wk, eps, B, C, T)\n        return wk\n    @staticmethod\n    def backward(ctx, gwk):\n        assert ctx.T % 4 == 0 and ctx.T <= T_MAX and ctx.B % B_GROUP_FORWARD == 0 and ctx.B % B_GROUP_BACKWARD == 0\n        w, k = ctx.saved_tensors\n        gw = torch.empty((ctx.B, ctx.C, ctx.T), device='cuda',"
        },
        {
            "comment": "This code snippet initializes a model's weights using RWKV-v3's time and channel mixing techniques. It creates empty tensors, performs backward calculations on the weight matrix (w) and key matrix (k), and returns gradients for further processing. The `RWKV_Init` function initializes the linear and embedding layers of a module using specific naming conventions.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":50-68",
            "content": "                         memory_format=torch.contiguous_format)\n        gk = torch.empty((ctx.B, ctx.C, ctx.T), device='cuda',\n                         memory_format=torch.contiguous_format)\n        timex_cuda.backward(w, k, gwk.contiguous(), gw,\n                            gk, ctx.B, ctx.C, ctx.T)\n        return (gw.sum(dim=0), gk, None, None, None, None)\n########################################################################################################\n# RWKV: RWKV Time-mix + RWKV Channel-mix\n########################################################################################################\ndef RWKV_Init(module, config):  # fancy initialization of all lin & emb layer in the module\n    for m in module.modules():\n        if not isinstance(m, (nn.Linear, nn.Embedding)):\n            continue\n        with torch.no_grad():\n            name = '[unknown weight]'\n            for name, parameter in module.named_parameters():  # find the name of the weight\n                if id(m.weight) == id(parameter):"
        },
        {
            "comment": "This code checks the type of a layer in the neural network model and applies different weight initialization depending on its type. If it is an Embedding or Linear layer, it adjusts the gain and scale accordingly. If it has a scale_init attribute, that value is used for initialization. If scale is -999, it initializes weights with identity matrix.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":69-97",
            "content": "                    break\n            shape = m.weight.data.shape\n            gain = 1.0\n            scale = 1.0  # extra scale for gain\n            if isinstance(m, nn.Embedding):\n                gain = math.sqrt(max(shape[0], shape[1]))\n                if shape[0] == config.vocab_size and shape[1] == config.n_embd:  # token emb?\n                    scale = 1e-4\n                else:\n                    scale = 0\n            if isinstance(m, nn.Linear):\n                if m.bias is not None:\n                    m.bias.data.zero_()\n                if shape[0] > shape[1]:\n                    gain = math.sqrt(shape[0] / shape[1])\n                if shape[0] == config.vocab_size and shape[1] == config.n_embd:  # final projection?\n                    scale = 0.5\n            if hasattr(m, 'scale_init'):\n                scale = m.scale_init\n            # print(str(shape[0]).ljust(5), str(shape[1]).ljust(5), f'{round(scale,2):g}'.ljust(4), name)\n            gain *= scale\n            if scale == -999:\n                nn.init.eye_(m.weight)"
        },
        {
            "comment": "This code initializes the weights of matrices in the RWKV_TimeMix module based on a gain value. If the gain is zero, it sets the weights to zero. If the gain is positive, it uses orthogonal initialization with the specified gain. Otherwise, it uses normal initialization with a mean of 0 and a negative standard deviation. The class RWKV_TimeMix is a custom module that takes a configuration and layer ID as inputs and performs time-based curve operations for attention scores. It also initializes a time_curve tensor and calculates a time_decay based on the layer ID.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":98-126",
            "content": "            elif gain == 0:\n                # zero init is great for some RWKV matrices\n                nn.init.zeros_(m.weight)\n            elif gain > 0:\n                nn.init.orthogonal_(m.weight, gain=gain)\n            else:\n                nn.init.normal_(m.weight, mean=0.0, std=-scale)\nclass RWKV_TimeMix(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n        self.ctx_len = config.ctx_len\n        self.n_embd = config.n_embd\n        attn_sz = config.n_embd\n        with torch.no_grad(): # fancy init\n            self.time_curve = torch.tensor([-(config.ctx_len - 2 - i) for i in range(config.ctx_len-1)]).unsqueeze(0)\n            self.time_curve = self.time_curve.to('cuda')\n            ratio_0_to_1 = (layer_id / (config.n_layer - 1)) # 0 to 1\n            ratio_1_to_almost0 = (1.0 - (layer_id / config.n_layer)) # 1 to ~0\n            # fancy time_decay\n            decay_speed = torch.ones(attn_sz, 1)\n            for h in range(attn_sz):\n                decay_speed[h][0] = -5 + 8 * (h / (attn_sz-1)) ** (0.7 + 1.3 * ratio_0_to_1)"
        },
        {
            "comment": "This code initializes and sets various parameters for an attention mechanism in a model. It includes time-related parameters such as `time_decay`, `time_first`, `time_mix_k`, `time_mix_v`, `time_mix_r` and `time_shift`. The `key`, `value`, and `receptance` layers are also defined, each with a specified number of input/output dimensions. These parameters will be used to calculate attention scores between queries and keys, allowing for more effective information retrieval from the input sequence.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":127-147",
            "content": "            self.time_decay = nn.Parameter(decay_speed)\n            # print(layer_id, self.time_decay.flatten()[:3].cpu().numpy(), '...', self.time_decay.flatten()[-3:].cpu().numpy())\n            # fancy time_first\n            zigzag = (torch.tensor([(i+1)%3 - 1 for i in range(attn_sz)]) * 0.5).unsqueeze(1)\n            self.time_first = nn.Parameter(torch.ones(attn_sz, 1) * math.log(0.3) + zigzag)\n            # fancy time_mix\n            x = torch.ones(1, 1, config.n_embd)\n            for i in range(config.n_embd):\n                x[0, 0, i] = i / config.n_embd\n            self.time_mix_k = nn.Parameter(torch.pow(x, ratio_1_to_almost0))\n            self.time_mix_v = nn.Parameter(torch.pow(x, ratio_1_to_almost0) + 0.3 * ratio_0_to_1)\n            self.time_mix_r = nn.Parameter(torch.pow(x, 0.5 * ratio_1_to_almost0))\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        self.key = nn.Linear(config.n_embd, attn_sz, bias=False)\n        self.value = nn.Linear(config.n_embd, attn_sz, bias=False)\n        self.receptance = nn.Linear(config.n_embd, attn_sz, bias=False)"
        },
        {
            "comment": "This code initializes the model and defines the forward function, which performs time-mixing operations on input data, then uses this mixed data to produce attention keys, values, and receptances. These are then clamped and transformed into exponential form before being combined in a weighted sum. A W-curve is also calculated for some future use that may eliminate the need for clamping.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":149-174",
            "content": "        self.output = nn.Linear(attn_sz, config.n_embd, bias=False)\n        self.key.scale_init = 0\n        self.receptance.scale_init = 0\n        self.output.scale_init = 0\n    def forward(self, x):\n        B, T, C = x.size() # x = (Batch,Time,Channel)\n        # Mix x with the previous timestep to produce xk, xv, xr\n        xx = self.time_shift(x) # self.time_shift = nn.ZeroPad2d((0,0,1,-1))\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        # Use xk, xv, xr to produce k, v, r\n        k = self.key(xk).transpose(-1, -2)\n        v = self.value(xv).transpose(-1, -2)\n        r = self.receptance(xr)\n        # RWKV_K_CLAMP can be removed if the CUDA kernel substracts the correct k_max for each k (I will do this later)\n        k = torch.clamp(k, max=RWKV_K_CLAMP) # clamp k to avoid overflow\n        k = torch.exp(k)\n        kv = k * v\n        # Compute the W-curve = [e^(-n * e^time_decay), e^(-(n-1) * e^time_decay), ..., 1, e^(time_first)]"
        },
        {
            "comment": "This code defines a RWKV_ChannelMix module that performs time-based mixing of kv and k tensors using the TimeX function. It also calculates wkv and wk tensors, applies sigmoid function, and outputs the resulting rwkv tensor. The class inherits from nn.Module and initializes with configuration parameters and layer ID. It includes a time_shift operation and sets fancy init for time_mix_k using torch.pow.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":175-203",
            "content": "        self.time_w = torch.cat(\n            [torch.exp(self.time_decay) * self.time_curve, self.time_first], dim=-1)\n        w = torch.exp(self.time_w)\n        # Use W to mix kv and k respectively. Add K_EPS to wk to avoid divide-by-zero\n        wkv = TimeX.apply(w, kv, B, C, T, 0)\n        # RWKV_K_EPS can be removed if the CUDA kernel sets 0/0 = 0 (I will do this later)\n        wk = TimeX.apply(w, k, B, C, T, RWKV_K_EPS)\n        rwkv = torch.sigmoid(r) * (wkv / wk).transpose(-1, -2)\n        rwkv = self.output(rwkv)\n        return rwkv\nclass RWKV_ChannelMix(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        with torch.no_grad(): # fancy init of time_mix\n            ratio_1_to_almost0 = (1.0 - (layer_id / config.n_layer)) # 1 to ~0\n            x = torch.ones(1, 1, config.n_embd)\n            for i in range(config.n_embd):\n                x[0, 0, i] = i / config.n_embd\n            self.time_mix_k = nn.Parameter(torch.pow(x, ratio_1_to_almost0))"
        },
        {
            "comment": "This code is for a GPT model implementation with time-based mixing. It initializes parameters, defines forward pass calculations, and contains a configuration class for the model. The model uses time shifting, mixing, key/value calculations, and sigmoid activation functions in its operations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":204-232",
            "content": "            self.time_mix_r = nn.Parameter(torch.pow(x, ratio_1_to_almost0))\n        hidden_sz = 4 * config.n_embd\n        self.key = nn.Linear(config.n_embd, hidden_sz, bias=False)\n        self.receptance = nn.Linear(config.n_embd, config.n_embd, bias=False)\n        self.value = nn.Linear(hidden_sz, config.n_embd, bias=False)\n        self.value.scale_init = 0\n        self.receptance.scale_init = 0\n    def forward(self, x):\n        xx = self.time_shift(x)\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        k = self.key(xk)\n        k = torch.square(torch.relu(k))\n        kv = self.value(k)\n        rkv = torch.sigmoid(self.receptance(xr)) * kv\n        return rkv\n########################################################################################################\n# The GPT Model with our blocks\n########################################################################################################\nclass GPTConfig:\n    def __init__(self, vocab_size, ctx_len, **kwargs):"
        },
        {
            "comment": "This code defines a class for the RWKV model's Block, which is an instance of nn.Module. It contains layer normalization layers and two modules: RWKV_TimeMix and RWKV_ChannelMix. The forward function applies these modules sequentially to input x after layer normalization. If this is the first block (layer_id == 0), it also includes an additional layer normalization and, if a specific model type is specified, applies the ffnPre module before the other modules.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":233-265",
            "content": "        self.vocab_size = vocab_size\n        self.ctx_len = ctx_len\n        for k, v in kwargs.items():\n            setattr(self, k, v)\nclass Block(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.config = config\n        self.layer_id = layer_id\n        self.ln1 = nn.LayerNorm(config.n_embd)\n        self.ln2 = nn.LayerNorm(config.n_embd)\n        if self.layer_id == 0:\n            self.ln0 = nn.LayerNorm(config.n_embd)\n        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':\n            self.ffnPre = RWKV_ChannelMix(config, layer_id+1000)\n        else:\n            self.att = RWKV_TimeMix(config, layer_id)\n        self.ffn = RWKV_ChannelMix(config, layer_id)\n    def forward(self, x):\n        if self.layer_id == 0:\n            x = self.ln0(x)        \n        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':\n            x = x + self.ffnPre(self.ln1(x))  # better in some cases\n        else:\n            x = x + self.att(self.ln1(x))\n        x = x + self.ffn(self.ln2(x))"
        },
        {
            "comment": "The code defines a class called \"GPT\" which inherits from nn.Module and initializes its parameters based on the given configuration. It includes an embedding layer, multiple blocks, a normalization layer, and different linear layers for output. If RWKV_HEAD_QK_DIM is greater than 0, it also initializes extra head layers for Q and K. The code ends by printing the total number of parameters in the model and logging it.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":266-296",
            "content": "        return x\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.step = 0\n        self.config = config\n        self.emb = nn.Embedding(config.vocab_size, config.n_embd)\n        self.blocks = nn.Sequential(*[Block(config, i)\n                                    for i in range(config.n_layer)])\n        self.ln_out = nn.LayerNorm(config.n_embd)\n        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        if RWKV_HEAD_QK_DIM > 0:\n            self.head_q = nn.Linear(config.n_embd, RWKV_HEAD_QK_DIM, bias=False)\n            self.head_q.scale_init = 0\n            self.head_k = nn.Linear(config.n_embd, RWKV_HEAD_QK_DIM, bias=False)\n            self.head_k.scale_init = 0.1\n            self.register_buffer(\"copy_mask\", torch.tril(\n                torch.ones(config.ctx_len, config.ctx_len)))\n        self.ctx_len = config.ctx_len\n        RWKV_Init(self, config)\n        logger.info(\"number of parameters: %e\", sum(p.numel()\n                    for p in self.parameters()))"
        },
        {
            "comment": "This code defines a model with functions for getting the context length, initializing weights, and configuring optimizers. It separates parameters into those subject to weight decay and those not, and ensures no parameter is included in both sets.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":298-323",
            "content": "    def get_ctx_len(self):\n        return self.ctx_len\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear)):\n            module.weight.data.normal_(mean=0.0, std=0.01)\n        if isinstance(module, (nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=1e-5)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n    def configure_optimizers(self, train_config):\n        # separate out all parameters to those that will and won't experience regularizing weight decay\n        decay = set()\n        no_decay = set()\n        for mn, m in self.named_modules():  # here we disable weight_decay\n            for pn, p in m.named_parameters():\n                fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name\n                no_decay.add(fpn)\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        inter_params = decay & no_decay\n        union_params = decay | no_decay\n        assert len(\n            inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )"
        },
        {
            "comment": "This code defines a model and an optimization function. The model has embedding layers, blocks, and a layer normalization layer. It also includes optional attention heads for query-key calculations. The forward function performs the model's computations based on input indexes and optionally produces output from attention heads. The optimizer sets up the Adam optimizer for training with specified learning rate and betas.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":324-353",
            "content": "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n            % (str(param_dict.keys() - union_params), )\n        optim_groups = [\n            {\"params\": [param_dict[pn]\n                        for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n        ]\n        optimizer = torch.optim.Adam(\n            optim_groups, lr=train_config.learning_rate, betas=train_config.betas, eps=train_config.eps)\n        return optimizer\n    def forward(self, idx, targets=None):\n        self.step += 1\n        B, T = idx.size()\n        assert T <= self.ctx_len, \"Cannot forward, because len(input) > model ctx_len.\"\n        x = self.emb(idx)\n        x = self.blocks(x)\n        x = self.ln_out(x)\n        if RWKV_HEAD_QK_DIM > 0:\n            q = self.head_q(x)[:, :T, :]\n            k = self.head_k(x)[:, :T, :]\n            c = (q @ k.transpose(-2, -1)) * (1.0 / RWKV_HEAD_QK_DIM)\n            c = c.masked_fill(self.copy_mask[:T, :T] == 0, 0)\n            c = c @ F.one_hot(idx, num_classes=self.config.vocab_size).float()"
        },
        {
            "comment": "This code calculates the output of a model and optionally computes a cross-entropy loss if targets are provided. If targets are None, it just returns the output. The head layer is used to process the input 'x'.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/src/model.py\":354-362",
            "content": "            x = self.head(x) + c\n        else:\n            x = self.head(x)\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(x.view(-1, x.size(-1)), targets.view(-1))\n        return x, loss"
        }
    ]
}