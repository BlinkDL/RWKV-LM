{
    "summary": "This code initializes neural network weights, sets up time curves for RWKV_TimeMix module, defines Transformer models with rotary position embeddings and applies rotary encoding. It also includes channel mix and tiny attn mechanisms.",
    "details": [
        {
            "comment": "The code defines a function called `RWKV_Init` that initializes the weights of all linear and embedding layers within a module. It uses fancy initialization, which is a method for assigning weight values to layers based on their names. The function iterates through each layer in the module and checks if it's an instance of either `nn.Linear` or `nn.Embedding`. If so, it proceeds with the weight assignment process.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":0-21",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport math\nimport logging\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nlogger = logging.getLogger(__name__)\n########################################################################################################\n# RWKV: RWKV Time-mix + RWKV Channel-mix\n########################################################################################################\ndef RWKV_Init(module, config): # fancy initialization of all lin & emb layer in the module\n    for m in module.modules():\n        if not isinstance(m, (nn.Linear, nn.Embedding)):\n            continue\n        with torch.no_grad():\n            name = '[unknown weight]'\n            for name, parameter in module.named_parameters(): # find the name of the weight"
        },
        {
            "comment": "The code initializes gain and scale for orthogonal and normal distributions in a neural network. It checks if the weight id is equal to the parameter id, breaks the loop if so. If it's a linear layer, it adjusts the gain based on the shape of the input/output dimensions, and if it matches specific values, scales accordingly. For embedding layers, it does a similar check and scaling. Finally, it checks for a scale_init attribute and assigns it to scale. Lastly, it prints the shape dimensions, scale value, and name.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":22-45",
            "content": "                if id(m.weight) == id(parameter):\n                    break\n            shape = m.weight.data.shape\n            gain = 1.0  # positive: gain for orthogonal, negative: std for normal\n            scale = 1.0 # extra scale for gain\n            if isinstance(m, nn.Linear):\n                if m.bias is not None:\n                    m.bias.data.zero_()\n                if shape[0] > shape[1]:\n                    gain = math.sqrt(shape[0] / shape[1])\n                if shape[0] == config.vocab_size and shape[1] == config.n_embd: # final projection?\n                    scale = config.rwkv_emb_scale\n            if isinstance(m, nn.Embedding):\n                gain = math.sqrt(max(shape[0], shape[1]))\n                if shape[0] == config.vocab_size and shape[1] == config.n_embd: # token emb?\n                    scale = config.rwkv_emb_scale\n            if hasattr(m, 'scale_init'):\n                scale = m.scale_init\n            print(str(shape[0]).ljust(5), str(shape[1]).ljust(5), f'{round(scale,2):g}'.ljust(4), name)"
        },
        {
            "comment": "This code initializes the RWKV_TimeMix module, which is a part of the RWKV model. It sets up the time curves for better convergence using orthogonal or normal initialization based on gain values. The code also ensures that the number of attention heads divides evenly into the total number of attention heads specified in the config file.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":47-69",
            "content": "            gain *= scale\n            if gain == 0:\n                nn.init.zeros_(m.weight) # zero init is great for some RWKV matrices\n            elif gain > 0:\n                nn.init.orthogonal_(m.weight, gain=gain)\n            else:\n                nn.init.normal_(m.weight, mean=0, std=-gain)\nclass RWKV_TimeMix(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        assert config.n_attn % config.n_head == 0\n        self.layer_id = layer_id\n        self.ctx_len = config.ctx_len\n        self.n_head = config.n_head\n        self.head_size = config.n_attn // config.n_head\n        with torch.no_grad(): # initial time_w curves for better convergence\n            ww = torch.ones(config.n_head, config.ctx_len)\n            curve = torch.tensor([-(config.ctx_len - 1 - i) for i in range(config.ctx_len)]) # the distance\n            for h in range(config.n_head):\n                if h < config.n_head - 1:\n                    decay_speed = math.pow(config.ctx_len, -(h+1)/(config.n_head-1))"
        },
        {
            "comment": "This code initializes a multi-head attention layer. It determines the decay speed for each head, sets parameters for time-based operations, creates linear layers for keys and values, and optionally initializes a tiny_att layer if configured. Finally, it sets scale init values for the key, receptance, and output layers to 0.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":70-93",
            "content": "                else:\n                    decay_speed = 0.0\n                ww[h] = torch.exp(curve * decay_speed)\n                # print('layer', layer_id, 'head', h, 'decay_speed', round(decay_speed, 4), ww[h][:5].numpy(), '...', ww[h][-5:].numpy())\n        self.time_w = nn.Parameter(ww)\n        self.time_alpha = nn.Parameter(torch.ones(self.n_head, 1, config.ctx_len))\n        self.time_beta = nn.Parameter(torch.ones(self.n_head, config.ctx_len, 1))\n        self.time_gamma = nn.Parameter(torch.ones(config.ctx_len, 1))\n        self.time_shift = nn.ZeroPad2d((0,0,1,-1))\n        self.key = nn.Linear(config.n_embd, config.n_attn)\n        self.value = nn.Linear(config.n_embd, config.n_attn)\n        self.receptance = nn.Linear(config.n_embd, config.n_attn)\n        # if config.rwkv_tiny_attn > 0:\n        #     self.tiny_att = RWKV_TinyAttn(config)\n        self.output = nn.Linear(config.n_attn, config.n_embd)\n        self.key.scale_init = 0\n        self.receptance.scale_init = 0\n        self.output.scale_init = 0"
        },
        {
            "comment": "This code defines a forward pass for a model. It performs element-wise operations on input 'x' using learnable parameters 'w', 'time_alpha', and 'time_beta'. It also applies 'self.receptance', 'self.key', and 'self.value' operations to the input, clamps extreme values of 'k', calculates cumulative sums, and performs element-wise multiplication. Finally, it applies a sigmoid function and layer normalization to the results.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":95-124",
            "content": "    def forward(self, x):\n        B, T, C = x.size()\n        TT = self.ctx_len\n        w = F.pad(self.time_w, (0, TT))\n        w = torch.tile(w, [TT])\n        w = w[:, :-TT].reshape(-1, TT, 2 * TT - 1)\n        w = w[:, :, TT-1:] # w is now a circulant matrix\n        w = w[:, :T, :T] * self.time_alpha[:, :, :T] * self.time_beta[:, :T, :]\n        x = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)\n        # if hasattr(self, 'tiny_att'):\n        #     tiny_att = self.tiny_att(x, self.mask)\n        k = self.key(x)\n        v = self.value(x)\n        r = self.receptance(x)\n        k = torch.clamp(k, max=30, min=-60) # clamp extreme values. e^30 = 10^13\n        k = torch.exp(k)\n        sum_k = torch.cumsum(k, dim=1)\n        kv = (k * v).view(B, T, self.n_head, self.head_size)\n        wkv = (torch.einsum('htu,buhc->bthc', w, kv)).contiguous().view(B, T, -1)\n        rwkv = torch.sigmoid(r) * wkv / sum_k\n        rwkv = self.output(rwkv)\n        # if hasattr(self, 'tiny_att'):\n        #     rwkv += tiny_att"
        },
        {
            "comment": "This code defines three classes for attention mechanisms in the RWKV model. The RWKV_ChannelMix class represents a channel-wise attention mechanism, while RWKV_TinyAttn is an extra tiny version of this attention mechanism. Both use linear layers and apply Mish activation before performing element-wise multiplication with a sigmoid gated weight. This helps in controlling the contribution of each input to the output during attention computation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":126-157",
            "content": "        return rwkv * self.time_gamma[:T, :]\nclass RWKV_ChannelMix(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n        self.time_shift = nn.ZeroPad2d((0,0,1,-1))\n        hidden_sz = 5 * config.n_ffn // 2 # can use smaller hidden_sz because of receptance gating\n        self.key = nn.Linear(config.n_embd, hidden_sz)\n        self.value = nn.Linear(config.n_embd, hidden_sz)\n        self.weight = nn.Linear(hidden_sz, config.n_embd)\n        self.receptance = nn.Linear(config.n_embd, config.n_embd)\n        self.receptance.scale_init = 0\n        self.weight.scale_init = 0\n    def forward(self, x):\n        B, T, C = x.size()\n        x = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)\n        k = self.key(x)\n        v = self.value(x)\n        r = self.receptance(x)\n        wkv = self.weight(F.mish(k) * v) # i find mish is a bit better than gelu\n        rwkv = torch.sigmoid(r) * wkv\n        return rwkv\nclass RWKV_TinyAttn(nn.Module): # extra tiny attention"
        },
        {
            "comment": "This code defines a model class for attention mechanism in RWKV-v1. It initializes the model's attributes and then applies attention to input using multi-head self-attention. It transposes and masks the results with a mask before applying softmax and returning the output.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":158-179",
            "content": "    def __init__(self, config):\n        super().__init__()\n        self.d_attn = config.rwkv_tiny_attn\n        self.n_head = config.rwkv_tiny_head\n        self.head_size = self.d_attn // self.n_head\n        self.qkv = nn.Linear(config.n_embd, self.d_attn * 3)\n        self.out = nn.Linear(self.d_attn, config.n_embd)\n    def forward(self, x, mask):\n        B, T, C = x.size()\n        qkv = self.qkv(x)\n        q, k, v = qkv.chunk(3, dim = -1)\n        if self.n_head > 1:\n            q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)      # (B, T, C) -> (B, nh, T, hs)\n            k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)      # (B, T, C) -> (B, nh, T, hs)\n            v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)      # (B, T, C) -> (B, nh, T, hs)\n        qk = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_size))     # (B, nh, T, hs) * (B, nh, hs, T) -> (B, nh, T, T)\n        qk = qk.masked_fill(mask == 0, float('-inf'))\n        qk = F.softmax(qk, dim = -1)"
        },
        {
            "comment": "The code is defining a module for multi-head attention with rotary embedding and GEGLU FFN. The model computes the query, key, value matrices (QKV) using dot product attention. If there are multiple heads, it transposes and reshapes the QKV matrix to output a single sequence of size (B, T, C). It then applies a linear transformation from the output layer. The code also includes a RotaryEmbedding class for applying rotary positional encoding to the input sequence.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":180-202",
            "content": "        qkv = qk @ v                                                           # (B, nh, T, T) * (B, nh, T, hs) -> (B, nh, T, hs)\n        if self.n_head > 1:\n            qkv = qkv.transpose(1, 2).contiguous().view(B, T, -1)              # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, C)\n        return self.out(qkv)\n########################################################################################################\n# MHA_rotary: Multi-head Attention + Rotary Encoding + GeGLU FFN\n########################################################################################################\nclass RotaryEmbedding(torch.nn.Module):\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        inv_freq = 1. / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n        self.seq_len_cached = None\n        self.cos_cached = None\n        self.sin_cached = None\n    def forward(self, x, seq_len=None):\n        if seq_len != self.seq_len_cached:\n            self.seq_len_cached = seq_len"
        },
        {
            "comment": "Code snippet is from the RWKV-LM's model.py file and it defines a class `MHA_rotary` which applies multi-head attention with rotary position embedding. The function `apply_rotary_pos_emb` performs rotation of half embeddings and multiplies the queries (q) and keys (k) with cosine and sine of corresponding rotary position embeddings. It returns the transformed q and k for further computation in the model.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":203-229",
            "content": "            t = torch.arange(seq_len, device=x.device)\n            freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.cos_cached = emb.cos()\n            self.sin_cached = emb.sin()\n        return self.cos_cached, self.sin_cached\ndef rotate_half(x):\n    x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]\n    return torch.cat((-x2, x1), -1)\n@torch.jit.script\ndef apply_rotary_pos_emb(q, k, cos, sin):\n    cos, sin = cos[...,:q.shape[-2],:], sin[...,:q.shape[-2],:]\n    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\nclass MHA_rotary(nn.Module):\n    def __init__(self, config, layer_id, time_shift = False):\n        super().__init__()\n        self.layer_id = layer_id\n        assert config.n_attn % config.n_head == 0\n        self.n_head = config.n_head\n        self.ctx_len = config.ctx_len\n        self.head_size = config.n_attn // config.n_head\n        if time_shift:\n            self.time_shift = nn.ZeroPad2d((0,0,1,-1))"
        },
        {
            "comment": "This code is initializing the model for the Transformer architecture. It defines the query, key, value, and output layers, as well as a rotary embedding layer. The forward function then reshapes the input and splits it into query, key, and value matrices, which are used in subsequent computations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":231-252",
            "content": "        self.query = nn.Linear(config.n_embd, config.n_attn)\n        self.key = nn.Linear(config.n_embd, config.n_attn)\n        self.value = nn.Linear(config.n_embd, config.n_attn)\n        self.register_buffer(\"mask\", torch.tril(torch.ones(config.ctx_len, config.ctx_len)))\n        self.rotary_ndims = int(self.head_size * 0.5)\n        self.rotary_emb = RotaryEmbedding(self.rotary_ndims)\n        self.output = nn.Linear(config.n_attn, config.n_embd)\n    def forward(self, x):\n        B, T, C = x.size()\n        if hasattr(self, 'time_shift'):\n            x = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)\n        q = self.query(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)       # (B, T, C) -> (B, nh, T, hs)\n        k = self.key(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)         # (B, T, C) -> (B, nh, T, hs)\n        v = self.value(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)       # (B, T, C) -> (B, nh, T, hs)\n        q, query_pass = q[..., :self.rotary_ndims], q[..., self.rotary_ndims:]"
        },
        {
            "comment": "This code applies rotary encoding to query and key tensors, computes self-attention weights using dot product between query and key tensors, applies a causal mask for sequence generation, normalizes the attention weights using softmax, multiplies the weighted keys with corresponding values, transposes and reshapes the result, and finally passes it through an output layer.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":253-269",
            "content": "        k, key_pass = k[..., :self.rotary_ndims], k[..., self.rotary_ndims:]\n        cos, sin = self.rotary_emb(q, seq_len=T)\n        q, k = apply_rotary_pos_emb(q, k, cos, sin)                                     # rotary encoding\n        q = torch.cat((q, query_pass), dim=-1)\n        k = torch.cat((k, key_pass), dim=-1)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))                 # self-attention: (B, nh, T, hs) * (B, nh, hs, T) -> (B, nh, T, T)\n        att = att.masked_fill(self.mask[:T,:T] == 0, float('-inf'))                     # causal mask\n        att = F.softmax(att, dim = -1)                                                  # softmax\n        x = att @ v                                                                     # (B, nh, T, T) * (B, nh, T, hs) -> (B, nh, T, hs)\n        x = x.transpose(1, 2).contiguous().view(B, T, -1)                               # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, C)\n        x = self.output(x)\n        return x\nclass GeGLU(torch.nn.Module):"
        },
        {
            "comment": "In the given code, a Multi-Head Attention (MHA) module is being defined. It takes in an embedding dimension and creates multiple attention heads. Each head performs scaled dot product attention before concatenating and applying linear transformations to obtain final outputs.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":270-298",
            "content": "    def __init__(self, config, layer_id, time_shift = False):\n        super().__init__()\n        self.layer_id = layer_id\n        if time_shift:\n            self.time_shift = nn.ZeroPad2d((0,0,1,-1))\n        hidden_sz = 3 * config.n_ffn\n        self.key = nn.Linear(config.n_embd, hidden_sz)\n        self.value = nn.Linear(config.n_embd, hidden_sz)\n        self.weight = nn.Linear(hidden_sz, config.n_embd)\n    def forward(self, x):\n        B, T, C = x.size()\n        if hasattr(self, 'time_shift'):\n            x = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)\n        k = self.key(x)\n        v = self.value(x)        \n        y = self.weight(F.gelu(k) * v)\n        return y\n########################################################################################################\n# MHA_pro: with more tricks\n########################################################################################################\nclass MHA_pro(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()"
        },
        {
            "comment": "This code initializes the necessary parameters and layers for an attention mechanism in a transformer model. It sets layer-specific attributes, creates learnable parameters, registers a buffer for masking, and defines convolutional and embedding layers for processing input embeddings.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":299-319",
            "content": "        self.layer_id = layer_id\n        assert config.n_attn % config.n_head == 0\n        self.n_head = config.n_head\n        self.ctx_len = config.ctx_len\n        self.head_size = config.n_attn // config.n_head\n        self.time_w = nn.Parameter(torch.ones(self.n_head, config.ctx_len))\n        self.time_alpha = nn.Parameter(torch.ones(self.n_head, 1, config.ctx_len))\n        self.time_beta = nn.Parameter(torch.ones(self.n_head, config.ctx_len, 1))\n        self.time_gamma = nn.Parameter(torch.ones(config.ctx_len, 1))\n        self.register_buffer(\"mask\", torch.tril(torch.ones(config.ctx_len, config.ctx_len)))\n        self.time_shift = nn.ZeroPad2d((0,0,1,-1))\n        self.query = nn.Linear(config.n_embd, config.n_attn)\n        self.key = nn.Linear(config.n_embd, config.n_attn)\n        self.value = nn.Linear(config.n_embd, config.n_attn)\n        self.rotary_ndims = int(self.head_size * 0.5)\n        self.rotary_emb = RotaryEmbedding(self.rotary_ndims)\n        self.head_mix = nn.Conv2d(self.n_head, self.n_head, kernel_size=1, bias=False)  # talking heads"
        },
        {
            "comment": "This code defines a model for attention in transformer architecture. It includes the linear layer, time-shift mixing operation, and query/key/value projections. The rotary embedding is used for positional encoding, and the forward function performs matrix multiplications and element-wise operations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":321-338",
            "content": "        self.output = nn.Linear(config.n_attn, config.n_embd)\n    def forward(self, x):\n        B, T, C = x.size()\n        TT = self.ctx_len\n        w = F.pad(self.time_w, (0, TT))\n        w = torch.tile(w, [TT])\n        w = w[:, :-TT].reshape(-1, TT, 2 * TT - 1)\n        w = w[:, :, TT-1:] # w is now a circulant matrix\n        w = w[:, :T, :T] * self.time_alpha[:, :, :T] * self.time_beta[:, :T, :]\n        x = torch.cat([self.time_shift(x[:, :, :C//2]), x[:, :, C//2:]], dim = -1)      # time-shift mixing\n        q = self.query(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)       # (B, T, C) -> (B, nh, T, hs)\n        k = self.key(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)         # (B, T, C) -> (B, nh, T, hs)\n        v = self.value(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)       # (B, T, C) -> (B, nh, T, hs)\n        q, query_pass = q[..., :self.rotary_ndims], q[..., self.rotary_ndims:]\n        k, key_pass = k[..., :self.rotary_ndims], k[..., self.rotary_ndims:]"
        },
        {
            "comment": "This code applies rotary positional encoding to the queries and keys, concatenates them with additional vectors, performs self-attention by multiplying and dividing query-key matrix by weights, applies a causal mask, calculates attention scores using softmax, multiplies attention scores by values, and finally reshapes the output",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":339-351",
            "content": "        cos, sin = self.rotary_emb(q, seq_len=T)\n        q, k = apply_rotary_pos_emb(q, k, cos, sin)                                     # rotary encoding\n        q = torch.cat((q, query_pass), dim=-1)\n        k = torch.cat((k, key_pass), dim=-1)  \n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))                 # self-attention: (B, nh, T, hs) * (B, nh, hs, T) -> (B, nh, T, T)\n        att = att.masked_fill(self.mask[:T,:T] == 0, float('-inf'))                     # causal mask\n        att = F.softmax(att, dim = -1)                                                  # softmax\n        att = att * w                                                                   # time-weighting\n        att = self.head_mix(att)                                                        # talking heads\n        x = att @ v                                                                     # (B, nh, T, T) * (B, nh, T, hs) -> (B, nh, T, hs)\n        x = x.transpose(1, 2).contiguous().view(B, T, -1)                               # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, C)"
        },
        {
            "comment": "This code defines two classes, RMSNorm and FixedNorm, both extending the nn.Module class in PyTorch. These classes are used as normalization layers for a GPT (Generative Pretrained Transformer) model. The RMSNorm class applies root mean square normalization to the input tensor x, while the FixedNorm class performs fixed normalization. Both classes return normalized inputs after applying a weight parameter to the output. The code also includes an initialization for the GPTConfig class which takes parameters such as vocab_size and ctx_len.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":353-384",
            "content": "        x = self.output(x) * self.time_gamma[:T, :]\n        return x\n########################################################################################################\n# The GPT Model with our blocks\n########################################################################################################\nclass RMSNorm(nn.Module):\n    def __init__(self, d):\n        super().__init__()\n        self.dd = d ** (-1. / 2)\n        self.weight = nn.Parameter(torch.ones(d))\n    def forward(self, x):\n        norm_x = x.norm(2, dim=-1, keepdim=True)\n        x_normed = x / (norm_x * self.dd + 1e-12)\n        return self.weight * x_normed\nclass FixedNorm(nn.Module):\n    def __init__(self, d):\n        super().__init__()\n        self.dd = d ** (-1. / 2)\n    def forward(self, x):\n        norm_x = x.norm(2, dim=-1, keepdim=True)\n        x_normed = x / (norm_x * self.dd + 1e-12)\n        return x_normed\n########################################################################################################\nclass GPTConfig:\n    def __init__(self, vocab_size, ctx_len, **kwargs):"
        },
        {
            "comment": "This code initializes a `Block` object with a specified configuration and layer ID. It includes multiple layers of normalization (LayerNorm) and different attention mechanisms depending on the model type specified in the configuration. The attention mechanisms can be RWKV_TimeMix, MHA_rotary, MHA_shift, or MHA_pro. These blocks are used to create a transformer model with adaptive layers for different tasks.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":385-413",
            "content": "        self.vocab_size = vocab_size\n        self.ctx_len = ctx_len\n        for k,v in kwargs.items():\n            setattr(self, k, v)\nclass Block(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.config = config\n        self.ln1 = nn.LayerNorm(config.n_embd)\n        self.ln2 = nn.LayerNorm(config.n_embd)\n        if config.model_type == 'RWKV':\n            # self.ln1 = FixedNorm(config.n_embd)\n            # self.ln2 = FixedNorm(config.n_embd)\n            self.attn = RWKV_TimeMix(config, layer_id)\n            self.mlp = RWKV_ChannelMix(config, layer_id)\n        elif config.model_type == 'MHA_rotary':\n            self.attn = MHA_rotary(config, layer_id)\n            self.mlp = GeGLU(config, layer_id)\n        elif config.model_type == 'MHA_shift':\n            self.attn = MHA_rotary(config, layer_id, time_shift=True)\n            self.mlp = GeGLU(config, layer_id, time_shift=True)\n        elif config.model_type == 'MHA_pro':\n            self.attn = MHA_pro(config, layer_id)"
        },
        {
            "comment": "The code initializes a GPT model with the given configuration. It includes an embedding layer, multiple blocks, final layer normalization, and attention parameters for context generation. The model type is checked to be 'RWKV' specifically.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":414-444",
            "content": "            self.mlp = RWKV_ChannelMix(config, layer_id)\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n        self.blocks = nn.Sequential(*[Block(config, i) for i in range(config.n_layer)])\n        self.ln_f = nn.LayerNorm(config.n_embd)\n        self.time_out = nn.Parameter(torch.ones(1,config.ctx_len,1)) # reduce confidence of early tokens\n        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.head_q = nn.Linear(config.n_embd, 256)\n        self.head_q.scale_init = 0.01\n        self.head_k = nn.Linear(config.n_embd, 256)\n        self.head_k.scale_init = 0.01\n        self.register_buffer(\"copy_mask\", torch.tril(torch.ones(config.ctx_len, config.ctx_len)))\n        self.ctx_len = config.ctx_len\n        if self.config.model_type == 'RWKV':"
        },
        {
            "comment": "This code initializes and configures the RWKV model. It initializes the weights using normal distribution with mean 0.0 and standard deviation 0.01. It also separates out parameters to be regularized by weight decay or not, whitelisting Linear layers while blacklisting RMSNorm, LayerNorm, and Embedding layers.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":445-471",
            "content": "            RWKV_Init(self, config)\n        else:\n            self.apply(self._init_weights)\n        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n    def get_ctx_len(self):\n        return self.ctx_len\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=0.01)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n    def configure_optimizers(self, train_config):\n        # separate out all parameters to those that will and won't experience regularizing weight decay\n        decay = set()\n        no_decay = set()\n        whitelist_weight_modules = (nn.Linear, )\n        blacklist_weight_modules = (RMSNorm, nn.LayerNorm, nn.Embedding)\n        for mn, m in self.named_modules():\n            for pn, p in m.named_parameters():\n                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n                if pn.endswith('bias') or ('time' in fpn) or ('head' in fpn):"
        },
        {
            "comment": "This code is organizing the model's parameters into two categories: decay and no_decay. It uses sets to ensure every parameter is considered, then creates optimizer groups for each category with different weight decay values. This helps in training by applying different learning rates to different parameters during backpropagation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":472-488",
            "content": "                    no_decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n                    decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                    no_decay.add(fpn)\n        # validate that we considered every parameter\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        inter_params = decay & no_decay\n        union_params = decay | no_decay\n        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n                                                    % (str(param_dict.keys() - union_params), )\n        optim_groups = [\n            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},"
        },
        {
            "comment": "This code defines a model and its forward pass. It takes an input index, performs token embedding and blocks transformations, then calculates query and key matrices for attention. It applies the attention mechanism by multiplying queries and keys, scales the result, fills zeros, and multiplies with one-hot encoded indices. It also reduces the confidence of early tokens and adds the attention result to the model output. Finally, it calculates the cross-entropy loss if targets are provided.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/model.py\":489-516",
            "content": "        ]\n        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas, eps=train_config.eps)\n        return optimizer\n    def forward(self, idx, targets=None):\n        B, T = idx.size()\n        assert T <= self.ctx_len, \"Cannot forward, because len(input) > model ctx_len.\"\n        x = self.tok_emb(idx)\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        q = self.head_q(x)[:,:T,:]\n        k = self.head_k(x)[:,:T,:]\n        c = (q @ k.transpose(-2, -1)) * (1.0 / 256)\n        c = c.masked_fill(self.copy_mask[:T,:T] == 0, 0)\n        c = c @ F.one_hot(idx, num_classes = self.config.vocab_size).float()\n        x = x * self.time_out[:, :T, :] # reduce confidence of early tokens\n        x = self.head(x) + c\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(x.view(-1, x.size(-1)), targets.view(-1))\n        return x, loss"
        }
    ]
}