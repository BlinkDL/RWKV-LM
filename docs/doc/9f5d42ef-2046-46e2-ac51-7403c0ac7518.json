{
    "summary": "The code defines a `my_save()` function for saving PyTorch Lightning model data using AWS S3, handles learning rate scheduling and logs progress. It also loads, reshapes and converts a model dictionary, performs interpolation, saves epoch information, and generates initial weights for model training.",
    "details": [
        {
            "comment": "This code defines a function `my_save()` that saves the model's data depending on the file path (`ff`) and calls another function `train_callback()`, which is a PyTorch Lightning callback class. The code also uses subprocess to move saved files to AWS S3 storage and incorporates the usage of Deepspeed for distributed training.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/trainer.py\":0-29",
            "content": "import os, math, time, datetime, subprocess\nimport torch\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities import rank_zero_info, rank_zero_only\ndef my_save(args, trainer, dd, ff):\n    if '14b-run1' in ff:\n        fn = ff.split('/')[-1]\n        fff = '/dev/shm/' + fn\n        torch.save(dd, fff)\n        subprocess.Popen(f\" aws s3 mv {fff} s3://rwkv-14b-4k/{fn} --quiet\", shell=True)\n    elif ('world/14b' in ff) or ('world/7b' in ff):\n        aa = ff.split('/')[1]\n        fn = ff.split('/')[-1]\n        fff = f'/dev/shm/{aa}-{fn}'\n        torch.save(dd, fff)\n        subprocess.Popen(f\" aws s3 mv {fff} s3://rwkv-world/{aa}-{fn} --quiet\", shell=True)\n    else:\n        if 'deepspeed_stage_3' in args.strategy:\n            trainer.save_checkpoint(ff, weights_only=True)\n        else:\n            torch.save(dd, ff)\nclass train_callback(pl.Callback):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):"
        },
        {
            "comment": "Code snippet handles learning rate (LR) scheduling and potentially clears GPU cache based on provided arguments. It calculates the real training step, determines if LR should be adjusted based on epoch count and warmup steps, and applies linear or exponential decay to adjust the learning rate. It also prints some info if it's the global zero trainer.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/trainer.py\":30-50",
            "content": "        args = self.args\n        # if args.cuda_cleanup > 0:\n        #     torch.cuda.empty_cache()\n        real_step = trainer.global_step + args.epoch_begin * args.epoch_steps\n        # LR schedule\n        w_step = args.warmup_steps\n        if args.lr_final == args.lr_init or args.epoch_count == 0:\n            lr = args.lr_init\n        else:\n            decay_step = real_step - args.my_pile_edecay * args.epoch_steps\n            decay_total = (args.epoch_count - args.my_pile_edecay) * args.epoch_steps\n            progress = (decay_step - w_step + 1) / (decay_total - w_step)\n            progress = min(1, max(0, progress))\n            if args.lr_final == 0 or args.lr_init == 0:  # linear decay\n                lr = args.lr_init + (args.lr_final - args.lr_init) * progress\n            else:  # exp decay\n                lr = args.lr_init * math.exp(math.log(args.lr_final / args.lr_init) * pow(progress, 1))\n            # if trainer.is_global_zero:\n            #     print(trainer.global_step, decay_step, decay_total, w_step, progress, lr)"
        },
        {
            "comment": "This code is setting the learning rate (lr) based on the exit tokens. If my_exit_tokens > 0, lr = lr_init * lr_mult. If my_exit_tokens < 0, lr = (lr + lr_init * lr_mult) / 2. The code also saves and exits if progress >= 1 or if global step is less than w_step. This appears to be part of a training process where the learning rate dynamically adjusts during training based on exit tokens.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/trainer.py\":52-71",
            "content": "        if args.my_exit_tokens != 0: # cosine decay\n            real_tokens = real_step * args.ctx_len * args.real_bsz\n            warmup_tokens = w_step * args.ctx_len * args.real_bsz\n            progress = (real_tokens - warmup_tokens) / (abs(args.my_exit_tokens) - warmup_tokens)\n            progress = max(0, min(1, progress))\n            lr_final_factor = args.lr_final / args.lr_init                \n            lr_mult = (0.5 + lr_final_factor / 2) + (0.5 - lr_final_factor / 2) * math.cos(math.pi * progress)\n            if args.my_exit_tokens > 0:\n                lr = args.lr_init * lr_mult\n            else:\n                lr = (lr + args.lr_init * lr_mult) / 2\n            if progress >= 1:\n                if (trainer.is_global_zero) or ('deepspeed_stage_3' in args.strategy):\n                    my_save(\n                        args, trainer,\n                        pl_module.state_dict(),\n                        f\"{args.proj_dir}/rwkv-final.pth\",\n                    )\n                    exit(0)\n        if trainer.global_step < w_step:"
        },
        {
            "comment": "The code sets the learning rate (lr) based on a decay formula and updates the weight decay (wd_now). It iterates through each param group, setting the lr and wd accordingly. If layerwise learning rate is enabled, it adjusts the lr further based on my_lr_scale. The trainer's current lr and wd are stored for future reference, and logging is initialized if this is the first global step.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/trainer.py\":72-96",
            "content": "            lr = lr * (0.2 + 0.8 * trainer.global_step / w_step)\n        if args.weight_decay_final > 0:\n            wd_now = args.weight_decay * math.exp(math.log(args.weight_decay_final / args.weight_decay) * progress)\n        else:\n            wd_now = args.weight_decay\n        for param_group in trainer.optimizers[0].param_groups:\n            if param_group[\"weight_decay\"] > 0:\n                param_group[\"weight_decay\"] = wd_now\n            if args.layerwise_lr > 0:\n                param_group[\"lr\"] = lr * param_group[\"my_lr_scale\"]\n                # print(param_group[\"lr\"], param_group[\"my_lr_scale\"])\n            else:\n                param_group[\"lr\"] = lr\n        trainer.my_lr = lr\n        trainer.my_wd = wd_now\n        # rank_zero_info(f\"{real_step} {lr}\")\n        if trainer.global_step == 0:\n            if trainer.is_global_zero:  # logging\n                trainer.my_loss_sum = 0\n                trainer.my_loss_count = 0\n                trainer.my_log = open(args.proj_dir + \"/train_log.txt\", \"a\")"
        },
        {
            "comment": "Writes log information to file, tries printing strategy configuration but handles exceptions, flushes the log, initializes W&B if enabled. In on_train_batch_end, calculates token per step, determines real step, logs only on global_step 0 (zero-based indexing).",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/trainer.py\":97-119",
            "content": "                trainer.my_log.write(f\"NEW RUN {args.my_timestamp}\\n{vars(self.args)}\\n\")\n                try:\n                    print(f\"\\n{trainer.strategy.config}\\n\")\n                    trainer.my_log.write(f\"{trainer.strategy.config}\\n\")\n                except:\n                    pass\n                trainer.my_log.flush()\n                if len(args.wandb) > 0:\n                    print(\"Login to wandb...\")\n                    import wandb\n                    wandb.init(\n                        project=args.wandb,\n                        name=args.run_name + \" \" + args.my_timestamp,\n                        config=args,\n                        save_code=False,\n                    )\n                    trainer.my_wandb = wandb\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        args = self.args\n        token_per_step = args.ctx_len * args.real_bsz\n        real_step = trainer.global_step + args.epoch_begin * args.epoch_steps\n        if trainer.is_global_zero:  # logging"
        },
        {
            "comment": "Code block calculates the time taken for training step, real iterations per second (it/s), and kilo-tokens per second (Kt/s). It also logs the learning rate (lr) and current loss for progress tracking. If using PyTorch 2 version, it retrieves loss value differently. It also logs the epoch loss and optionally sends information to W&B if specified in arguments.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/trainer.py\":120-141",
            "content": "            t_now = time.time_ns()\n            kt_s = 0\n            try:\n                t_cost = (t_now - trainer.my_time_ns) / 1e9\n                kt_s = token_per_step / t_cost / 1000\n                self.log(\"REAL it/s\", 1.0 / t_cost, prog_bar=True, on_step=True)\n                self.log(\"Kt/s\", kt_s, prog_bar=True, on_step=True)\n            except:\n                pass\n            trainer.my_time_ns = t_now\n            if pl.__version__[0]=='2':\n                trainer.my_loss = outputs[\"loss\"]\n            else:\n                trainer.my_loss = trainer.my_loss_all.float().mean().item()\n            trainer.my_loss_sum += trainer.my_loss\n            trainer.my_loss_count += 1\n            trainer.my_epoch_loss = trainer.my_loss_sum / trainer.my_loss_count\n            self.log(\"lr\", trainer.my_lr, prog_bar=True, on_step=True)\n            self.log(\"loss\", trainer.my_epoch_loss, prog_bar=True, on_step=True)\n            # self.log(\"s\", real_step, prog_bar=True, on_step=True)\n            if len(args.wandb) > 0:"
        },
        {
            "comment": "This code is responsible for logging training metrics and saving the model checkpoint. It checks if it's the global zero or using DeepSpeed, and saves the model state dictionary as \"rwkv-final.pth\" when the current step meets certain conditions related to batch size and random steps. The logging includes loss, learning rate, weight decay, and token count per second (if applicable).",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/trainer.py\":142-163",
            "content": "                lll = {\"loss\": trainer.my_loss, \"lr\": trainer.my_lr, \"wd\": trainer.my_wd, \"Gtokens\": real_step * token_per_step / 1e9}\n                if kt_s > 0:\n                    lll[\"kt/s\"] = kt_s\n                trainer.my_wandb.log(lll, step=int(real_step))\n        if (trainer.is_global_zero) or ('deepspeed_stage_3' in args.strategy): # save pth\n            if args.magic_prime > 0:\n                expand_factor = 2 if args.my_qa_mask > 0 else 1\n                if int(real_step) == int(args.magic_prime * expand_factor // args.real_bsz) - 1 + int(args.my_random_steps):\n                    to_save_dict = pl_module.state_dict()\n                    my_save(\n                        args, trainer,\n                        to_save_dict,\n                        f\"{args.proj_dir}/rwkv-final.pth\",\n                    )\n    def on_train_epoch_start(self, trainer, pl_module):\n        args = self.args\n        if pl.__version__[0]=='2':\n            dataset = trainer.train_dataloader.dataset\n        else:\n            dataset = trainer.train_dataloader.dataset.datasets"
        },
        {
            "comment": "This code snippet is part of the trainer class and defines a method 'on_train_epoch_end'. It asserts that the dataset has the name 'MyDataset' and assigns values to the dataset object properties. If global_zero or 'deepspeed_stage_3' strategy is used, it saves the model state at specified epoch intervals.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/trainer.py\":164-181",
            "content": "        assert \"MyDataset\" in str(dataset)\n        dataset.global_rank = trainer.global_rank\n        dataset.real_epoch = int(args.epoch_begin + trainer.current_epoch)\n        dataset.world_size = trainer.world_size\n        # print(f'########## world_size {dataset.world_size} global_rank {dataset.global_rank} real_epoch {dataset.real_epoch} ##########')\n    def on_train_epoch_end(self, trainer, pl_module):\n        args = self.args\n        to_save_dict = {}\n        if (trainer.is_global_zero) or ('deepspeed_stage_3' in args.strategy):  # save pth\n            if (args.epoch_save > 0 and trainer.current_epoch % args.epoch_save == 0) or (trainer.current_epoch == args.epoch_count - 1):\n                if args.data_type == 'wds_img':\n                    raw_dict = pl_module.state_dict()\n                    for k in raw_dict:\n                        if k.startswith('encoder.') or k.startswith('decoder.'):\n                            to_save_dict[k] = raw_dict[k]\n                else:\n                    to_save_dict = pl_module.state_dict()"
        },
        {
            "comment": "Trying to save the model, log epoch information, and optionally exit if the current epoch exceeds a specified limit. Additionally, there's a function for generating initial weights, combining with pre-existing ones if available.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/trainer.py\":182-207",
            "content": "                try:\n                    my_save(\n                        args, trainer,\n                        to_save_dict,\n                        f\"{args.proj_dir}/rwkv-{args.epoch_begin + trainer.current_epoch}.pth\",\n                    )\n                except Exception as e:\n                    print('Error\\n\\n', e, '\\n\\n')\n        if trainer.is_global_zero:  # logging\n            trainer.my_log.write(f\"{args.epoch_begin + trainer.current_epoch} {trainer.my_epoch_loss:.6f} {math.exp(trainer.my_epoch_loss):.4f} {trainer.my_lr:.8f} {datetime.datetime.now()} {trainer.current_epoch}\\n\")\n            trainer.my_log.flush()\n            trainer.my_loss_sum = 0\n            trainer.my_loss_count = 0\n            if (args.epoch_begin + trainer.current_epoch) >= args.my_exit:\n                exit(0)\n@rank_zero_only\ndef generate_init_weight(model, init_weight_name):\n    mm = model.generate_init_weight()\n    if model.args.my_pile_stage == 1:\n        if len(model.args.load_model) > 0:\n            print(f\"Combine weights from {model.args.load_model}...\")"
        },
        {
            "comment": "The code loads a dictionary from the specified model file, checks if all keys are present in 'mm' dictionary, and reshapes the loaded source to match the shape of existing data in 'mm'. If source shape doesn't match, it performs a linear interpolation to fit the new data. Finally, converts the source to float and cpu numpy array.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/trainer.py\":208-232",
            "content": "            load_dict = torch.load(model.args.load_model, map_location=\"cpu\")\n            for k in load_dict:\n                try:\n                    assert k in mm\n                except:\n                    print('missing', k)\n                    exit(0)\n                src = load_dict[k]\n                try:\n                    mm[k] = src.reshape(mm[k].shape)\n                except:\n                    tmp = mm[k].squeeze().clone()\n                    print(k, src.shape, '-->', mm[k].shape)\n                    ss = src.shape[0]\n                    dd = tmp.shape[0]\n                    for i in range(dd):\n                        pos = i / dd * ss\n                        if pos >= ss - 1:\n                            tmp[i] = src[ss-1]\n                        else:\n                            p0 = int(math.floor(pos))\n                            ii = pos - p0\n                            tmp[i] = src[p0] * (1-ii) + src[p0+1] * (ii)\n                    mm[k] = tmp.reshape(mm[k].shape)\n                    sss = src.squeeze().float().cpu().numpy()"
        },
        {
            "comment": "This code segment prints parts of 'sss' and 'mmm', saves 'mm' to a file, and if in stage 1, suggests moving on to stage 2. It seems to be part of a model training process where it displays data, saves an intermediate model checkpoint, and moves to the next phase.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/trainer.py\":233-242",
            "content": "                    print(sss[:10], '...', sss[-10:])\n                    mmm = mm[k].squeeze().float().cpu().numpy()\n                    print(mmm[:10], '...', mmm[-10:])\n    print(f\"Save to {init_weight_name}...\")\n    torch.save(mm, init_weight_name)\n    if model.args.my_pile_stage == 1:\n        print(\"Done. Now go for stage 2.\")\n        exit(0)"
        }
    ]
}