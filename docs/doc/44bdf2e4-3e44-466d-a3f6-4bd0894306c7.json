{
    "summary": "This code trains an RWKV-LM and GPT language model, optimizes GPU memory usage, saves partial models, and trains with specified parameters. The trained model's state dictionary is saved with identifiers for file identification.",
    "details": [
        {
            "comment": "The code is importing necessary modules and setting up the environment for training a language model called RWKV-LM. It also includes a special debug mode option that can be enabled but is currently set to False, and it sets seed for deterministic training. The code imports classes from other files, including GPT model and TrainerConfig for configuration settings. The logging module is configured to display specific information in the console.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/train.py\":0-21",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport os\n# if False: # True False ---> Set to False if you don't understand it\n#     print(\"\\n\\n[[[ SPECIAL DEBUG MODE FOR MYSELF. DON'T ENABLE THIS IF YOU DON'T UNDERSTAND IT ]]]\\n\\n\")\n#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n#     import src.utils\n#     src.utils.set_seed(42) # make training deterministic (including dataloader). if you are doing this, remember to change seed when you load a model (otherwise the dataloader loads old samples)\nimport logging\nimport datetime\nfrom src.model import GPT, GPTConfig\nfrom src.trainer import Trainer, TrainerConfig\nfrom src.utils import Dataset\nimport torch\nimport numpy as np\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)\nlogging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\","
        },
        {
            "comment": "This code sets up training parameters for the RWKV-v3 language model. It specifies the data file, model size, batch size, and enables CUDA features for efficient GPU utilization. The model type can be either 'RWKV' or 'RWKV-ffnPre', and there is an optional headQK dimension to improve loss. This configuration allows for training deeper models with better performance.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/train.py\":22-47",
            "content": "                    datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO,)\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cuda.matmul.allow_tf32 = True\n### Step 1: set training data ##########################################################################\ndatafile = \"../data/enwik8\" # your data\ndatafile_encoding = 'utf-8'\n# datafile_encoding = 'utf-16le'\n### Step 2: set model size #############################################################################\n# ----> test deeper models (n_layer at least 12) to see the advantage of RWKV-3 over RWKV-2\nctx_len = 1024 # increase T_MAX in model.py if your ctx_len > 1024\nn_layer = 6\nn_embd = 512\n# 'RWKV' (better for English) or 'RWKV-ffnPre' (better in some cases)\nmodel_type = 'RWKV'\n# ---> there is a RWKV_HEAD_QK_DIM in model.py and model_run.py\n# set it to 256, then it's using my headQK trick (similar to a tiny attention) to improve loss\n# set it to 0, then it's a pure RNN (attention-free)\n### Step 3: set batch size #############################################################################"
        },
        {
            "comment": "This code snippet provides guidance for setting batch_size, learning rate, and the number of mini-epochs in RWKV-v3's training script. It suggests adjusting batch_size to optimize GPU memory usage and offers suggestions on how to set learning rate values for different model sizes. The comments also advise saving a partially trained model for resuming later, with instructions on how to implement this.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/train.py\":49-64",
            "content": "# ---> batch_size must be divisible by B_GROUP_FORWARD and B_GROUP_BACKWARD in model.py\n# for example, if your batch_size = 20, you can set B_GROUP_FORWARD = 4, B_GROUP_BACKWARD = 2\n# if you see \"CUDA out of memory\", reduce batch_size. Use nvidia-smi to find the highest value for your GPU.\nbatch_size = 12\n### Step 4: set learning rate, number of mini-epochs #######################################################\n# By default we are using exponential LR decay.\n#\n# Here are my suggestions for training a good model.\n# Let's say you will train a L6-D512 model.\n# 1) Set lr_init = lr_final = 8e-4. Let it run for some mini-epochs, until the improvement of loss become slow.\n# 2) Check epoch_save_frequency and make sure the partially-trained model is saved. Ctrl+C to stop the run.\n# 3) Set lr_init = 8e-4, lr_final = 1e-5, warmup_tokens = ctx_len * batch_size * 50, betas = (0.9, 0.999).\n# 4) Search for \"torch.load\" here and modify it to load the partially-trained model. Continue the training.\n# \n# For L12-D768, set lr_init = 6e-4. For L24-D1024, set lr_init = 4e-4. For L24-D2048, set lr_init = 3e-4."
        },
        {
            "comment": "This code initializes learning rate, defines training parameters, loads the dataset, and sets up data processing for training RWKV language model. It specifies learning rate initialization, final value, number of epochs, fixed batch length, save frequency, gradient norm clipping, warmup tokens, betas for Adam optimizer, epsilon for stability, and number of workers for data loading.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/train.py\":66-95",
            "content": "lr_init = 8e-4 # we can use larger lr because of preLN\nlr_final = 1e-5\n# the mini-epoch is very short and of fixed length (length = ctx_len * epoch_length_fixed tokens)\nn_epoch = 500\nepoch_length_fixed = 10000\n# 0 = never, 1 = every mini-epoch, 2 = every two mini-epochs, ...\nepoch_save_frequency = 10\nepoch_save_path = 'trained-'\n########################################################################################################\ngrad_norm_clip = 1.0\nwarmup_tokens = ctx_len * batch_size * 0\nbetas = (0.9, 0.99)\neps = 4e-9\nnum_workers = 0\n########################################################################################################\n# Load data\n########################################################################################################\nprint('loading data... ' + datafile)\ntrain_dataset = Dataset(open(\n    datafile, \"r\", encoding=datafile_encoding).read(), ctx_len, epoch_length_fixed)\n########################################################################################################"
        },
        {
            "comment": "This code snippet is responsible for training a model using the GPT architecture. It loads a previously trained model, specifies the trainer configuration, and then proceeds to train the model with the specified number of epochs, batch size, learning rate, and other hyperparameters.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/train.py\":96-111",
            "content": "# Train model\n########################################################################################################\nif __name__ == '__main__':\n    model = GPT(GPTConfig(train_dataset.vocab_size, train_dataset.ctx_len, model_type=model_type,\n                          n_layer=n_layer, n_embd=n_embd)).cuda()\n    ### ---> load a trained model <---\n    # m2 = torch.load('trained-61.pth')\n    # model.load_state_dict(m2)\n    print('model', model_type, 'epoch', n_epoch, 'batchsz', batch_size, 'betas',\n          betas, 'eps', eps, 'ctx', ctx_len, 'layer', n_layer, 'embd', n_embd, )\n    tconf = TrainerConfig(model_type=model_type, max_epochs=n_epoch, batch_size=batch_size,\n                          learning_rate=lr_init, lr_decay=True, lr_final=lr_final, betas=betas, eps=eps, grad_norm_clip=grad_norm_clip,\n                          warmup_tokens=warmup_tokens, final_tokens=n_epoch*len(train_dataset)*ctx_len, num_workers=num_workers, epoch_save_frequency=epoch_save_frequency, epoch_save_path=epoch_save_path)"
        },
        {
            "comment": "Saving the model's state dictionary after training, including epoch number and run name, with a timestamp for file identification.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v3/train.py\":112-117",
            "content": "    trainer = Trainer(model, train_dataset, None, tconf)\n    trainer.train()\n    torch.save(model.state_dict(), 'trained-' + str(n_epoch) + '-' + trainer.get_run_name() +\n               '-' + datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S') + '.pth')"
        }
    ]
}