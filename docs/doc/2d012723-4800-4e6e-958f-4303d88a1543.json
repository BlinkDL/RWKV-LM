{
    "summary": "The code sets up an RWKV Language Model environment for English/Chinese, creates a multilingual chatbot in Python with response generation commands and independent question prompts, handling user input and generating text from the model.",
    "details": [
        {
            "comment": "Loading RWKV Language Model and setting up environment with specified device, float mode, and tokenizer for English/Chinese language.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":0-30",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nprint('Loading...')\nfrom src.model_run import RWKV_RNN\nimport numpy as np\nimport os, copy, types, gc, sys\nimport torch\nfrom src.utils import TOKENIZER\ntry:\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = sys.argv[1]\nexcept:\n    pass\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cuda.matmul.allow_tf32 = True\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)\nCHAT_LANG = 'English' # English Chinese\nWORD_NAME = [\n    \"20B_tokenizer.json\",\n    \"20B_tokenizer.json\",\n]  # [vocab, vocab] for Pile model\nUNKNOWN_CHAR = None\ntokenizer = TOKENIZER(WORD_NAME, UNKNOWN_CHAR=UNKNOWN_CHAR)\nargs = types.SimpleNamespace()\nargs.RUN_DEVICE = \"cuda\"  # 'cpu' (already very fast) // 'cuda'\nargs.FLOAT_MODE = \"fp16\" # fp32 (good for CPU) // fp16 (recommended for GPU) // bf16 (less accurate)"
        },
        {
            "comment": "Code is setting hyperparameters for RWKV-v4neo model, including vocabulary size, dimensions, and layers. It also has multiple conditionals to change these values based on the chat language, and provides aliases for user and bot.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":31-58",
            "content": "args.vocab_size = 50277\nargs.head_qk = 0\nargs.pre_ffn = 0\nargs.grad_cp = 0\nargs.my_pos_emb = 0\nargs.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-14b/RWKV-4-Pile-14B-20230108-5170'\nargs.n_layer = 40\nargs.n_embd = 5120\nargs.ctx_len = 1024\n# args.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-7b/RWKV-4-Pile-7B-20221115-8047'\n# args.n_layer = 32\n# args.n_embd = 4096\n# args.ctx_len = 1024\n# args.MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-3b/RWKV-4-Pile-3B-20221008-8023'\n# args.n_layer = 32\n# args.n_embd = 2560\n# args.ctx_len = 1024\nif CHAT_LANG == 'English':\n    user = \"User\"\n    bot = \"Bot\"\n    interface = \":\"\n    # The following is a verbose and detailed conversation between an AI assistant called {bot}, and a human user called {user}. {bot} is intelligent, knowledgeable, wise and polite.\n    # The following is a conversation between a highly knowledgeable and intelligent AI called {bot}, and a human called {user}. In the following interactions, {user} and {bot} converse in natural language, and {bot}"
        },
        {
            "comment": "This code contains a sample conversation between an AI assistant named {bot} and a user. The assistant provides answers to questions, is respectful and polite, and always tells the truth. The code also includes instructions for using the chat functionality and commands like 'say something' to initiate the conversation with the bot.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":58-85",
            "content": " do its best to answer {user}'s questions. {bot} is respectful, polite and inclusive. {bot} knows a lot, and always tells the truth.\n    init_prompt = f'''\nThe following is a verbose and detailed conversation between an AI assistant called {bot}, and a human user called {user}. {bot} is intelligent, knowledgeable, wise and polite.\n{user}{interface} french revolution what year\n{bot}{interface} The French Revolution started in 1789, and lasted 10 years until 1799.\n{user}{interface} 3+5=?\n{bot}{interface} The answer is 8.\n{user}{interface} guess i marry who ?\n{bot}{interface} Only if you tell me more about yourself - what are your interests?\n{user}{interface} solve for a: 9-a=2\n{bot}{interface} The answer is a = 7, because 9 - 7 = 2.\n{user}{interface} wat is lhc\n{bot}{interface} LHC is a high-energy particle collider, built by CERN, and completed in 2008. They used it to confirm the existence of the Higgs boson in 2012.\n'''\n    HELP_MSG = '''Commands:\nsay something --> chat with bot. use \\\\n for new line."
        },
        {
            "comment": "This code is for a chatbot implemented in Python using the RWKV-v4neo language model. It supports Chinese and English languages, allowing users to ask questions or generate free text. The code provides specific parameters and prompts for Chinese and English interactions, with the ability to reset the bot's memory, and instructions on how to use the chatbot effectively.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":86-117",
            "content": "+alt --> alternate chat reply\n+reset --> reset chat\n+gen YOUR PROMPT --> free generation with any prompt. use \\\\n for new line.\n+qa YOUR QUESTION --> free generation - ask any question (just ask the question). use \\\\n for new line.\n+more --> continue last free generation (only for +gen / +qa)\n+retry --> retry last free generation (only for +gen / +qa)\nNow talk with the bot and enjoy. Remember to +reset periodically to clean up the bot's memory. Use RWKV-4 14B for best results.\nThis is not instruct-tuned for conversation yet, so don't expect good quality. Better use +gen for free generation.\n'''\nelif CHAT_LANG == 'Chinese':\n    args.MODEL_NAME = '/fsx/BlinkDL/CODE/_PUBLIC_/RWKV-LM/RWKV-v4neo/7-run3z/rwkv-293'\n    args.n_layer = 32\n    args.n_embd = 4096\n    args.ctx_len = 1024\n    user = \"Q\"\n    bot = \"A\"\n    interface = \":\"\n    init_prompt = '''\nQ: \u4f01\u9e45\u4f1a\u98de\u5417\uff1f\nA: \u4f01\u9e45\u662f\u4e0d\u4f1a\u98de\u7684\u3002\u5b83\u4eec\u7684\u7fc5\u8180\u4e3b\u8981\u7528\u4e8e\u6e38\u6cf3\u548c\u5e73\u8861\uff0c\u800c\u4e0d\u662f\u98de\u884c\u3002\nQ: \u897f\u74dc\u662f\u4ec0\u4e48\nA: \u897f\u74dc\u662f\u4e00\u79cd\u5e38\u89c1\u7684\u6c34\u679c\uff0c\u662f\u4e00\u79cd\u591a\u5e74\u751f\u8513\u751f\u85e4\u672c\u690d\u7269\u3002\u897f\u74dc\u7684\u679c\u5b9e\u5448\u5706\u5f62\u6216\u5375\u5f62\uff0c\u901a\u5e38\u662f\u7eff\u8272\u7684\uff0c\u91cc\u9762\u6709\u7ea2\u8272\u6216\u9ec4\u8272\u7684\u8089\u548c\u5f88\u591a\u7684\u7c7d\u3002\u897f\u74dc\u5473\u751c\uff0c\u591a\u5403\u53ef\u4ee5\u589e\u52a0\u6c34\u5206\uff0c\u662f\u590f\u5b63\u975e\u5e38\u53d7\u6b22\u8fce\u7684\u6c34\u679c\u4e4b\u4e00\u3002\n'''\n    HELP_MSG = '''\u6307\u4ee4:"
        },
        {
            "comment": "This code is part of an interactive chatbot that uses the RWKV model for natural language processing. It supports various commands to generate or reset responses, and prompts for independent questions. The code loads the model and defines a function run_rnn() which takes input tokens, processes them through the RWKV model, and generates output.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":118-155",
            "content": "\u76f4\u63a5\u8f93\u5165\u5185\u5bb9 --> \u548c\u673a\u5668\u4eba\u804a\u5929\uff0c\u7528\\\\n\u4ee3\u8868\u6362\u884c\n+alt --> \u8ba9\u673a\u5668\u4eba\u6362\u4e2a\u56de\u7b54\n+reset --> \u91cd\u7f6e\u5bf9\u8bdd\n+gen \u67d0\u67d0\u5185\u5bb9 --> \u7eed\u5199\u4efb\u4f55\u4e2d\u82f1\u6587\u5185\u5bb9\uff0c\u7528\\\\n\u4ee3\u8868\u6362\u884c\n+qa \u67d0\u67d0\u95ee\u9898 --> \u95ee\u72ec\u7acb\u7684\u95ee\u9898\uff08\u5ffd\u7565\u4e0a\u4e0b\u6587\uff09\uff0c\u7528\\\\n\u4ee3\u8868\u6362\u884c\n+more --> \u7ee7\u7eed +gen / +qa \u7684\u56de\u7b54\n+retry --> \u6362\u4e2a +gen / +qa \u7684\u56de\u7b54\n\u73b0\u5728\u53ef\u4ee5\u8f93\u5165\u5185\u5bb9\u548c\u673a\u5668\u4eba\u804a\u5929\uff08\u6ce8\u610f\u5b83\u4e0d\u600e\u4e48\u61c2\u4e2d\u6587\uff0c\u5b83\u53ef\u80fd\u66f4\u61c2\u82f1\u6587\uff09\u3002\u8bf7\u7ecf\u5e38\u4f7f\u7528 +reset \u91cd\u7f6e\u673a\u5668\u4eba\u8bb0\u5fc6\u3002\n'''\n# Load Model\nos.environ[\"RWKV_RUN_DEVICE\"] = args.RUN_DEVICE\nMODEL_NAME = args.MODEL_NAME\nprint(f'loading... {MODEL_NAME}')\nmodel = RWKV_RNN(args)\nmodel_tokens = []\ncurrent_state = None\n########################################################################################################\ndef run_rnn(tokens, newline_adj = 0):\n    global model_tokens, current_state\n    for i in range(len(tokens)):\n        model_tokens += [int(tokens[i])]\n        if i == len(tokens) - 1:\n            out, current_state = model.forward(model_tokens, current_state)\n        else:\n            current_state = model.forward(model_tokens, current_state, preprocess_only = True)\n    # print(f'### model ###\\n[{tokenizer.tokenizer.decode(model_tokens)}]')\n    out[0] = -999999999  # disable <|endoftext|>"
        },
        {
            "comment": "This code is used for saving and loading all-state, running inference, and replying to messages. The all-state contains the RNN state, model tokens, and output. Inference is run on a prompt, and the output is saved and printed with decoded tokens. The reply_msg function can be used to reply to received messages.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":156-193",
            "content": "    out[187] += newline_adj\n    # if newline_adj > 0:\n    #     out[15] += newline_adj / 2 # '.'\n    return out\nall_state = {}\ndef save_all_stat(srv, name, last_out):\n    n = f'{name}_{srv}'\n    all_state[n] = {}\n    all_state[n]['out'] = last_out\n    all_state[n]['rnn'] = copy.deepcopy(current_state)\n    all_state[n]['token'] = copy.deepcopy(model_tokens)\ndef load_all_stat(srv, name):\n    global model_tokens, current_state\n    n = f'{name}_{srv}'\n    current_state = copy.deepcopy(all_state[n]['rnn'])\n    model_tokens = copy.deepcopy(all_state[n]['token'])\n    return all_state[n]['out']\n########################################################################################################\n# Run inference\nprint(f'\\nRun prompt...')\nout = run_rnn(tokenizer.tokenizer.encode(init_prompt))\ngc.collect()\ntorch.cuda.empty_cache()\nsave_all_stat('', 'chat_init', out)\nsrv_list = ['dummy_server']\nfor s in srv_list:\n    save_all_stat(s, 'chat', out)\nprint(f'### prompt ###\\n[{tokenizer.tokenizer.decode(model_tokens)}]\\n')\ndef reply_msg(msg):"
        },
        {
            "comment": "This code defines a function `on_message()` that processes incoming messages. It checks if the message is longer than 1000 tokens, applies temperature and top-p sampling parameters, handles reset requests, and possibly generates new text or asks questions based on message content. The processing includes loading and saving chat state, printing messages to console, and sending appropriate replies.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":194-229",
            "content": "    print(f'{bot}{interface} {msg}\\n')\ndef on_message(message):\n    global model_tokens, current_state\n    srv = 'dummy_server'\n    msg = message.replace('\\\\n','\\n').strip()\n    if len(msg) > 1000:\n        reply_msg('your message is too long (max 1000 tokens)')\n        return\n    x_temp = 1.0\n    x_top_p = 0.85\n    if (\"-temp=\" in msg):\n        x_temp = float(msg.split(\"-temp=\")[1].split(\" \")[0])\n        msg = msg.replace(\"-temp=\"+f'{x_temp:g}', \"\")\n        # print(f\"temp: {x_temp}\")\n    if (\"-top_p=\" in msg):\n        x_top_p = float(msg.split(\"-top_p=\")[1].split(\" \")[0])\n        msg = msg.replace(\"-top_p=\"+f'{x_top_p:g}', \"\")\n        # print(f\"top_p: {x_top_p}\")\n    if x_temp <= 0.2:\n        x_temp = 0.2\n    if x_temp >= 5:\n        x_temp = 5\n    if x_top_p <= 0:\n        x_top_p = 0\n    if msg == '+reset':\n        out = load_all_stat('', 'chat_init')\n        save_all_stat(srv, 'chat', out)\n        reply_msg(\"Chat reset.\")\n        return\n    elif msg[:5].lower() == '+gen ' or msg[:4].lower() == '+qa ' or msg.lower() == '+more' or msg.lower() == '+retry':"
        },
        {
            "comment": "This code checks if the message starts with \"+gen\", \"+qa\", or \"+more\" and performs corresponding actions. If \"+gen\" is found, it generates a response based on the given prompt. If \"+qa\" is found, it loads previous chat context and continues the conversation. If \"+more\" is found, it displays more content related to the current context. It utilizes tokenizer for encoding messages and run_rnn to generate responses. The generated responses are saved in 'gen_0' state for future reference.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":231-254",
            "content": "        if msg[:5].lower() == '+gen ':\n            new = '\\n' + msg[5:].strip()\n            # print(f'### prompt ###\\n[{new}]')\n            current_state = None\n            out = run_rnn(tokenizer.tokenizer.encode(new))\n            save_all_stat(srv, 'gen_0', out)\n        elif msg[:4].lower() == '+qa ':\n            out = load_all_stat('', 'chat_init')\n            real_msg = msg[4:].strip()\n            new = f\"{user}{interface} {real_msg}\\n\\n{bot}{interface}\"\n            # print(f'### qa ###\\n[{new}]')\n            out = run_rnn(tokenizer.tokenizer.encode(new))\n            save_all_stat(srv, 'gen_0', out)\n            # new = f\"\\nThe following is an excellent Q&A session consists of detailed and factual information.\\n\\nQ: What is 3+5?\\nA: The answer is 8.\\n\\nQ: {msg[9:].strip()}\\nA:\"\n            # print(f'### prompt ###\\n[{new}]')\n            # current_state = None\n            # out = run_rnn(tokenizer.tokenizer.encode(new))\n            # save_all_stat(srv, 'gen_0', out)\n        elif msg.lower() == '+more':"
        },
        {
            "comment": "The code is performing the following tasks:\n1. Loading statistics from a server and saving them, with error handling for any exceptions that might occur.\n2. Handling user input, specifically \"+retry\" command, by loading saved statistics from an alternative source if an exception occurs during the initial load.\n3. Generating tokens using tokenizer's sample_logits function, considering various parameters like temperature and top_p values.\n4. Running RNN model on generated tokens to process them, handling different cases based on user input.\n5. Printing generated output, handling special characters, and updating the tracking variables for the next iteration.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":255-288",
            "content": "            try:\n                out = load_all_stat(srv, 'gen_1')\n                save_all_stat(srv, 'gen_0', out)\n            except:\n                return\n        elif msg.lower() == '+retry':\n            try:\n                out = load_all_stat(srv, 'gen_0')\n            except:\n                return\n        begin = len(model_tokens)\n        out_last = begin\n        for i in range(150):\n            token = tokenizer.sample_logits(\n                out,\n                model_tokens,\n                args.ctx_len,\n                temperature=x_temp,\n                top_p_usual=x_top_p,\n                top_p_newline=x_top_p,\n            )\n            if msg[:4].lower() == '+qa ':\n                out = run_rnn([token], newline_adj=-1)\n            else:\n                out = run_rnn([token])\n            xxx = tokenizer.tokenizer.decode(model_tokens[out_last:])\n            if '\\ufffd' not in xxx:\n                print(xxx, end='', flush=True)\n                out_last = begin + i + 1\n        print('\\n')\n        # send_msg = tokenizer.tokenizer.decode(model_tokens[begin:]).strip()"
        },
        {
            "comment": "This code handles two cases: sending a message and adding a message to the chat history. If the message is \"+alt\", it loads the previous chat state. Otherwise, it loads the current chat state, generates a new message using an RNN model, saves the new message in the \"chat_pre\" file, and prints part of the output. The code also determines the appropriate newline adjustment for the generated text. It uses tokenizer.sample_logits to generate the next token based on the current state of the model.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":289-318",
            "content": "        # print(f'### send ###\\n[{send_msg}]')\n        # reply_msg(send_msg)\n        save_all_stat(srv, 'gen_1', out)\n    else:\n        if msg.lower() == '+alt':\n            try:\n                out = load_all_stat(srv, 'chat_pre')\n            except:\n                return\n        else:\n            out = load_all_stat(srv, 'chat')\n            new = f\"{user}{interface} {msg}\\n\\n{bot}{interface}\"\n            # print(f'### add ###\\n[{new}]')\n            out = run_rnn(tokenizer.tokenizer.encode(new), newline_adj=-999999999)\n            save_all_stat(srv, 'chat_pre', out)\n        begin = len(model_tokens)\n        out_last = begin\n        print(f'{bot}{interface}', end='', flush=True)\n        for i in range(999):\n            if i <= 0:\n                newline_adj = -999999999\n            elif i <= 30:\n                newline_adj = (i - 30) / 10\n            elif i <= 130:\n                newline_adj = 0\n            else:\n                newline_adj = (i - 130) * 0.25 # MUST END THE GENERATION\n            token = tokenizer.sample_logits("
        },
        {
            "comment": "This code is responsible for generating text from a model, tokenizing the output, and printing it until a newline or specified context length is reached. It also handles breaking the loop when encountering a double newline or specific user/bot messages.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":319-343",
            "content": "                out,\n                model_tokens,\n                args.ctx_len,\n                temperature=x_temp,\n                top_p_usual=x_top_p,\n                top_p_newline=x_top_p,\n            )\n            out = run_rnn([token], newline_adj=newline_adj)\n            xxx = tokenizer.tokenizer.decode(model_tokens[out_last:])\n            if '\\ufffd' not in xxx:\n                print(xxx, end='', flush=True)\n                out_last = begin + i + 1\n            send_msg = tokenizer.tokenizer.decode(model_tokens[begin:])\n            if '\\n\\n' in send_msg:\n                send_msg = send_msg.strip()\n                break\n            # send_msg = tokenizer.tokenizer.decode(model_tokens[begin:]).strip()\n            # if send_msg.endswith(f'{user}{interface}'): # warning: needs to fix state too !!!\n            #     send_msg = send_msg[:-len(f'{user}{interface}')].strip()\n            #     break\n            # if send_msg.endswith(f'{bot}{interface}'):\n            #     send_msg = send_msg[:-len(f'{bot}{interface}')].strip()"
        },
        {
            "comment": "The code handles user input, continuously prompts the user for messages, passes them to a function `on_message()`, and saves chat data if necessary. If no valid input is provided, it prints an error message.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/chat.py\":344-360",
            "content": "            #     break\n        # print(f'{model_tokens}')\n        # print(f'[{tokenizer.tokenizer.decode(model_tokens)}]')\n        # print(f'### send ###\\n[{send_msg}]')\n        # reply_msg(send_msg)\n        save_all_stat(srv, 'chat', out)\nprint(HELP_MSG)\nwhile True:\n    msg = input(f'{user}{interface} ')\n    if len(msg.strip()) > 0:\n        on_message(msg)\n    else:\n        print('Erorr: please say something')"
        }
    ]
}