{
    "summary": "The code initializes the RWKV v2-RNN language model, optimizes it for performance, and utilizes CUDA for efficient computation. It implements time-decay parameters, defines a GPT model with RWKV blocks, and sets up optimization methods.",
    "details": [
        {
            "comment": "The code imports necessary libraries and defines constants for the RWKV v2-RNN Language Model, which is a neural network language model. It loads a CUDA kernel called \"timex\" for efficient computation on GPUs using Torch's `torch.utils.cpp_extension.load` function. The constants T_MAX, B_GROUP_FORWARD, and B_GROUP_BACKWARD are set to optimize performance.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":0-22",
            "content": "########################################################################################################\n# The RWKV v2-RNN Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nfrom torch.utils.cpp_extension import load\nimport math\nimport numpy as np\nimport logging\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nlogger = logging.getLogger(__name__)\n########################################################################################################\n# CUDA Kernel\n########################################################################################################\nT_MAX = 1024          # increase this if your ctx_len > 1024\nB_GROUP_FORWARD = 4   # set to 8 for best performance\nB_GROUP_BACKWARD = 2  # set to 2 for best performance\ntimex_cuda = load(name=\"timex\", sources=[\"cuda/timex_op.cpp\", \"cuda/timex_cuda.cu\"],\n                  verbose=True, extra_cuda_cflags=['--use_fast"
        },
        {
            "comment": "Defines a TimeX class that implements the Time-X function using CUDA for efficient computation. The class takes in weights (w), kernel (k), batch size (B), number of channels (C), sequence length (T), and epsilon (eps) as input, and returns the output tensor (wk). It also ensures all parameters meet certain conditions before forwarding and backward propagation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":22-46",
            "content": "_math', '--extra-device-vectorization', f'-DTmax={T_MAX}', f'-DBF={B_GROUP_FORWARD}', f'-DBB={B_GROUP_BACKWARD}'])\nclass TimeX(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, w, k, B, C, T, eps):\n        ctx.B = B\n        ctx.C = C\n        ctx.T = T\n        assert ctx.T % 4 == 0 and ctx.T <= T_MAX and ctx.B % B_GROUP_FORWARD == 0 and ctx.B % B_GROUP_BACKWARD == 0\n        w = w.contiguous()\n        k = k.contiguous()\n        ctx.save_for_backward(w, k)\n        wk = torch.empty((B, C, T), device='cuda',\n                         memory_format=torch.contiguous_format)\n        timex_cuda.forward(w, k, wk, eps, B, C, T)\n        return wk\n    @staticmethod\n    def backward(ctx, gwk):\n        assert ctx.T % 4 == 0 and ctx.T <= T_MAX and ctx.B % B_GROUP_FORWARD == 0 and ctx.B % B_GROUP_BACKWARD == 0\n        w, k = ctx.saved_tensors\n        gw = torch.empty((ctx.B, ctx.C, ctx.T), device='cuda',\n                         memory_format=torch.contiguous_format)\n        gk = torch.empty((ctx.B, ctx.C, ctx.T), device='cuda',"
        },
        {
            "comment": "This code initializes the RWKV model by setting specific parameters and configurations for each layer. It uses fancy initialization to set the weights of linear and embedding layers in the module. The code also defines constants like RWKV_K_CLAMP, RWKV_K_EPS, and RWKV_HEAD_QK_DIM for further calculations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":47-72",
            "content": "                         memory_format=torch.contiguous_format)\n        timex_cuda.backward(w, k, gwk.contiguous(), gw,\n                            gk, ctx.B, ctx.C, ctx.T)\n        return (gw.sum(dim=0), gk, None, None, None, None)\n########################################################################################################\n# RWKV: RWKV Time-mix + RWKV Channel-mix\n########################################################################################################\nRWKV_K_CLAMP = 60  # e^60 = 1e26\nRWKV_K_EPS = 1e-16\nRWKV_HEAD_QK_DIM = 256\ndef RWKV_Init(module, config):  # fancy initialization of all lin & emb layer in the module\n    for m in module.modules():\n        if not isinstance(m, (nn.Linear, nn.Embedding)):\n            continue\n        with torch.no_grad():\n            name = '[unknown weight]'\n            for name, parameter in module.named_parameters():  # find the name of the weight\n                if id(m.weight) == id(parameter):\n                    break\n            shape = m.weight.data.shape"
        },
        {
            "comment": "The code adjusts the gain and scale of layer weights in a neural network model, depending on the type and shape of the layer. It initializes embeddings with a small scale and linear layers with zero or identity matrices, based on specific conditions. The final projection has a different scale, while zero initialization is used if the scale is set to -999 and gain is 0.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":73-100",
            "content": "            gain = 1.0\n            scale = 1.0  # extra scale for gain\n            if isinstance(m, nn.Embedding):\n                gain = math.sqrt(max(shape[0], shape[1]))\n                if shape[0] == config.vocab_size and shape[1] == config.n_embd:  # token emb?\n                    scale = 1e-4\n                else:\n                    scale = 0\n            if isinstance(m, nn.Linear):\n                if m.bias is not None:\n                    m.bias.data.zero_()\n                if shape[0] > shape[1]:\n                    gain = math.sqrt(shape[0] / shape[1])\n                if shape[0] == config.vocab_size and shape[1] == config.n_embd:  # final projection?\n                    scale = 0.5\n            if hasattr(m, 'scale_init'):\n                scale = m.scale_init\n            # print(str(shape[0]).ljust(5), str(shape[1]).ljust(5), f'{round(scale,2):g}'.ljust(4), name)\n            gain *= scale\n            if scale == -999:\n                nn.init.eye_(m.weight)\n            elif gain == 0:\n                # zero init is great for some RWKV matrices"
        },
        {
            "comment": "This code initializes the time_w curves for a RWKV_TimeMix module in the model. It uses different initialization methods depending on the gain value. If the gain is zero, it initializes the weight as zeros. If the gain is positive, it initializes the weight with orthogonal values. If the gain is negative, it initializes the weight with normal distribution mean 0 and standard deviation of -scale. The time_w curves are initialized for better convergence using a decay speed variable and calculating f1 and f2 based on layer ID.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":101-128",
            "content": "                nn.init.zeros_(m.weight)\n            elif gain > 0:\n                nn.init.orthogonal_(m.weight, gain=gain)\n            else:\n                nn.init.normal_(m.weight, mean=0.0, std=-scale)\nclass RWKV_TimeMix(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n        self.ctx_len = config.ctx_len\n        self.n_embd = config.n_embd\n        attn_sz = config.n_embd\n        ############# fancy init of time_w curves ###################################\n        f1_begin = 3.0\n        f1_end = 1.2\n        f2_begin = 0.65\n        f2_end = 0.4\n        with torch.no_grad():  # initial time_w curves for better convergence\n            decay_speed = torch.ones(attn_sz, 1)\n            first_sa_layer_id = 1\n            for h in range(attn_sz):\n                f1 = f1_begin + (layer_id-first_sa_layer_id) / \\\n                    (config.n_layer-1-first_sa_layer_id) * (f1_end - f1_begin)\n                f2 = f2_begin + (layer_id-first_sa_layer_id) / \\"
        },
        {
            "comment": "This code initializes the time-decay and related parameters for a transformer model. It sets `self.time_decay` as a learnable parameter, calculates `self.time_curve`, and initializes `self.time_first` and `self.time_shift`. The decay speed is adjusted based on the layer id to control how quickly time decays in the attention mechanism.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":129-146",
            "content": "                    (config.n_layer-1-first_sa_layer_id) * (f2_end - f2_begin)\n                if layer_id == first_sa_layer_id:\n                    f1 += 0.5\n                if layer_id == config.n_layer-2:\n                    f2 = 0.4\n                if layer_id == config.n_layer-1:\n                    f2 = 0.37\n                decay_speed[h][0] = math.pow(f2, h / (attn_sz-1) * 7) * f1\n        self.time_decay = nn.Parameter(torch.log(decay_speed)) # will use exp(self.time_decay) to ensure time_decay > 0\n        self.time_curve = torch.tensor(\n            [-(config.ctx_len - 2 - i) for i in range(config.ctx_len-1)]).unsqueeze(0)\n        self.time_curve = self.time_curve.to('cuda')\n        self.time_first = nn.Parameter(torch.ones(attn_sz, 1) * math.log(0.3))\n        #############################################################################\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        with torch.no_grad():  # init to \"shift half of the channels\"\n            ww = torch.ones(1, 1, config.n_embd)"
        },
        {
            "comment": "This code initializes a model for the RWKV-v2 architecture. It defines the time_mix parameter, and several linear layers (key, value, receptance, output). The forward function applies these layers to input x, scales key and receptance to zero, clamps key within certain bounds, exponentials it, and performs a weighted sum with value before returning the result.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":147-176",
            "content": "            for i in range(config.n_embd // 2):\n                ww[0, 0, i] = 0\n        self.time_mix = nn.Parameter(ww)\n        self.key = nn.Linear(config.n_embd, attn_sz, bias=False)\n        self.value = nn.Linear(config.n_embd, attn_sz, bias=False)\n        self.receptance = nn.Linear(config.n_embd, attn_sz, bias=False)\n        self.output = nn.Linear(attn_sz, config.n_embd, bias=False)\n        self.key.scale_init = 0\n        self.receptance.scale_init = 0\n        self.output.scale_init = 0\n    def forward(self, x):\n        B, T, C = x.size()\n        x = x * self.time_mix + self.time_shift(x) * (1 - self.time_mix)\n        k = self.key(x).transpose(-1, -2)\n        v = self.value(x).transpose(-1, -2)\n        r = self.receptance(x)\n        # RWKV_K_CLAMP can be removed if the CUDA kernel substracts the correct k_max for each k (I will do this later)\n        k = torch.clamp(k, max=RWKV_K_CLAMP)\n        k = torch.exp(k)\n        kv = k * v\n        self.time_w = torch.cat(\n            [torch.exp(self.time_decay) * self.time_curve, self.time_first], dim=-1)"
        },
        {
            "comment": "This code defines a RWKV Channel Mix module for a transformer model. It initializes channel mixing parameters, applies time shifting and mixing operations, and then performs a key-value attention mechanism to produce the final output. The TimeX class is applied to compute weighting factors based on time steps.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":177-206",
            "content": "        w = torch.exp(self.time_w)\n        wkv = TimeX.apply(w, kv, B, C, T, 0)\n        # RWKV_K_EPS can be removed if the CUDA kernel sets 0/0 = 0 (I will do this later)\n        wk = TimeX.apply(w, k, B, C, T, RWKV_K_EPS)\n        rwkv = torch.sigmoid(r) * (wkv / wk).transpose(-1, -2)\n        rwkv = self.output(rwkv)\n        return rwkv\nclass RWKV_ChannelMix(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        with torch.no_grad():  # init to \"shift half of the channels\"\n            x = torch.ones(1, 1, config.n_embd)\n            for i in range(config.n_embd // 2):\n                x[0, 0, i] = 0\n        self.time_mix = nn.Parameter(x)\n        hidden_sz = 4 * config.n_embd\n        self.key = nn.Linear(config.n_embd, hidden_sz, bias=False)\n        self.receptance = nn.Linear(config.n_embd, config.n_embd, bias=False)\n        self.value = nn.Linear(hidden_sz, config.n_embd, bias=False)\n        self.value.scale_init = 0"
        },
        {
            "comment": "The code is defining a GPT model with RWKV blocks. It includes an initialization for the receptance scale, forward function to process input data, and a class for the GPTConfig and Block modules. The RWKV-ffnPre model type initializes additional layers in the first layer.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":207-241",
            "content": "        self.receptance.scale_init = 0\n    def forward(self, x):\n        x = x * self.time_mix + self.time_shift(x) * (1 - self.time_mix)\n        k = self.key(x)\n        k = torch.square(torch.relu(k))\n        kv = self.value(k)\n        rkv = torch.sigmoid(self.receptance(x)) * kv\n        return rkv\n########################################################################################################\n# The GPT Model with our blocks\n########################################################################################################\nclass GPTConfig:\n    def __init__(self, vocab_size, ctx_len, **kwargs):\n        self.vocab_size = vocab_size\n        self.ctx_len = ctx_len\n        for k, v in kwargs.items():\n            setattr(self, k, v)\nclass Block(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.config = config\n        self.layer_id = layer_id\n        self.ln1 = nn.LayerNorm(config.n_embd)\n        self.ln2 = nn.LayerNorm(config.n_embd)\n        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':"
        },
        {
            "comment": "The code defines a GPT model with layers and a forward function. It initializes an embedding layer, a sequence of blocks, and output and attention layers. The forward function applies layer normalization and feeds the input through the attention and feed-forward networks. If the layer_id is 0 and the config's model_type is 'RWKV-ffnPre', it adds the ffnPre to the input for better performance in some cases.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":242-274",
            "content": "            self.ffnPre = RWKV_ChannelMix(config, layer_id+1000)\n        else:\n            self.att = RWKV_TimeMix(config, layer_id)\n        self.ffn = RWKV_ChannelMix(config, layer_id)\n    def forward(self, x):\n        x = self.ln1(x)\n        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':\n            x = x + self.ffnPre(x)  # better in some cases\n        else:\n            x = x + self.att(x)\n        x = self.ln2(x)\n        x = x + self.ffn(x)\n        return x\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.step = 0\n        self.config = config\n        self.emb = nn.Embedding(config.vocab_size, config.n_embd)\n        self.blocks = nn.Sequential(*[Block(config, i)\n                                    for i in range(config.n_layer)])\n        self.ln_out = nn.LayerNorm(config.n_embd)\n        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.head_q = nn.Linear(config.n_embd, RWKV_HEAD_QK_DIM, bias=False)\n        self.head_q.scale_init = 0"
        },
        {
            "comment": "This code defines a model class with initialization and optimization configuration methods. It initializes the linear layer, copy mask, and registers buffers for the model. It also sets the context length (ctx_len) and calls RWKV_Init function to initialize other parameters. The _init_weights method is used to set weights in the layers. Finally, the configure_optimizers method separates model parameters into those with and without weight decay for optimization.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":275-303",
            "content": "        self.head_k = nn.Linear(config.n_embd, RWKV_HEAD_QK_DIM, bias=False)\n        self.head_k.scale_init = 0.1\n        self.register_buffer(\"copy_mask\", torch.tril(\n            torch.ones(config.ctx_len, config.ctx_len)))\n        self.ctx_len = config.ctx_len\n        RWKV_Init(self, config)\n        logger.info(\"number of parameters: %e\", sum(p.numel()\n                    for p in self.parameters()))\n    def get_ctx_len(self):\n        return self.ctx_len\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear)):\n            module.weight.data.normal_(mean=0.0, std=0.01)\n        if isinstance(module, (nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=1e-5)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n    def configure_optimizers(self, train_config):\n        # separate out all parameters to those that will and won't experience regularizing weight decay\n        decay = set()\n        no_decay = set()\n        for mn, m in self.named_modules():  # here we disable weight_decay"
        },
        {
            "comment": "Looping through model parameters, separating them into decay and no_decay groups. Creating optimizer groups for no_decay params with weight_decay=0, then initializing an Adam optimizer for training.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":304-328",
            "content": "            for pn, p in m.named_parameters():\n                fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name\n                no_decay.add(fpn)\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        inter_params = decay & no_decay\n        union_params = decay | no_decay\n        assert len(\n            inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n            % (str(param_dict.keys() - union_params), )\n        optim_groups = [\n            {\"params\": [param_dict[pn]\n                        for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n        ]\n        optimizer = torch.optim.Adam(\n            optim_groups, lr=train_config.learning_rate, betas=train_config.betas, eps=train_config.eps)\n        return optimizer\n    def forward(self, idx, targets=None):\n        self.step += 1\n        B, T = idx.size()"
        },
        {
            "comment": "This code is part of a model's forward pass. It checks if the input length (T) is within the model's context length, embeds the input, passes it through multiple blocks, applies layer normalization, and performs attention calculations for query and key tensors. If targets are provided, it calculates the cross-entropy loss.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/src/model.py\":329-348",
            "content": "        assert T <= self.ctx_len, \"Cannot forward, because len(input) > model ctx_len.\"\n        x = self.emb(idx)\n        x = self.blocks(x)\n        x = self.ln_out(x)\n        q = self.head_q(x)[:, :T, :]\n        k = self.head_k(x)[:, :T, :]\n        c = (q @ k.transpose(-2, -1)) * (1.0 / RWKV_HEAD_QK_DIM)\n        c = c.masked_fill(self.copy_mask[:T, :T] == 0, 0)\n        c = c @ F.one_hot(idx, num_classes=self.config.vocab_size).float()\n        x = self.head(x) + c\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(x.view(-1, x.size(-1)), targets.view(-1))\n        return x, loss"
        }
    ]
}