{
    "summary": "This code initializes an RWKV model for generating text, evaluates it on a given dataset considering parameters like layers and temperature, and trains the model in a script. It checks for existing files, loads or runs the model, debugs output metrics, converts output to text using tokenizer, and displays time taken.",
    "details": [
        {
            "comment": "The code is initializing the RWKV v2-RNN Language Model, setting context length (ctx_len), number of layers (n_layer), embedding dimension (n_embd), and model type (either 'RWKV' or 'RWKV-ffnPre'). It also specifies the name of a pre-trained model (MODEL_NAME) and a vocabulary file (WORD_NAME). The code is written in Python, using TensorFlow and PyTorch libraries for deep learning tasks.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/run.py\":0-30",
            "content": "# -*- coding:utf-8 -*-\n########################################################################################################\n# The RWKV v2-RNN Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport numpy as np\nimport math\nimport time\nimport types\nimport copy\nimport torch\nfrom torch.nn import functional as F\nfrom src.utils import TOKENIZER, Dataset\nfrom src.model_run import RWKV_RNN\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cuda.matmul.allow_tf32 = True\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)\n### Step 1: set model ##################################################################################\nctx_len = 1024\nn_layer = 6\nn_embd = 512\nmodel_type = 'RWKV'           # 'RWKV' or 'RWKV-ffnPre'\n# your trained model\nMODEL_NAME = 'trained-31'\nWORD_NAME = 'vocab'           # the .json vocab (generated by train.py\n# ########## Uncomment these to test my 27M params enwik8 model ##########"
        },
        {
            "comment": "The code initializes an RWKV model with the specified name, device (CPU or CUDA), model type, number of layers, embedding size, and context length. The context is set as a prompt, with a given number of trials and length per trial. Temperature and top-p values are also defined for generating text. Unknown characters in the context are denoted by a space character.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/run.py\":31-57",
            "content": "# MODEL_NAME = 'enwik8-ppl1.65-6064-1024-RWKV-6-512-2022-03-25-21-05-13'\n# WORD_NAME = 'enwik8-vocab'\n# EVAL_DATA = 'enwik8'  # uncomment this for EVAL MODE (no text generation)\n# ########################################################################\n# --> set UNKNOWN_CHAR to the rarest token in your vocab.json <--\n# --> all unknown tokens in your context will be denoted by it <--\nUNKNOWN_CHAR = ' '   # here we just set it to [space] for simplicity\nRUN_DEVICE = 'cpu'   # 'cpu' (already very fast) or 'cuda'\nDEBUG_DEBUG = False  # True False - show softmax output\n### Step 2: set context ################################################################################\ncontext = \"\\nIn the\"       # ==> this is your prompt\nNUM_TRIALS = 999\nLENGTH_PER_TRIAL = 500\nTEMPERATURE = 1.0\ntop_p = 0.7\ntop_p_newline = 0.9\n########################################################################################################\nprint(f'Loading {MODEL_NAME}...')\nmodel = RWKV_RNN(MODEL_NAME, RUN_DEVICE, model_type, n_layer, n_embd, ctx_len)"
        },
        {
            "comment": "This code is evaluating the model on a given dataset (EVAL_DATA) by taking random samples, creating context sequences, and calculating average loss over ctx_len. The model's performance is assessed based on this average loss across sampled data.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/run.py\":58-88",
            "content": "tokenizer = TOKENIZER(WORD_NAME, UNKNOWN_CHAR=UNKNOWN_CHAR)\n########################################################################################################\nif 'EVAL_DATA' in vars() or 'EVAL_DATA' in globals():\n    print('Evaluating on ' + EVAL_DATA + ' ...')\n    data = open(EVAL_DATA, \"r\", encoding='utf-8').read()\n    loss_table = np.zeros(ctx_len)\n    N_SAMPLE = 1000\n    for iii in range(N_SAMPLE):\n        pos = np.random.randint(0, len(data) - ctx_len-1)\n        context = data[pos:pos+ctx_len+1]\n        ctx = [tokenizer.stoi.get(s, tokenizer.UNKNOWN_CHAR) for s in context]\n        model.clear()\n        for i in range(1, ctx_len+1):\n            x = ctx[:i]\n            out = model.run(x)\n            prob = F.softmax(torch.tensor(out), dim=-1)\n            loss_table[i-1] += -math.log(prob[ctx[i]])\n        print(f'Tested {iii+1} samples: avg_loss over ctx_len =',\n              np.mean(loss_table) / (iii+1))\n    exit(0)\n########################################################################################################"
        },
        {
            "comment": "This code performs the first run of RWKV-v2 model on a prompt, with future versions expected to be faster. It uses an RNN for processing and saves/loads model states between trials. The input prompt is tokenized, and its length affects the number of trials performed. The DEBUG_DEBUG flag can skip the first run.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/run.py\":90-119",
            "content": "context = tokenizer.refine_context(context)\nprint('\\nYour prompt has ' + str(len(context)) + ' tokens.')\nprint('\\n--> Currently the first run takes a while if your prompt is long, as we are using RNN to process the prompt. This will be much faster in future versions. <--\\n')\nfor TRIAL in range(1 if DEBUG_DEBUG else NUM_TRIALS):\n    t_begin = time.time_ns()\n    src_len = len(context)\n    ctx = [tokenizer.stoi.get(s, tokenizer.UNKNOWN_CHAR) for s in context]\n    print(('-' * 30) + context, end='')\n    model.clear()\n    if TRIAL == 0:\n        init_state = types.SimpleNamespace()\n        for i in range(src_len):\n            x = ctx[:i+1]\n            if i == src_len - 1:\n                init_state.out = model.run(x)\n            else:\n                model.run(x)\n        model.save(init_state)\n    else:\n        model.load(init_state)\n    for i in range(src_len, src_len + (1 if DEBUG_DEBUG else LENGTH_PER_TRIAL)):\n        x = ctx[:i+1]\n        x = x[-ctx_len:]\n        if i == src_len:\n            out = copy.deepcopy(init_state.out)"
        },
        {
            "comment": "This code is part of a model training script. It checks if there's an existing file for the same context and loads it if it exists, or runs the model on input data if not. The debugging option prints certain metrics about the model's output. The output is then converted to text using the tokenizer and printed. Finally, the time taken for this operation is displayed.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v2-RNN/run.py\":120-132",
            "content": "        else:\n            out = model.run(x)\n        if DEBUG_DEBUG:\n            print('model', np.array(x), '==>', np.array(\n                out), np.max(out), np.min(out))\n        char = tokenizer.sample_logits(out, x, ctx_len, temperature=TEMPERATURE,\n                                       top_p_usual=top_p, top_p_newline=top_p_newline)\n        char = char.item()\n        print(tokenizer.itos[int(char)], end='', flush=True)\n        ctx += [char]\n    t_end = time.time_ns()\n    print(\"\\n----------\", round((t_end - t_begin) / (10 ** 9), 2), end='s ')"
        }
    ]
}