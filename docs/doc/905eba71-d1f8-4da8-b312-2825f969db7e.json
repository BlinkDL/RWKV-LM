{
    "summary": "The code sets up a RWKV model trainer, configures the Trainer object with DeepSpeed, and enables FP16 mode for efficient RWKV-v4 language model training.",
    "details": [
        {
            "comment": "This code is an import section for a language model, specifically RWKV-v4. It sets up logging, imports necessary modules, and includes options to set debug mode or seed the training process for determinism. The code is part of the 'train.py' file in the RWKV-LM repository.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":0-18",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport os\nimport logging, types\nfrom src.utils import Dataset\nimport torch\nimport numpy as np\nfrom src.binidx import MMapIndexedDataset\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)\nlogging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n                    datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO,)\n# if False: # True False ---> Set to False if you don't understand it\n#     print(\"\\n\\n[[[ SPECIAL DEBUG MODE FOR MYSELF. DON'T ENABLE THIS IF YOU DON'T UNDERSTAND IT ]]]\\n\\n\")\n#     import src.utils\n#     src.utils.set_seed(42) # make training deterministic (including dataloader). if you are doing this, remember to change seed when you load a model (otherwise the dataloader loads old samples)"
        },
        {
            "comment": "This code sets the training data and configuration for a text generation model. It uses the RWKV-LM's Pile model and can be fine-tuned with different model names and types. The datafile specifies where to find the training data, and datafile_encoding specifies the file encoding format. If EXPRESS_PILE_MODE is True, the code uses a specific pile model for fine-tuning.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":20-41",
            "content": "########################################################################################################\n# Step 1: set training data & cfg\n########################################################################################################\nEXPRESS_PILE_MODE = False # True: express mode for fine-tuning a pile model // False: usual training\nEXPRESS_PILE_MODEL_NAME = 'RWKV-4-Pile-169M-20220807-8023'\nEXPRESS_PILE_MODEL_TYPE = 'RWKV-4-Pile-169M'\n# EXPRESS_PILE_MODEL_NAME = 'RWKV-4-Pile-430M-20220808-8066'\n# EXPRESS_PILE_MODEL_TYPE = 'RWKV-4-Pile-430M'\n# EXPRESS_PILE_MODEL_NAME = 'RWKV-4-Pile-1B5-20220903-8040'\n# EXPRESS_PILE_MODEL_TYPE = 'RWKV-4-Pile-1B5'\n########################################################################################################\ndatafile = \"../data/enwik8\" # your data\ndatafile_encoding = 'utf-8' # 'utf-8' / 'utf-16le' / 'numpy' (for fine-tuning pile models) / 'binidx' (the Megatron-LM 'binidx' format)\n# datafile = 'my-gpt_seq_document'\n# datafile_encoding = 'binidx'\nif EXPRESS_PILE_MODE:"
        },
        {
            "comment": "This code sets the datafile, datafile_encoding, and VOCAB_SIZE environment variables for RWKV-v4 training. It also suggests a procedure for multi-GPU training involving setting RWKV_NUM_GPUS and other parameters. The supported precisions are 'bf16' and 'fp16'.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":42-64",
            "content": "    datafile = 'train.npy' # use 'prepare-data.py' in https://github.com/BlinkDL/RWKV-v2-RNN-Pile/tree/main/RWKV-v3 to tokenize .txt into .npy\n    datafile_encoding = 'numpy'\n#\n# set VOCAB_SIZE = 0 (auto-compute) if you are training a char-level LM from scratch\n# set VOCAB_SIZE = 50277 for fine-tuning pile models\n# set VOCAB_SIZE = your_vocab_size for 'binidx' data\n#\nos.environ['VOCAB_SIZE'] = '0'\nif EXPRESS_PILE_MODE:\n    os.environ['VOCAB_SIZE'] = '50277'\n#\n# Currently it's slow to initialize a new model. Hence I suggest this procedure for multi-GPU training:\n# 1) set RWKV_NUM_GPUS = '1' and let it run for 1 miniEpoch and it will save a trained-1.pth\n# 2) set RWKV_NUM_GPUS = '8' (or your #GPU), batch_size = single_gpu_batchsz * RWKV_NUM_GPUS,\n#    EPOCH_BEGIN = 1, LOAD_MODEL = True, and it will load 'trained-1.pth' and continue the training from it\n#\nos.environ['RWKV_NUM_GPUS'] = '1' # num of GPUs to use\n#\n# 'bf16' (fast & stable)\n# 'fp16' (fast & will overflow after training a large model for very long. can be solved in the future)"
        },
        {
            "comment": "The code is setting the environment variables for the training process. It allows choosing different float modes and deciding whether to use DeepSpeed for improved efficiency or not. Additionally, it sets the model details such as number of layers, embedding size, and context length.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":65-87",
            "content": "# 'tf32' (decent speed & stable)\n# 'fp32' (!!!very slow!!! only for verification)\nos.environ['RWKV_FLOAT_MODE'] = 'bf16'\nos.environ['RWKV_DEEPSPEED'] = '1' # Use DeepSpeed? 0 = False, 1 = True\nif int(os.environ['RWKV_NUM_GPUS']) == 1: # Usually you don't need DeepSpeed for 1 GPU training.\n    os.environ['RWKV_DEEPSPEED'] = '0'    # However, sometimes DeepSpeed saves VRAM even for 1 GPU training. So you shall try it.\nos.environ['USE_WANDB'] = '0' # wandb logging. 0 = False, 1 = True\n########################################################################################################\n# Step 2: set model details\n########################################################################################################\nEPOCH_BEGIN = 0 # begins with miniEpoch = EPOCH_BEGIN\nLOAD_MODEL = False # shall we load the #EPOCH_BEGIN model and continue the training from it?\nn_layer = 6\nn_embd = 512\nctx_len = 1024 # increase T_MAX in src/model.py if your ctx_len is longer\nmodel_type = 'RWKV' # 'RWKV' or 'RWKV-ffnPre' (sometimes better)"
        },
        {
            "comment": "The code is setting the hyperparameters and model configuration for the RWKV-v4 language model based on the chosen EXPRESS_PILE_MODEL_TYPE. It defines the number of layers, embedding dimension, context length, and batch size according to the selected model type. The code also advises reducing the batch size if encountering \"CUDA out of memory\" error.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":89-113",
            "content": "# there is also a RWKV_HEAD_QK_DIM in model.py and model_run.py\n# set it to 256, then it's using my headQK trick (a tiny attention) to improve loss\n# set it to 0, then it's a pure RNN (attention-free)\nif EXPRESS_PILE_MODE:\n    LOAD_MODEL = True\n    if EXPRESS_PILE_MODEL_TYPE == 'RWKV-4-Pile-169M':\n        n_layer = 12\n        n_embd = 768\n        ctx_len = 1024\n    elif EXPRESS_PILE_MODEL_TYPE == 'RWKV-4-Pile-430M':\n        n_layer = 24\n        n_embd = 1024\n        ctx_len = 1024\n    elif EXPRESS_PILE_MODEL_TYPE == 'RWKV-4-Pile-1B5':\n        n_layer = 24\n        n_embd = 2048\n        ctx_len = 1024\n########################################################################################################\n# Step 3: set batch size & learning rate etc.\n########################################################################################################\n# if you see \"CUDA out of memory\", reduce batch_size. Use nvidia-smi to find the highest value for your GPU.\nbatch_size = 12 * int(os.environ['RWKV_NUM_GPUS'])"
        },
        {
            "comment": "The code provides suggestions for training an RWKV-v4 model. It recommends setting the initial and final learning rates (lr_init and lr_final) based on the model size, saving partially trained models with epoch_save_frequency, and adjusting the epoch length and batch size according to available GPU resources.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":114-134",
            "content": "assert (batch_size % int(os.environ['RWKV_NUM_GPUS']) == 0)\n# By default we are using exponential LR decay.\n# Here are my suggestions for training.\n# Let's say you are training a L6-D512 model.\n# 1) Set lr_init = lr_final = 8e-4. Let it run for some mini-epochs, until you feel like reducing LR.\n# 2) Check epoch_save_frequency and make sure the partially-trained model is saved. Ctrl+C to stop the run.\n# 3) Set lr_init = 8e-4, lr_final = 1e-5, betas = (0.9, 0.999).\n# 4) Set EPOCH_BEGIN & LOAD_MODEL to load the partially-trained model. Continue the training.\n# \n# For L12-D768, set lr_init = 6e-4. For L24-D1024, set lr_init = 4e-4. For L24-D2048, set lr_init = 3e-4.\nlr_init = 8e-4\nlr_final = 1e-5\n# the mini-epoch is very short and of fixed length (length = ctx_len * epoch_length_fixed tokens)\nn_epoch = 500\nepoch_length_fixed = (10000 // batch_size) * batch_size # feel free to increase it if you have lots of GPU\n# epoch_save_frequency 0 = never, 1 = every mini-epoch, 2 = every two mini-epochs, ...\nepoch_save_frequency = 10"
        },
        {
            "comment": "This code sets up various parameters for model training, such as learning rate, epoch number, warmup tokens, betas for optimizer, epsilon, number of data loader workers, and number of GPUs. It also configures some environment variables and enables CUDA benchmarking based on the floating point mode. The code then assigns a model name based on the epoch number or the express pile mode if enabled.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":135-167",
            "content": "epoch_save_path = 'trained-'\nif EXPRESS_PILE_MODE:\n    if EXPRESS_PILE_MODEL_TYPE == 'RWKV-4-Pile-169M':\n        lr_init = 2e-5\n    else:\n        lr_init = 1e-5\n    lr_final = 1e-5\n    n_epoch = 100000\n### misc stuffs ########################################################################################\nif LOAD_MODEL and EPOCH_BEGIN > 0: # we are not saving gradients, so let's have some warmup if we load a model\n    warmup_tokens = 50 * ctx_len * batch_size // NUM_GPUS\nelse:\n    warmup_tokens = 0\nbetas = (0.9, 0.99) # set betas = (0.9, 0.999) if your model has been trained for a while\neps = 1e-8\nnum_workers = 1 # DataLoader worker. I only tested num_workers = 1\nNUM_GPUS = int(os.environ['RWKV_NUM_GPUS'])\nos.environ['RWKV_LOAD_MODEL'] = str(LOAD_MODEL)\nMODEL_NAME = epoch_save_path + str(EPOCH_BEGIN)\nif EXPRESS_PILE_MODE:\n    betas = (0.9, 0.999)\n    MODEL_NAME = EXPRESS_PILE_MODEL_NAME\ntorch.backends.cudnn.benchmark = True\nif os.environ['RWKV_FLOAT_MODE'] == 'fp32':\n    torch.backends.cudnn.allow_tf32 = False"
        },
        {
            "comment": "Loading and preparing data for training the model, with support for different data file formats (binidx, numpy, or text file). Ensures CUDA and cudnn settings are properly configured based on the environment.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":168-189",
            "content": "    torch.backends.cuda.matmul.allow_tf32 = False\nelse:\n    torch.backends.cudnn.allow_tf32 = True\n    torch.backends.cuda.matmul.allow_tf32 = True\n########################################################################################################\n# Load data\n########################################################################################################\nprint(f'loading {datafile_encoding} data... ' + datafile)\nif datafile_encoding == 'binidx':\n    train_dataset = Dataset(MMapIndexedDataset(datafile), ctx_len, epoch_length_fixed)\nelif datafile_encoding == 'numpy':\n    train_dataset = Dataset(np.load(datafile).astype('int'), ctx_len, epoch_length_fixed)\nelse:\n    train_dataset = Dataset(open(datafile, \"r\", encoding=datafile_encoding).read(), ctx_len, epoch_length_fixed)\n########################################################################################################\n# Train model\n########################################################################################################\nif __name__ == '__main__':"
        },
        {
            "comment": "This code sets up a trainer for the RWKV model. It prints out information such as the model type, float mode, epoch count, and other relevant parameters before configuring the Trainer object with these details. The code also checks if DeepSpeed should be used based on an environment variable and sets up the Trainer accordingly, using either FP16 precision for GPU acceleration or regular floating point precision for CPU-only execution.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":190-208",
            "content": "    from src.trainer import Trainer, TrainerConfig\n    print('\\nmodel', model_type, os.environ['RWKV_FLOAT_MODE'], 'epoch', n_epoch, 'batchsz', batch_size, 'betas',\n          betas, 'eps', eps, 'ctx', ctx_len, 'layer', n_layer, 'embd', n_embd, '\\n')\n    tconf = TrainerConfig(model_type=model_type, max_epochs=n_epoch, batch_size=batch_size,\n                          learning_rate=lr_init, lr_decay=True, lr_final=lr_final, betas=betas, eps=eps,\n                          warmup_tokens=warmup_tokens, final_tokens=n_epoch*len(train_dataset)*ctx_len, num_workers=num_workers, epoch_save_frequency=epoch_save_frequency, epoch_save_path=epoch_save_path)\n    m_cfg = types.SimpleNamespace()\n    m_cfg.model_type = model_type\n    m_cfg.n_layer = n_layer\n    m_cfg.n_embd = n_embd\n    m_cfg.EPOCH_BEGIN = EPOCH_BEGIN\n    m_cfg.LOAD_MODEL = LOAD_MODEL\n    m_cfg.MODEL_NAME = MODEL_NAME\n    if os.environ['RWKV_DEEPSPEED'] == '0':\n        if os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n            trainer = Trainer(devices=NUM_GPUS, accelerator=\"gpu\", precision=16)            "
        },
        {
            "comment": "This code checks the value of the environment variable \"RWKV_FLOAT_MODE\" to determine the trainer type and precision for training. If it's 'bf16', a GPU trainer with bf16 precision is used, otherwise if it contains '32', a GPU trainer with 32-bit precision is used. Otherwise, a DeepSpeedStrategy is imported, and its configuration is set up for further optimization during the training process.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":209-232",
            "content": "        elif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n            trainer = Trainer(devices=NUM_GPUS, accelerator=\"gpu\", precision='bf16')\n        elif '32' in os.environ['RWKV_FLOAT_MODE']:\n            trainer = Trainer(devices=NUM_GPUS, accelerator=\"gpu\", precision=32)\n    else:\n        from pytorch_lightning.strategies import DeepSpeedStrategy\n        DEEPSPEED_CFG = {\n            \"zero_allow_untested_optimizer\":True,\n            \"zero_optimization\":{\n                \"stage\":2,\n                \"contiguous_gradients\":True,\n                \"overlap_comm\":True,\n                \"allgather_partitions\":True,\n                \"reduce_scatter\":True,\n                \"allgather_bucket_size\":200000000,\n                \"reduce_bucket_size\":200000000,\n                \"sub_group_size\":1000000000000\n            },\n            \"activation_checkpointing\":{\n                \"partition_activations\":False,\n                \"cpu_checkpointing\":False,\n                \"contiguous_memory_optimization\":False,\n                \"synchronize_checkpoint_boundary\":False"
        },
        {
            "comment": "This code configures DeepSpeed settings for RWKV-v4 model training. It includes various options such as block size, queue depth, and overlap events for the aio section, gradient clipping and accumulation steps, and DeepSpeed optimization settings like stage, contiguous gradients, and bucket sizes when using 1 GPU. Additionally, it enables FP16 mode if RWKV_FLOAT_MODE is set to 'fp16'.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":233-262",
            "content": "            },\n            \"aio\":{\n                \"block_size\":1048576,\n                \"queue_depth\":8,\n                \"single_submit\":False,\n                \"overlap_events\":True,\n                \"thread_count\":1\n            },\n            \"gradient_clipping\": 1.0,\n            \"gradient_accumulation_steps\": 1,\n        }\n        if NUM_GPUS == 1:\n            DEEPSPEED_CFG['zero_optimization'] = {\n                \"stage\":1, # saves some VRAM\n                \"contiguous_gradients\":False,\n                \"overlap_comm\":False,\n                \"allgather_partitions\":False,\n                \"reduce_scatter\":False,\n                \"allgather_bucket_size\":200000000,\n                \"reduce_bucket_size\":200000000,\n                \"sub_group_size\":1000000000000\n            }\n        if os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n            DEEPSPEED_CFG[\"fp16\"] = {\n                \"fp16\": True,\n                \"enabled\": True,\n                \"loss_scale\": 0,\n                \"initial_scale_power\": 12,\n                \"loss_scale_window\": 1000,"
        },
        {
            "comment": "The code checks the environment variable 'RWKV_FLOAT_MODE' and configures the DeepSpeed strategy accordingly. If it is set to 'fp16', it enables fp16 mode with hysteresis and minimum loss scale. If it is set to 'bf16', it enables bf16 mode. If '32' is present in the environment variable, it uses 32-bit precision. The trainer is then initialized with these configurations and the training process starts using the specified strategy.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/train.py\":263-279",
            "content": "                \"hysteresis\": 2,\n                \"min_loss_scale\": 1\n            }\n            trainer = Trainer(strategy=DeepSpeedStrategy(config=DEEPSPEED_CFG), devices=NUM_GPUS, accelerator=\"gpu\", precision=16)\n        elif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n            DEEPSPEED_CFG[\"bf16\"] = {\n                \"enabled\": True\n            }\n            trainer = Trainer(strategy=DeepSpeedStrategy(config=DEEPSPEED_CFG), devices=NUM_GPUS, accelerator=\"gpu\", precision='bf16')\n        elif '32' in os.environ['RWKV_FLOAT_MODE']:\n            trainer = Trainer(strategy=DeepSpeedStrategy(config=DEEPSPEED_CFG), devices=NUM_GPUS, accelerator=\"gpu\", precision=32)\n        print(trainer._strategy.config)\n    trainer.run(m_cfg, train_dataset, None, tconf)"
        }
    ]
}