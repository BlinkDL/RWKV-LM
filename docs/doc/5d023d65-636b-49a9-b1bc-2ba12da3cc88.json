{
    "summary": "The Trainer class trains a model, handles datasets and configurations, performs backpropagation, adjusts learning rate, logs loss, initializes progress bar, runs epochs, saves the model, and resets tokens counter for decay.",
    "details": [
        {
            "comment": "TrainerConfig class sets the maximum number of epochs, batch size, learning rate, optimizer settings, and other training parameters. The Trainer class initializes the model, train and test datasets, and a TrainerConfig object. It allows for customization by passing keyword arguments to the config instance.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/trainer.py\":0-34",
            "content": "import math, sys, datetime\nimport logging\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.utils.data.dataloader import DataLoader\nlogger = logging.getLogger(__name__)\n# print('logging to wandb... (comment it if you don\\'t have wandb)')\n# import wandb # comment this if you don't have wandb\nclass TrainerConfig:\n    max_epochs = 10\n    batch_size = 64\n    learning_rate = 4e-4\n    betas = (0.9, 0.99)\n    eps = 1e-8\n    grad_norm_clip = 1.0\n    weight_decay = 0.01\n    lr_decay = False # linear warmup followed by cosine decay\n    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper\n    final_tokens = 260e9 # at which point do we reach lr_final\n    epoch_save_frequency = 0\n    epoch_save_path = 'trained-'\n    num_workers = 0 # for DataLoader\n    def __init__(self, **kwargs):\n        for k,v in kwargs.items():\n            setattr(self, k, v)\nclass Trainer:\n    def __init__(self, model, train_dataset, test_dataset, config):"
        },
        {
            "comment": "This code initializes the trainer class, setting its model, train and test datasets, config, average loss, and steps. It also checks for availability of CUDA and moves the model to GPU if available. The get_run_name method returns a string based on the model's configuration parameters.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/trainer.py\":35-56",
            "content": "        self.model = model\n        self.train_dataset = train_dataset\n        self.test_dataset = test_dataset\n        self.config = config\n        self.avg_loss = -1\n        self.steps = 0\n        if 'wandb' in sys.modules:\n            cfg = model.config\n            for k in config.__dict__:\n                setattr(cfg, k, config.__dict__[k]) # combine cfg\n            wandb.init(project=\"RWKV-LM\", name=self.get_run_name() + '-' + datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S'), config=cfg, save_code=False)\n        self.device = 'cpu'\n        if torch.cuda.is_available(): # take over whatever gpus are on the system\n            self.device = torch.cuda.current_device()\n            self.model = torch.nn.DataParallel(self.model).to(self.device)\n    def get_run_name(self):\n        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n        cfg = raw_model.config\n        run_name = str(cfg.vocab_size) + '-' + str(cfg.ctx_len) + '-' + cfg.model_type + '-' + str(cfg.n_layer) + '-' + str(cfg.n_embd)"
        },
        {
            "comment": "Trains the model based on specified dataset, applies optimization for the configured optimizer and calculates average loss across all GPUs.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/trainer.py\":57-80",
            "content": "        return run_name\n    def train(self):\n        model, config = self.model, self.config\n        raw_model = model.module if hasattr(self.model, \"module\") else model\n        optimizer = raw_model.configure_optimizers(config)\n        def run_epoch(split):\n            is_train = split == 'train'\n            model.train(is_train)\n            data = self.train_dataset if is_train else self.test_dataset\n            loader = DataLoader(data, shuffle=True, pin_memory=True,\n                                batch_size=config.batch_size,\n                                num_workers=config.num_workers)\n            pbar = tqdm(enumerate(loader), total=len(loader), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}') if is_train else enumerate(loader)\n            for it, (x, y) in pbar:\n                x = x.to(self.device) # place data on the correct device\n                y = y.to(self.device)\n                with torch.set_grad_enabled(is_train):\n                    _, loss = model(x, y) # forward the model\n                    loss = loss.mean()         # collapse all losses if they are scattered on multiple gpus"
        },
        {
            "comment": "This code snippet checks if training is ongoing. If it is, it performs backpropagation, updates parameters, clips gradients, and handles learning rate decay based on the number of tokens processed. It uses linear warmup for the first config.warmup_tokens and cosine decay afterwards.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/trainer.py\":82-98",
            "content": "                if is_train: # backprop and update the parameters                    \n                    model.zero_grad()\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n                    optimizer.step()\n                    if config.lr_decay: # decay the learning rate based on our progress\n                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n                        lr_final_factor = config.lr_final / config.learning_rate\n                        if self.tokens < config.warmup_tokens:\n                            # linear warmup\n                            lr_mult = lr_final_factor + (1 - lr_final_factor) * float(self.tokens) / float(config.warmup_tokens)\n                            progress = 0\n                        else:\n                            # cosine learning rate decay\n                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))"
        },
        {
            "comment": "This code adjusts the learning rate based on progress and fine-tunes it. It then updates the optimizer's learning rate, logs the loss, and calculates a moving average of the loss for tracking.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/trainer.py\":99-118",
            "content": "                            # progress = min(progress * 1.1, 1.0) # more fine-tuning with low LR\n                            lr_mult = (0.5 + lr_final_factor / 2) + (0.5 - lr_final_factor / 2) * math.cos(math.pi * progress) # better 1.0 ~ 0.1\n                        lr = config.learning_rate * lr_mult\n                        for param_group in optimizer.param_groups:\n                            param_group['lr'] = lr\n                    else:\n                        lr = config.learning_rate\n                    now_loss = loss.item() # report progress\n                    if 'wandb' in sys.modules:\n                        wandb.log({\"loss\": now_loss}, step = self.steps * self.config.batch_size)\n                    self.steps += 1\n                    if self.avg_loss < 0:\n                        self.avg_loss = now_loss\n                    else:\n                        # factor = max(1.0 / 300, 1.0 / math.sqrt(it + 1))\n                        factor = 1 / (it + 1)\n                        self.avg_loss = self.avg_loss * (1.0 - factor) + now_loss * factor"
        },
        {
            "comment": "This code is initializing a progress bar and updating it with epoch, progress percentage, iteration, perplexity, loss value, and learning rate. It also resets the tokens counter for learning rate decay and runs the training epochs. If the current epoch meets save frequency or is the last epoch, it saves the model to a file.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v1/src/trainer.py\":119-129",
            "content": "                    pbar.set_description(f\"epoch {epoch+1} progress {progress*100.0:.2f}% iter {it}: ppl {math.exp(self.avg_loss):.2f} loss {self.avg_loss:.4f} lr {lr:e}\")\n        while True:\n            self.tokens = 0 # counter used for learning rate decay\n            for epoch in range(config.max_epochs):\n                run_epoch('train')\n                if (self.config.epoch_save_frequency > 0 and epoch % self.config.epoch_save_frequency == 0) or (epoch == config.max_epochs - 1):\n                    raw_model = self.model.module if hasattr(self.model, \"module\") else self.model # DataParallel wrappers keep raw model object in .module\n                    torch.save(raw_model, self.config.epoch_save_path + str(epoch+1) + '.pth')"
        }
    ]
}