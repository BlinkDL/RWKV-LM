{
    "summary": "This code uses CUDA kernels, fancy initialization, and attention mechanism layers for RWKV models. It implements a time-mixing channel model with custom GPT layers, layer normalization, attention, feed-forward layers, and DeepSpeed's optimizer for improved performance.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines a function called L2Wrap for calculating the loss. It also initializes RWKV_HEAD_QK_DIM variable and begins defining a class likely to be used in model training. The class uses the L2Wrap function to encourage logits to be close to 0.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":0-31",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport math, os\nimport numpy as np\nimport logging\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntry:\n    from deepspeed.ops.adam import FusedAdam\nexcept:\n    pass # some poor windows users cant install deepspeed\nlogger = logging.getLogger(__name__)\nRWKV_HEAD_QK_DIM = 0\nprint(f'\\nRWKV_HEAD_QK_DIM {RWKV_HEAD_QK_DIM}\\n')\nclass L2Wrap(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, loss, y):\n        ctx.save_for_backward(y)\n        return loss\n    @staticmethod\n    def backward(ctx, grad_output):\n        y = ctx.saved_tensors[0]\n        # to encourage the logits to be close to 0\n        factor = 1e-4 / (y.shape[0] * y.shape[1])\n        maxx, ids = torch.max(y, -1, keepdim=True)\n        gy = torch.zeros_like(y)"
        },
        {
            "comment": "This code defines a WKV class that uses the wkv_op.cpp and wkv_cuda.cu CUDA kernel files to perform computations on input B, T, C, w, u, k, and v. The function checks if T is within the maximum allowed value (T_MAX) and if the product of B and C is divisible by the smaller of B and C. If these conditions are met, it proceeds with further computations using a 32-bit floating point mode if '32' is present in the RWKV_FLOAT_MODE environment variable.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":32-54",
            "content": "        gy.scatter_(-1, ids, maxx * factor)\n        return (grad_output, gy)\n########################################################################################################\n# CUDA Kernel\n########################################################################################################\nT_MAX = 1024 # increase this if your ctx_len is long [NOTE: TAKES LOTS OF VRAM!]\n# it's possible to go beyond CUDA limitations if you slice the ctx and pass the hidden state in each slice\nfrom torch.utils.cpp_extension import load\nwkv_cuda = load(name=\"wkv\", sources=[\"cuda/wkv_op.cpp\", \"cuda/wkv_cuda.cu\"],\n                verbose=True, extra_cuda_cflags=['-res-usage', '--maxrregcount 60', '--use_fast_math', '-O3', '-Xptxas -O3', f'-DTmax={T_MAX}'])\nclass WKV(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, B, T, C, w, u, k, v):\n        ctx.B = B\n        ctx.T = T\n        ctx.C = C\n        assert T <= T_MAX\n        assert B * C % min(C, 1024) == 0\n        if '32' in os.environ['RWKV_FLOAT_MODE']:"
        },
        {
            "comment": "This code snippet initializes tensors and handles different float modes for model forward pass. It saves tensors for backward pass, ensures correct shape and alignment, and returns the result based on the specified float mode. The backward method performs cleanup by creating zeros tensors for gradients and asserts that certain conditions are met before proceeding with calculations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":55-83",
            "content": "            w = -torch.exp(w.contiguous())\n            u = u.contiguous()\n            k = k.contiguous()\n            v = v.contiguous()\n        else:\n            w = -torch.exp(w.float().contiguous())\n            u = u.float().contiguous()\n            k = k.float().contiguous()\n            v = v.float().contiguous()\n        ctx.save_for_backward(w, u, k, v)\n        y = torch.empty((B, T, C), device='cuda', memory_format=torch.contiguous_format)\n        wkv_cuda.forward(B, T, C, w, u, k, v, y)\n        if '32' in os.environ['RWKV_FLOAT_MODE']:\n            return y\n        elif os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n            return y.half()\n        elif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n            return y.bfloat16()\n    @staticmethod\n    def backward(ctx, gy):\n        B = ctx.B\n        T = ctx.T\n        C = ctx.C\n        assert T <= T_MAX\n        assert B * C % min(C, 1024) == 0\n        w, u, k, v = ctx.saved_tensors\n        gw = torch.zeros((B, C), device='cuda').contiguous()\n        gu = torch.zeros((B, C), device='cuda').contiguous()"
        },
        {
            "comment": "The code defines a function that performs backward pass for the RWKV model on CUDA devices. It initializes gradients for weights and inputs, then applies the backward pass using the provided weights and input tensors. Depending on the RWKV_FLOAT_MODE environment variable, it returns gradients in different floating-point precisions or None if not running on a CUDA device. The RUN_CUDA function wraps this logic for convenience by moving model weights and inputs to CUDA devices before applying the backward pass.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":84-102",
            "content": "        gk = torch.zeros((B, T, C), device='cuda').contiguous()\n        gv = torch.zeros((B, T, C), device='cuda').contiguous()\n        if '32' in os.environ['RWKV_FLOAT_MODE']:\n            wkv_cuda.backward(B, T, C, w, u, k, v, gy.contiguous(), gw, gu, gk, gv)\n        else:\n            wkv_cuda.backward(B, T, C, w, u, k, v, gy.float().contiguous(), gw, gu, gk, gv)\n        gw = torch.sum(gw, dim=0)\n        gu = torch.sum(gu, dim=0)\n        if '32' in os.environ['RWKV_FLOAT_MODE']:\n            return (None, None, None, gw, gu, gk, gv)\n        elif os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n            return (None, None, None, gw.half(), gu.half(), gk.half(), gv.half())\n        elif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n            return (None, None, None, gw.bfloat16(), gu.bfloat16(), gk.bfloat16(), gv.bfloat16())\ndef RUN_CUDA(B, T, C, w, u, k, v):\n    return WKV.apply(B, T, C, w.cuda(), u.cuda(), k.cuda(), v.cuda())\n########################################################################################################"
        },
        {
            "comment": "This code initializes all linear and embedding layers in a model using fancy initialization. This is done by iterating through all modules of the model, skipping non-linear and non-embedding layers. It finds the weight parameters for these layers and performs some operations to initialize them, including finding their names. The code also provides some information about the process, such as it being slow for large models and needing to be run on a single GPU before loading onto others.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":103-125",
            "content": "# RWKV: RWKV Time-mix + RWKV Channel-mix\n########################################################################################################\ndef RWKV_Init(model, args):  # fancy initialization of all lin & emb layer in the model\n    print(\"\\n[--> first run, init model params (very slow for large models) <--]\")\n    print(\"[so you shall only do it for 1 single GPU and save the checkpt and load it when using multiple GPU]\\n\")\n    for mm in model.modules():\n        if \"RecursiveScriptModule\" in str(type(mm)):\n            if mm.original_name not in [\"Linear\"]:\n                continue\n            ww = None\n            for name, param in mm.named_parameters():\n                if name == \"weight\":\n                    ww = param\n        else:\n            m = mm\n            if not isinstance(m, (nn.Linear, nn.Embedding)):\n                continue\n            ww = m.weight\n        with torch.no_grad():\n            name = \"[unknown weight]\"\n            for name, parameter in model.named_parameters():  # find the name of the weight"
        },
        {
            "comment": "This code is adjusting the weight matrix (`ww`) initializer of various neural network layers based on their shapes and types. It sets the gain and scale factors accordingly to optimize the model's performance. If `scale` is -999, it initializes with eye initialization. If `gain` is 0, it uses zero initialization. This process helps in setting up the weight matrices efficiently for RWKV models.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":126-155",
            "content": "                if id(ww) == id(parameter):\n                    break\n            shape = ww.shape\n            gain = 1.0\n            scale = 1.0  # extra scale for gain\n            if isinstance(m, nn.Embedding):\n                gain = math.sqrt(max(shape[0], shape[1]))\n                if shape[0] == args.vocab_size and shape[1] == args.n_embd:  # token emb?\n                    scale = 1e-4\n                else:\n                    scale = 0\n            if isinstance(m, nn.Linear):\n                if shape[0] > shape[1]:\n                    gain = math.sqrt(shape[0] / shape[1])\n                if shape[0] == args.vocab_size and shape[1] == args.n_embd:  # final projection?\n                    scale = 0.5\n            if hasattr(m, \"scale_init\"):\n                scale = m.scale_init\n            # print(f\"{str(shape[0]).ljust(5)} {str(shape[1]).ljust(5)} {str(scale).ljust(4)} {name}\")\n            gain *= scale\n            if scale == -999:\n                nn.init.eye_(ww)\n            elif gain == 0:\n                # zero init is great for some RWKV matrices"
        },
        {
            "comment": "This code is initializing a layer of the RWKV_TimeMix model with fancy initialization for time decay parameter. It calculates the time decay speed based on the current layer and attenuation size, and assigns it to the `self.time_decay` parameter in the class.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":156-183",
            "content": "                nn.init.zeros_(ww)\n            elif gain > 0:\n                nn.init.orthogonal_(ww, gain=gain)\n            else:\n                nn.init.normal_(ww, mean=0.0, std=-scale)\nclass RWKV_TimeMix(torch.jit.ScriptModule):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n        self.ctx_len = config.ctx_len\n        self.n_embd = config.n_embd\n        attn_sz = config.n_embd\n        with torch.no_grad(): # fancy init\n            ratio_0_to_1 = (layer_id / (config.n_layer - 1)) # 0 to 1\n            ratio_1_to_almost0 = (1.0 - (layer_id / config.n_layer)) # 1 to ~0\n            # fancy time_decay\n            decay_speed = torch.ones(attn_sz)\n            for h in range(attn_sz):\n                decay_speed[h] = -5 + 8 * (h / (attn_sz-1)) ** (0.7 + 1.3 * ratio_0_to_1)\n            self.time_decay = nn.Parameter(decay_speed)\n            # print(layer_id, self.time_decay.flatten()[:3].cpu().numpy(), '...', self.time_decay.flatten()[-3:].cpu().numpy())\n            # fancy time_first"
        },
        {
            "comment": "The code above initializes various layers for an attention mechanism in a transformer model. It defines parameters for time-based mixing, shifting, and linear transformations for keys, values, and output. The key, value, and output layers are initialized with zero scaling.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":184-205",
            "content": "            zigzag = (torch.tensor([(i+1)%3 - 1 for i in range(attn_sz)]) * 0.5)\n            self.time_first = nn.Parameter(torch.ones(attn_sz) * math.log(0.3) + zigzag)\n            # fancy time_mix\n            x = torch.ones(1, 1, config.n_embd)\n            for i in range(config.n_embd):\n                x[0, 0, i] = i / config.n_embd\n            self.time_mix_k = nn.Parameter(torch.pow(x, ratio_1_to_almost0))\n            self.time_mix_v = nn.Parameter(torch.pow(x, ratio_1_to_almost0) + 0.3 * ratio_0_to_1)\n            self.time_mix_r = nn.Parameter(torch.pow(x, 0.5 * ratio_1_to_almost0))\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        self.key = nn.Linear(config.n_embd, attn_sz, bias=False)\n        self.value = nn.Linear(config.n_embd, attn_sz, bias=False)\n        self.receptance = nn.Linear(config.n_embd, attn_sz, bias=False)\n        self.output = nn.Linear(attn_sz, config.n_embd, bias=False)\n        self.key.scale_init = 0\n        self.receptance.scale_init = 0\n        self.output.scale_init = 0"
        },
        {
            "comment": "The code defines a model that performs channel-wise mixing and applies a time shift. The jit_func method takes an input tensor x, mixes it with the previous timestep to produce xk, xv, xr, and then calculates k, v, r using these mixed tensors. The forward method takes an input tensor x, calls the jit_func to obtain sr, k, v, applies a time decay, and returns the output from an output layer. RWKV_ChannelMix is a ScriptModule class that initializes the time shift and uses fancy initialization for time_mix.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":207-241",
            "content": "    @torch.jit.script_method\n    def jit_func(self, x):\n        # Mix x with the previous timestep to produce xk, xv, xr\n        xx = self.time_shift(x)\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        # Use xk, xv, xr to produce k, v, r\n        k = self.key(xk)\n        v = self.value(xv)\n        r = self.receptance(xr)\n        sr = torch.sigmoid(r)\n        return sr, k, v\n    def forward(self, x):\n        B, T, C = x.size() # x = (Batch,Time,Channel)\n        sr, k, v = self.jit_func(x)\n        rwkv = sr * RUN_CUDA(B, T, C, self.time_decay, self.time_first, k, v)\n        rwkv = self.output(rwkv)\n        return rwkv\nclass RWKV_ChannelMix(torch.jit.ScriptModule):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.layer_id = layer_id\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        with torch.no_grad(): # fancy init of time_mix"
        },
        {
            "comment": "Code snippet initializes two parameters for time-mixing, sets hidden size and defines linear layers for key, receptance, and value in a transformer model. The forward method applies time-shifting, mixing, passing through key and value layers, and calculates the final output using sigmoid activation and multiplication.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":242-269",
            "content": "            ratio_1_to_almost0 = (1.0 - (layer_id / config.n_layer)) # 1 to ~0\n            x = torch.ones(1, 1, config.n_embd)\n            for i in range(config.n_embd):\n                x[0, 0, i] = i / config.n_embd\n            self.time_mix_k = nn.Parameter(torch.pow(x, ratio_1_to_almost0))\n            self.time_mix_r = nn.Parameter(torch.pow(x, ratio_1_to_almost0))\n        hidden_sz = 4 * config.n_embd\n        self.key = nn.Linear(config.n_embd, hidden_sz, bias=False)\n        self.receptance = nn.Linear(config.n_embd, config.n_embd, bias=False)\n        self.value = nn.Linear(hidden_sz, config.n_embd, bias=False)\n        self.value.scale_init = 0\n        self.receptance.scale_init = 0\n    @torch.jit.script_method\n    def forward(self, x):\n        xx = self.time_shift(x)\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        k = self.key(xk)\n        k = torch.square(torch.relu(k))\n        kv = self.value(k)\n        rkv = torch.sigmoid(self.receptance(xr)) * kv"
        },
        {
            "comment": "The code defines a GPT model with customizable blocks and config parameters. The GPTConfig class holds the vocabulary size, context length, and additional keyword-value pairs. The Block class is a module for these customizable blocks, using layer normalization, attention, and feed-forward layers, depending on the block type and position.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":270-302",
            "content": "        return rkv\n########################################################################################################\n# The GPT Model with our blocks\n########################################################################################################\nclass GPTConfig:\n    def __init__(self, vocab_size, ctx_len, **kwargs):\n        self.vocab_size = vocab_size\n        self.ctx_len = ctx_len\n        for k, v in kwargs.items():\n            setattr(self, k, v)\nclass Block(nn.Module):\n    def __init__(self, config, layer_id):\n        super().__init__()\n        self.config = config\n        self.layer_id = layer_id\n        self.ln1 = nn.LayerNorm(config.n_embd)\n        self.ln2 = nn.LayerNorm(config.n_embd)\n        if self.layer_id == 0:\n            self.ln0 = nn.LayerNorm(config.n_embd)\n        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':\n            self.ffnPre = RWKV_ChannelMix(config, 0)\n        else:\n            self.att = RWKV_TimeMix(config, layer_id)\n        self.ffn = RWKV_ChannelMix(config, layer_id)"
        },
        {
            "comment": "This code defines a GPT model class with layer normalization, embedding, and multiple blocks. The forward function performs feed-forward and attention mechanisms, and the __init__ function initializes the model parameters based on the given configuration.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":304-332",
            "content": "    def forward(self, x):\n        if self.layer_id == 0:\n            x = self.ln0(x)        \n        if self.layer_id == 0 and self.config.model_type == 'RWKV-ffnPre':\n            x = x + self.ffnPre(self.ln1(x))  # better in some cases\n        else:\n            x = x + self.att(self.ln1(x))\n        x = x + self.ffn(self.ln2(x))\n        return x\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.step = 0\n        self.config = config\n        self.emb = nn.Embedding(config.vocab_size, config.n_embd)\n        self.blocks = nn.Sequential(*[Block(config, i)\n                                    for i in range(config.n_layer)])\n        self.ln_out = nn.LayerNorm(config.n_embd)\n        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        if RWKV_HEAD_QK_DIM > 0:\n            self.head_q = nn.Linear(config.n_embd, RWKV_HEAD_QK_DIM, bias=False)\n            self.head_q.scale_init = 0\n            self.head_k = nn.Linear(config.n_embd, RWKV_HEAD_QK_DIM, bias=False)"
        },
        {
            "comment": "This code initializes the model, sets parameters such as head_k scale and copy mask, checks if the model should be loaded, and logs the number of parameters. It also defines methods for getting ctx_len, initializing weights with specific values, and configuring optimizers.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":333-363",
            "content": "            self.head_k.scale_init = 0.1\n            self.register_buffer(\"copy_mask\", torch.tril(\n                torch.ones(config.ctx_len, config.ctx_len)))\n        self.ctx_len = config.ctx_len\n        try:\n            if os.environ['RWKV_LOAD_MODEL'] == str(False):\n                RWKV_Init(self, config) \n        except:\n            pass\n        logger.info(\"number of parameters: %e\", sum(p.numel()\n                    for p in self.parameters()))\n    def get_ctx_len(self):\n        return self.ctx_len\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear)):\n            module.weight.data.normal_(mean=0.0, std=0.01)\n        if isinstance(module, (nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=1e-5)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n    def configure_optimizers(self, train_config):\n        no_decay = set()\n        for mn, m in self.named_modules():  # here we disable weight_decay\n            for pn, p in m.named_parameters():"
        },
        {
            "comment": "This code initializes an optimizer for a model, either using DeepSpeed's FusedAdam if available or falling back to torch.optim.Adam. It defines the full param name and creates optimization groups based on whether or not weight decay should be applied. The forward function performs forward pass of the model and asserts that the input length is less than or equal to the context length.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":364-386",
            "content": "                fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name\n                no_decay.add(fpn)\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        optim_groups = [\n            {\"params\": [param_dict[pn]\n                        for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n        ]\n        try:\n            optimizer = FusedAdam(optim_groups, lr=train_config.learning_rate, betas=train_config.betas, eps=train_config.eps, bias_correction=True, adam_w_mode=False, weight_decay=0, amsgrad=False)\n        except:\n            print('\\n\\nDeepSpeed not found. Using torch optimizer instead (probably slower)\\n\\n')\n            optimizer = torch.optim.Adam(optim_groups, lr=train_config.learning_rate, betas=train_config.betas, eps=train_config.eps)\n        return optimizer\n    def forward(self, idx, targets=None):\n        idx = idx.to(self.emb.weight.device)\n        self.step += 1\n        B, T = idx.size()\n        assert T <= self.ctx_len, \"Cannot forward, because len(input) > model ctx_len.\""
        },
        {
            "comment": "This code calculates the attention scores using head layers, and then applies them to the output. If RWKV_HEAD_QK_DIM is greater than 0, it performs multi-head attention by computing the attention scores and scaling them based on the number of heads. Then, it adds a one-hot vector to the output depending on the RWKV_FLOAT_MODE environment variable, and finally calculates the cross-entropy loss between the output and targets (if provided), and returns the L2Wrap applied output and the loss.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4/src/model.py\":388-413",
            "content": "        x = self.emb(idx)\n        x = self.blocks(x)\n        x = self.ln_out(x)\n        if RWKV_HEAD_QK_DIM > 0:\n            q = self.head_q(x)[:, :T, :]\n            k = self.head_k(x)[:, :T, :]\n            c = (q @ k.transpose(-2, -1)) * (1.0 / RWKV_HEAD_QK_DIM)\n            c = c.masked_fill(self.copy_mask[:T, :T] == 0, 0)\n            if '32' in os.environ['RWKV_FLOAT_MODE']:\n                c = c @ F.one_hot(idx, num_classes=self.config.vocab_size)\n            elif os.environ['RWKV_FLOAT_MODE'] == 'fp16':\n                c = c @ F.one_hot(idx, num_classes=self.config.vocab_size).half()\n            elif os.environ['RWKV_FLOAT_MODE'] == 'bf16':\n                c = c @ F.one_hot(idx, num_classes=self.config.vocab_size).bfloat16()\n            x = self.head(x) + c\n        else:\n            x = self.head(x)\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(x.view(-1, x.size(-1)), targets.to(x.device).view(-1))\n        return L2Wrap.apply(loss, x)"
        }
    ]
}