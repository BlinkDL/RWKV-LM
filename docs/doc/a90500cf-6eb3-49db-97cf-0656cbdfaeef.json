{
    "summary": "This code initializes RWKV Language Model, configures parameters, handles potential issues, and utilizes the RWKV-v4neo model for text generation, context refinement, tokenizer error checking, and prompt processing. It iterates through tokens, predicts next tokens, handles special cases, and prints debug output while tracking time for preprocessing, generation, and flushing buffer after each character.",
    "details": [
        {
            "comment": "This code is initializing the RWKV Language Model. It imports necessary libraries, sets the CUDA device for GPU or CPU usage, and ensures compatibility between v4 and v4neo models. The arguments for model configuration are set to 'cuda' for GPU acceleration or 'cpu' for CPU-only processing.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/run.py\":0-22",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport numpy as np\nimport math, os, sys, types, time, gc\nimport torch\nfrom src.utils import TOKENIZER\ntry:\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = sys.argv[1]\nexcept:\n    pass\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cuda.matmul.allow_tf32 = True\nnp.set_printoptions(precision=4, suppress=True, linewidth=200)\nargs = types.SimpleNamespace()\n########################################################################################################\n# Step 1: set model & config (use v4 to run your trained-from-scratch models. v4 and v4neo are compatible)\n########################################################################################################\nargs.RUN_DEVICE = \"cuda\" # 'cuda' // 'cpu' (already fast)"
        },
        {
            "comment": "This code sets various parameters for an RWKV model, including float mode (fp16, fp32 or bf16), JIT environment, tokenizer files, and Pile model options. It also specifies the MODEL_NAME based on downloaded models or a user-defined fine-tuned model. The code is designed for GPU and CPU usage, but some elements may require benchmarking due to potential issues or reduced accuracy.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/run.py\":23-55",
            "content": "args.FLOAT_MODE = \"fp16\" # fp16 (good for GPU, does not work for CPU) // fp32 (good for CPU) // bf16 (less accurate, but works for CPU)\n# if args.RUN_DEVICE == \"cuda\":\n#     os.environ[\"RWKV_RUN_BACKEND\"] = 'nvfuser' # !!!BUGGY!!! wrong output\nos.environ[\"RWKV_JIT_ON\"] = '1' # '1' or '0'. very useful for GPU/CPU fp32, but might be harmful for GPU fp16. please benchmark !!!\nTOKEN_MODE = \"pile\"\nWORD_NAME = [\n    \"20B_tokenizer.json\",\n    \"20B_tokenizer.json\",\n]  # [vocab, vocab] for Pile model\nUNKNOWN_CHAR = None\nvocab_size = 50277\n# Download Pile models: https://huggingface.co/BlinkDL\n# or, set MODEL_NAME to your fine-tuned model\n# MODEL_NAME = \"/fsx/BlinkDL/rwkv-release/RWKV-4-Pile-169M-20220807-8023\"\n# n_layer = 12\n# n_embd = 768\n# ctx_len = 1024\n# MODEL_NAME = '/fsx/BlinkDL/rwkv-release/RWKV-4-Pile-430M-20220808-8066'\n# n_layer = 24\n# n_embd = 1024\n# ctx_len = 1024\n# MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-1b5/RWKV-4-Pile-1B5-20220903-8040'\n# n_layer = 24\n# n_embd = 2048\n# ctx_len = 1024\n# MODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-3b/RWKV-4-Pile-3B-20221008-8023'"
        },
        {
            "comment": "This code sets the model parameters (n_layer, n_embd, ctx_len) and environment variables for RWKV-v4neo's run.py. The context variable holds a text prompt for question and answer tasks in various languages.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/run.py\":56-88",
            "content": "# n_layer = 32\n# n_embd = 2560\n# ctx_len = 1024\nMODEL_NAME = '/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-7b/RWKV-4-Pile-7B-20221115-8047'\nn_layer = 32\nn_embd = 4096\nctx_len = 1024\nargs.MODEL_NAME = MODEL_NAME\nargs.n_layer = n_layer\nargs.n_embd = n_embd\nargs.ctx_len = ctx_len\nargs.vocab_size = vocab_size\nargs.head_qk = 0\nargs.pre_ffn = 0\nargs.grad_cp = 0\nargs.my_pos_emb = 0\nos.environ[\"RWKV_RUN_DEVICE\"] = args.RUN_DEVICE\n########################################################################################################\n# Step 2: set prompt & sampling stuffs\n########################################################################################################\n# context = 'A'\n# context = \"\\nIn the\"\n# context = '\\nSugar:'\ncontext = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n# context = \"\\n\u6df1\u5733\u662f\" # test Chinese\n# context = \"\\n\u6771\u4eac\u306f\" # test Japanese\n# ###### A good prompt for Q&A ######"
        },
        {
            "comment": "This code is a chatbot prompt featuring a conversation between a user and an intelligent AI assistant. The user asks various questions about politics, history, and personal preferences, and the AI provides accurate and informative responses.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/run.py\":89-116",
            "content": "# context = '''\n# Questions & Helpful Answers\n# Ask Research Experts\n# Question:\n# Can penguins fly?\n# Full Answer:\n# '''\n# ###### A good prompt for chatbot ######\n# context = '''\n# The following is a conversation between a highly knowledgeable and intelligent AI assistant called Bot, and a human user called User. In the following interactions, User and Bot converse in natural language, and Bot always answer User's questions. Bot is very smart, polite and humorous. Bot knows a lot, and always tells the truth. The conversation begins.\n# User: who is president of usa?\n# Bot: It\u2019s Joe Biden; he was sworn in earlier this year.\n# User: french revolution what year\n# Bot: It started in 1789, but it lasted 10 years until 1799.\n# User: guess i marry who ?\n# Bot: Only if you tell me more about yourself - what are your interests?\n# User: wat is lhc\n# Bot: It\u2019s a large and very expensive piece of science equipment. If I understand correctly, it\u2019s a high-energy particle collider, built by CERN, and completed in 2008. They used it to confirm the existence of the Higgs boson in 2012."
        },
        {
            "comment": "This code snippet is part of a larger program that uses the RWKV-v4neo model for text generation. It sets up necessary parameters and initializes the model, optimizer, and tokenizer. The context provided to the model is refined if character mode is enabled. The code also includes error checking for specific conditions related to the tokenizer being used.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/run.py\":118-155",
            "content": "# User:''' # type your question here\nNUM_TRIALS = 999\nLENGTH_PER_TRIAL = 333\nTEMPERATURE = 1.0\ntop_p = 0.8\ntop_p_newline = 0.9  # only used in TOKEN_MODE = char\nDEBUG_DEBUG = False  # True False --> show softmax output\n########################################################################################################\nprint(f'\\nUsing {args.RUN_DEVICE.upper()}. Loading {MODEL_NAME}...')\nfrom src.model_run import RWKV_RNN\nmodel = RWKV_RNN(args)\nprint(f'\\nOptimizing speed...')\nout, _ = model.forward([187], None)\n# print(out)\ngc.collect()\ntorch.cuda.empty_cache()\n# input(0)\nprint(f'\\nLoading tokenizer {WORD_NAME}...')\ntokenizer = TOKENIZER(WORD_NAME, UNKNOWN_CHAR=UNKNOWN_CHAR)\nif TOKEN_MODE == \"pile\":\n    assert tokenizer.tokenizer.decode([187]) == '\\n'\n########################################################################################################\nif tokenizer.charMode:\n    context = tokenizer.refine_context(context)\n    ctx = [tokenizer.stoi.get(s, tokenizer.UNKNOWN_CHAR) for s in context]\nelse:\n    ctx = tokenizer.tokenizer.encode(context)"
        },
        {
            "comment": "This code is running a neural language model for a given prompt. It first processes the prompt to generate an initial state and output, which are then used in subsequent trials. The processing involves using a recurrent neural network (RNN) for the first run if the prompt is long, or GPT otherwise. The code also keeps track of time taken during various operations for potential performance improvements.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/run.py\":156-193",
            "content": "src_len = len(ctx)\nsrc_ctx = ctx.copy()\nprint(\"\\nYour prompt has \" + str(src_len) + \" tokens.\")\nprint(\n    \"Note: currently the first run takes a while if your prompt is long, as we are using RNN to preprocess the prompt. Use GPT to build the hidden state for better speed.\\n\"\n)\ntime_slot = {}\ntime_ref = time.time_ns()\ndef record_time(name):\n    if name not in time_slot:\n        time_slot[name] = 1e20\n    tt = (time.time_ns() - time_ref) / 1e9\n    if tt < time_slot[name]:\n        time_slot[name] = tt\ninit_state = None\ninit_out = None\nstate = None\nout = None\nfor TRIAL in range(1 if DEBUG_DEBUG else NUM_TRIALS):\n    print((\"-\" * 50) + '\\n' + context, end=\"\")\n    time_ref = time.time_ns()\n    ctx = src_ctx.copy()\n    if TRIAL == 0:\n        for i in range(src_len):\n            x = ctx[: i + 1]\n            if i == src_len - 1:\n                init_out, init_state = model.forward(x, init_state)\n            else:\n                init_state = model.forward(x, init_state, preprocess_only=True)\n        gc.collect()\n        torch.cuda.empty_cache()"
        },
        {
            "comment": "This code is iterating through a sequence of tokens, using a model to predict the next token based on the previous ones. If in debug mode, it prints out the output of the model for each step. It also has special handling for the \"<|endoftext|>\" token, disabling it if the tokenizer mode is set to \"pile\". The code then adds the predicted token to the context and either prints out each character if in character mode or combines the tokens into a string if not.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/run.py\":195-226",
            "content": "    record_time('preprocess')\n    out_last = src_len\n    for i in range(src_len, src_len + (1 if DEBUG_DEBUG else LENGTH_PER_TRIAL)):\n        x = ctx[: i + 1]\n        x = x[-ctx_len:]\n        if i == src_len:\n            out = init_out.clone()\n            state = init_state.clone()\n        else:\n            out, state = model.forward(x, state)\n        if DEBUG_DEBUG:\n            print(\"model\", np.array(x), \"==>\", np.array(out), np.max(out.cpu().numpy()), np.min(out.cpu().numpy()))\n        if TOKEN_MODE == \"pile\":\n            out[0] = -999999999  # disable <|endoftext|>\n        ttt = tokenizer.sample_logits(\n            out,\n            x,\n            ctx_len,\n            temperature=TEMPERATURE,\n            top_p_usual=top_p,\n            top_p_newline=top_p_newline,\n        )\n        ctx += [ttt]\n        if tokenizer.charMode:\n            char = tokenizer.itos[ttt]\n            print(char, end=\"\", flush=True)\n        else:\n            char = tokenizer.tokenizer.decode(ctx[out_last:])\n            if '\\ufffd' not in char: # is valid utf8 string?"
        },
        {
            "comment": "This code block prints the time taken for preprocessing and generation, separates with a line of dashes, and then proceeds to print information about the time slots. It also flushes the buffer immediately after each character is printed to update the output instantly. The comments are for record-keeping and provide an organized summary of the code's actions.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/run.py\":227-236",
            "content": "                print(char, end=\"\", flush=True)\n                out_last = i+1\n    record_time('total')\n    # print(f'\\n\\n{time_slot}\\n\\n')\n    print(\n        f\"\\n\\n--- preprocess {round(time_slot['preprocess'], 2)}s, generation {round(time_slot['total']-time_slot['preprocess'], 2)}s \", end = ''\n    )\nprint((\"-\" * 50) + '\\n')"
        }
    ]
}