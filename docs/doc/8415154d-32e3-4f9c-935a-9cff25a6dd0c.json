{
    "summary": "The code snippet includes a tokenizer and class for binary file manipulation, indexing using RWKV language model, handling exceptions during decoding, verifying prime numbers, and displaying decoded results.",
    "details": [
        {
            "comment": "This code snippet imports various modules and defines a tokenizer object. It then provides instructions on how to use the make_data.py script for creating data from a JSONL file, including shuffling, duplicating, loading, tokenizing, saving, and computing magic prime for ctxlen 4096. The code is part of the RWKV language model's tokenization process.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/make_data.py\":0-33",
            "content": "import json, math, random, sys, time, shutil, os, string, re, fileinput\nimport numpy as np\n\"\"\"\nHow to use:\npython make_data.py demo.jsonl 3 4096\nThis will:\n==> shuffle & duplicate demo.jsonl (for 3 epochs, good for finetuning) note: this will be very slow for large jsonl and we need more efficient code.\n==> load jsonl and tokenize\n==> save as demo.bin & demo.idx\n==> compute \"magic_prime\" for ctxlen 4096\nExample:\nAssume your source jsonl is:\n{\"text\":\"aa\"}\n{\"text\":\"bb\"}\n{\"text\":\"cc\"}\n{\"text\":\"dd\"}\nThe final binidx will be like (here \"/\" means end_of_doc, which is actually token [0]):\nbb/aa/dd/cc/dd/aa/bb/cc/dd/bb/cc/aa/\nwhere the data is repeated 3 times (each time with different shuffle)\n\"\"\"\n########################################################################################################\n# MMapIndexedDatasetBuilder\n########################################################################################################\nfrom tokenizer.rwkv_tokenizer import TRIE_TOKENIZER\ntokenizer = TRIE_TOKENIZER(\"tokenizer/rwkv_vocab_v20230424.txt\")"
        },
        {
            "comment": "This code defines a class `MMapIndexedDatasetBuilder` that allows adding items to a binary file and building an index. It uses the `MMapIndexedDataset` from `src.binidx`. The `add_item()` function appends arrays to the data file, `end_document()` marks document end, and `finalize()` writes the sizes of items in the data file and the document indices into an index file. It also checks if encoded raw is the same as original raw using tokenizer from rwkv.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/make_data.py\":34-62",
            "content": "from src.binidx import MMapIndexedDataset\ndef index_file_path(prefix_path):\n    return prefix_path + \".idx\"\ndef data_file_path(prefix_path):\n    return prefix_path + \".bin\"\nclass MMapIndexedDatasetBuilder(object):\n    def __init__(self, out_file, dtype=np.uint16):\n        self._data_file = open(out_file, \"wb\")\n        self._dtype = dtype\n        self._sizes = []\n        self._doc_idx = [0]\n    def add_item(self, np_array):\n        assert np_array.dtype == self._dtype\n        self._data_file.write(np_array.tobytes(order=\"C\"))\n        self._sizes.append(np_array.size)\n    def end_document(self):\n        self._doc_idx.append(len(self._sizes))\n    def finalize(self, index_file):\n        self._data_file.close()\n        with MMapIndexedDataset.Index.writer(index_file, self._dtype) as index:\n            index.write(self._sizes, self._doc_idx)\ncnt = 0\ndef add_raw(raw):\n    global builder, cnt\n    out = tokenizer.encode(raw)\n    if tokenizer.decode(out) != raw:\n        print(\"ERROR\" * 100)\n        exit(0)\n    out.append(0)  # [0] = end_of_doc for rwkv tokenizer"
        },
        {
            "comment": "The code is loading a file, checking for non-empty lines, and then iterating over each epoch to generate data. The is_prime function checks if a number is prime or not. It writes the data to a temporary file before creating a final binary/index file with the specified length.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/make_data.py\":63-98",
            "content": "    builder.add_item(np.array(out, dtype=np.uint16))\n    builder.end_document()\n    if cnt % 500 == 0:\n        print(cnt, end=\" \", flush=True)\n    cnt += 1\ndef is_prime(n):\n    if n <= 1:\n        return False\n    if n <= 3:\n        return True\n    if n % 2 == 0 or n % 3 == 0:\n        return False\n    i = 5\n    while i * i <= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n    return True\n########################################################################################################\nN_EPOCH = int(sys.argv[2].strip())\nIN_FILE = sys.argv[1].strip()\nOUT_NAME = os.path.splitext(os.path.basename(IN_FILE))[0]\nCTX_LEN = int(sys.argv[3].strip())\nTEMP_FILE = \"make_data_temp.jsonl\"\nprint(f\"### Convert {IN_FILE} to {OUT_NAME}.bin/idx...\")\nwith open(IN_FILE, \"r\", encoding=\"utf-8\") as file:\n    non_empty_lines = [line.strip() for line in file if line.strip()]\nprint(f\"### Found {len(non_empty_lines)} non-empty lines in {IN_FILE}\")\nfile = open(TEMP_FILE, \"w\", encoding=\"utf-8\")\nfor i in range(N_EPOCH):"
        },
        {
            "comment": "The code shuffles non-empty lines, writes them to a file, builds an indexed dataset from the temporary file, and finally verifies the result by checking if the last element is zero and displays a portion of the decoded data.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/make_data.py\":99-132",
            "content": "    print(f\"Shuffle: {i+1} out of {N_EPOCH}\")\n    random.shuffle(non_empty_lines)\n    for entry in non_empty_lines:\n        file.write(entry + \"\\n\")\nfile.close()\n########################################################################################################\nprint(\"### Building binidx...\")\nbuilder = MMapIndexedDatasetBuilder(f\"{OUT_NAME}.bin\")\nwith fileinput.input(TEMP_FILE, encoding=\"utf-8\") as ffff:\n    for line in ffff:\n        x = json.loads(line)[\"text\"]\n        add_raw(x)\nbuilder.finalize((f\"{OUT_NAME}.idx\"))\nprint(\"done\")\nprint(\"### Verifying result...\")\ndata = MMapIndexedDataset(OUT_NAME)\ndata_len = len(data)\ndata_size = len(data._bin_buffer) // data._index._dtype_size\nTODO = [0, data_len - 1]\nPREVIEW_LIMIT = 100\nfor idx in TODO:\n    ptr, size = data._index[idx]\n    dix = data.get(idx=idx, offset=0, length=size).astype(int)\n    print(\"-\" * 70 + f\"[{OUT_NAME} idx {idx} sz {size}]\")\n    assert dix[-1] == 0\n    dix = dix[:-1]\n    if len(dix) > PREVIEW_LIMIT:\n        try:\n            print(tokenizer.decode(dix[:PREVIEW_LIMIT]))"
        },
        {
            "comment": "Except blocks to handle potential exceptions when decoding dix with different lengths for preview, avoids UTF-8 bug. Else block to decode the entire dix. Checks if data size is greater than 3 times CTX_LEN and prints magic prime if it's a prime number within this condition.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/make_data.py\":133-157",
            "content": "        except:\n            try:\n                print(tokenizer.decode(dix[: PREVIEW_LIMIT + 1]))\n            except:\n                print(tokenizer.decode(dix[: PREVIEW_LIMIT + 2]))\n        print(\"\u00b7 \" * 30)\n        try:  # avoid utf-8 bug\n            print(tokenizer.decode(dix[-PREVIEW_LIMIT:]))\n        except:\n            try:\n                print(tokenizer.decode(dix[-PREVIEW_LIMIT - 1 :]))\n            except:\n                print(tokenizer.decode(dix[-PREVIEW_LIMIT - 2 :]))\n    else:\n        print(tokenizer.decode(dix))\nprint(f\"{'-'*80}\\n### Final {OUT_NAME}.bin/idx has {data_size} tokens, {data_len} items. Dtype {data._index.dtype}\")\nif data_size >= CTX_LEN * 3:\n    n_chunk = int(data_size // CTX_LEN) - 1\n    for i in range(n_chunk, 0, -1):\n        if i % 3 == 2:\n            if is_prime(i):\n                print(f\"\\n### magic_prime = {i} (for ctxlen {CTX_LEN})\\n\")\n                exit(0)"
        }
    ]
}