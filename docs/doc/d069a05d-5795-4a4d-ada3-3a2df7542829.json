{
    "summary": "This code imports libraries, sets up profiling, defines modules based on environment variables, utilizes CUDA for efficient computation, and initializes a neural network model with backpropagation support. It applies layer normalization using CUDA function calculations and introduces the RWKV_TimeMix class for the RWKV5 model. The code initializes optimizer groups and models with layer-specific learning rates, performs all-gather for losses, handles various settings, and initializes model weights using orthogonal initialization.",
    "details": [
        {
            "comment": "This code imports necessary libraries, sets up profiling executor and mode for torch.nn, defines MyModule and MyFunction based on JIT environment variable, and imports DeepSpeed if available.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":0-30",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nimport os, math, gc, importlib\nimport torch\n# torch._C._jit_set_profiling_executor(True)\n# torch._C._jit_set_profiling_mode(True)\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities import rank_zero_info, rank_zero_only\nfrom pytorch_lightning.strategies import DeepSpeedStrategy\nif importlib.util.find_spec('deepspeed'):\n    import deepspeed\n    from deepspeed.ops.adam import DeepSpeedCPUAdam, FusedAdam\n# from deepspeed.runtime.fp16.onebit.zoadam import ZeroOneAdam\ntry:\n    print('RWKV_MY_TESTING', os.environ[\"RWKV_MY_TESTING\"])\nexcept:\n    os.environ[\"RWKV_MY_TESTING\"] = ''\ndef __nop(ob):\n    return ob\nMyModule = nn.Module\nMyFunction = __nop\nif os.environ[\"RWKV_JIT_ON\"] == \"1\":"
        },
        {
            "comment": "This code defines a class 'WKV' that utilizes CUDA for efficient computation, and loads a corresponding CUDA kernel module depending on the environment variable \"RWKV_FLOAT_MODE\". This process involves setting T_MAX and loading the appropriate C++ modules with specific compiler flags.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":31-51",
            "content": "    MyModule = torch.jit.ScriptModule\n    MyFunction = torch.jit.script_method\n########################################################################################################\n# CUDA Kernel\n########################################################################################################\nT_MAX = int(os.environ[\"RWKV_T_MAX\"])  # TAKES LOTS OF VRAM!\n# it's possible to go beyond CUDA limitations if you slice the ctx and pass the hidden state in each slice\nfrom torch.utils.cpp_extension import load\nif os.environ[\"RWKV_FLOAT_MODE\"] == \"bf16\":\n    wkv_cuda = load(name=f\"wkv_{T_MAX}_bf16\", sources=[\"cuda/wkv_op_bf16.cpp\", \"cuda/wkv_cuda_bf16.cu\"], verbose=True, extra_cuda_cflags=[\"-t 4\", \"-std=c++17\", \"-res-usage\", \"--maxrregcount 60\", \"--use_fast_math\", \"-O3\", \"-Xptxas -O3\", \"--extra-device-vectorization\", f\"-DTmax={T_MAX}\"])\n    class WKV(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, B, T, C, w, u, k, v):\n            ctx.B = B\n            ctx.T = T\n            ctx.C = C"
        },
        {
            "comment": "This code defines a model function with forward and backward operations. It checks some conditions, initializes tensors, performs model computations, and saves the intermediate states for later backpropagation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":52-72",
            "content": "            assert T <= T_MAX\n            assert B * C % min(C, 32) == 0\n            w = -torch.exp(w.float().contiguous())\n            u = u.contiguous()\n            k = k.contiguous()\n            v = v.contiguous()\n            y = torch.empty((B, T, C), device=w.device, memory_format=torch.contiguous_format, dtype=torch.bfloat16)\n            wkv_cuda.forward(B, T, C, w, u, k, v, y)\n            ctx.save_for_backward(w, u, k, v, y)\n            return y\n        @staticmethod\n        def backward(ctx, gy):\n            B = ctx.B\n            T = ctx.T\n            C = ctx.C\n            assert T <= T_MAX\n            assert B * C % min(C, 32) == 0\n            w, u, k, v, y = ctx.saved_tensors\n            gw = torch.empty((B, C), device=gy.device, memory_format=torch.contiguous_format, dtype=torch.bfloat16)\n            gu = torch.empty((B, C), device=gy.device, memory_format=torch.contiguous_format, dtype=torch.bfloat16)\n            gk = torch.empty((B, T, C), device=gy.device, memory_format=torch.contiguous_format, dtype=torch.bfloat16)"
        },
        {
            "comment": "This code initializes a tensor and calls a function. It then calculates the sum along dimension 0 for two other tensors, and returns them alongside others. It seems to be part of a neural network model with CUDA support. The forward method creates an instance variable for each argument, checks if T is within a limit, and performs some transformations on certain arguments.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":73-92",
            "content": "            gv = torch.empty((B, T, C), device=gy.device, memory_format=torch.contiguous_format, dtype=torch.bfloat16)\n            wkv_cuda.backward(B, T, C, w, u, k, v, y, gy.contiguous(), gw, gu, gk, gv)\n            gw = torch.sum(gw, dim=0)\n            gu = torch.sum(gu, dim=0)\n            return (None, None, None, gw, gu, gk, gv)\nelse:\n    wkv_cuda = load(name=f\"wkv_{T_MAX}\", sources=[\"cuda/wkv_op.cpp\", \"cuda/wkv_cuda.cu\"], verbose=True, extra_cuda_cflags=[\"-res-usage\", \"--maxrregcount 60\", \"--use_fast_math\", \"-O3\", \"-Xptxas -O3\", \"--extra-device-vectorization\", f\"-DTmax={T_MAX}\"])\n    class WKV(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, B, T, C, w, u, k, v):\n            ctx.B = B\n            ctx.T = T\n            ctx.C = C\n            assert T <= T_MAX\n            assert B * C % min(C, 32) == 0\n            if \"32\" in os.environ[\"RWKV_FLOAT_MODE\"]:\n                w = -torch.exp(w.contiguous())\n                u = u.contiguous()\n                k = k.contiguous()\n                v = v.contiguous()"
        },
        {
            "comment": "The code is handling the forward and backward passes of a model. It first checks the environment variable 'RWKV_FLOAT_MODE' to determine the data type for output 'y'. If '32' is present in this environment variable, it directly returns 'y'. If 'fp16' is set, it converts 'y' to half precision and returns it. If 'bf16' is set, it converts 'y' to BFloat16 and returns it. The backward method applies constraints on the dimensions of tensors and retrieves saved tensors from context for gradients calculation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":93-116",
            "content": "            else:\n                w = -torch.exp(w.float().contiguous())\n                u = u.float().contiguous()\n                k = k.float().contiguous()\n                v = v.float().contiguous()\n            y = torch.empty((B, T, C), device=w.device, memory_format=torch.contiguous_format)\n            wkv_cuda.forward(B, T, C, w, u, k, v, y)\n            ctx.save_for_backward(w, u, k, v, y)\n            if \"32\" in os.environ[\"RWKV_FLOAT_MODE\"]:\n                return y\n            elif os.environ[\"RWKV_FLOAT_MODE\"] == \"fp16\":\n                return y.half()\n            elif os.environ[\"RWKV_FLOAT_MODE\"] == \"bf16\":\n                return y.bfloat16()\n        @staticmethod\n        def backward(ctx, gy):\n            B = ctx.B\n            T = ctx.T\n            C = ctx.C\n            assert T <= T_MAX\n            assert B * C % min(C, 32) == 0\n            w, u, k, v, y = ctx.saved_tensors\n            gw = torch.empty((B, C), device=gy.device, memory_format=torch.contiguous_format)\n            gu = torch.empty((B, C), device=gy.device, memory_format=torch.contiguous_format)"
        },
        {
            "comment": "The code defines a function for running CUDA on BERT-like transformer models. It initializes empty tensors for gradients, and then calls the backward operation of WKV to compute gradients. Depending on the float mode environment variable, it returns the gradients in different precisions: None, half (fp16), or bfloat16. If no CUDA is used, the code calls a RUN_CUDA function, which applies the transformer model with CUDA.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":117-134",
            "content": "            gk = torch.empty((B, T, C), device=gy.device, memory_format=torch.contiguous_format)\n            gv = torch.empty((B, T, C), device=gy.device, memory_format=torch.contiguous_format)\n            if \"32\" in os.environ[\"RWKV_FLOAT_MODE\"]:\n                wkv_cuda.backward(B, T, C, w, u, k, v, y, gy.contiguous(), gw, gu, gk, gv)\n            else:\n                wkv_cuda.backward(B, T, C, w, u, k, v, y, gy.float().contiguous(), gw, gu, gk, gv)\n            gw = torch.sum(gw, dim=0)\n            gu = torch.sum(gu, dim=0)\n            if \"32\" in os.environ[\"RWKV_FLOAT_MODE\"]:\n                return (None, None, None, gw, gu, gk, gv)\n            elif os.environ[\"RWKV_FLOAT_MODE\"] == \"fp16\":\n                return (None, None, None, gw.half(), gu.half(), gk.half(), gv.half())\n            elif os.environ[\"RWKV_FLOAT_MODE\"] == \"bf16\":\n                return (None, None, None, gw.bfloat16(), gu.bfloat16(), gk.bfloat16(), gv.bfloat16())\ndef RUN_CUDA(B, T, C, w, u, k, v):\n    return WKV.apply(B, T, C, w, u, k, v)"
        },
        {
            "comment": "This code defines a class for the RWKV_TimeMix_RWKV5_Preview module, which is an extension of the MyModule class. It initializes instance variables related to the model's parameters and performs some sanity checks on the input arguments. The time_mix_k and time_mix_v parameters are calculated based on a combination of the layer index and other factors.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":136-161",
            "content": "########################################################################################################\nclass RWKV_TimeMix_RWKV5_Preview(MyModule):\n    def __init__(self, args, layer_id):\n        super().__init__()\n        self.args = args\n        self.layer_id = layer_id\n        self.head_size = 64\n        self.n_head = args.dim_att // self.head_size\n        assert args.dim_att % self.n_head == 0\n        self.head_size_divisor = 8\n        self.chunk_len = 512\n        assert args.ctx_len % self.chunk_len == 0\n        with torch.no_grad():\n            ratio_0_to_1 = layer_id / (args.n_layer - 1)  # 0 to 1\n            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)  # 1 to ~0\n            ddd = torch.ones(1, 1, args.n_embd)\n            for i in range(args.n_embd):\n                ddd[0, 0, i] = i / args.n_embd\n            # fancy time_mix\n            self.time_mix_k = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n            self.time_mix_v = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0) + 0.3 * ratio_0_to_1)"
        },
        {
            "comment": "This code initializes parameters for a time-related model component, including time_mix_r, time_mix_g (conditionally), time_decay, and time_faaaa (conditionally). The values are determined by ratios and layer index. Printing the parameter values is optional based on environment variables.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":162-181",
            "content": "            self.time_mix_r = nn.Parameter(torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n            if 'r3' in os.environ[\"RWKV_MY_TESTING\"]:\n                self.time_mix_g = nn.Parameter(torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n                self.gate = nn.Linear(args.n_embd, args.dim_att, bias=False)\n            # fancy time_decay\n            decay_speed = torch.ones(self.n_head)\n            for h in range(self.n_head):\n                decay_speed[h] = -6 + 5 * (h / (self.n_head - 1)) ** (0.7 + 1.3 * ratio_0_to_1)\n            self.time_decay = nn.Parameter(decay_speed)\n            # print(layer_id, self.time_decay.flatten()[:3].cpu().numpy(), '...', self.time_decay.flatten()[-3:].cpu().numpy())\n            if 'r2' in os.environ[\"RWKV_MY_TESTING\"]:\n                tmp = torch.zeros(self.n_head)\n                for h in range(self.n_head):\n                    tmp[h] = ratio_0_to_1 * (1 - (h / (self.n_head - 1)))\n                self.time_faaaa = nn.Parameter(tmp)\n            else:\n                self.time_first = nn.Parameter(torch.ones(self.n_head) * (-3.0))"
        },
        {
            "comment": "The code defines a model with time shift, receptance, key, value, and output layers. It also includes a group normalization layer (ln_x). If 'r3' is present in the environment, it creates a jit_func that processes input x by mixing it with the previous timestep to produce k, v, r, and g. The receptance layer then transforms the r values into BHTS format.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":183-203",
            "content": "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        self.receptance = nn.Linear(args.n_embd, args.dim_att, bias=False)\n        self.key = nn.Linear(args.n_embd, args.dim_att, bias=False)\n        self.value = nn.Linear(args.n_embd, args.dim_att, bias=False)\n        self.output = nn.Linear(args.dim_att, args.n_embd, bias=False)\n        self.ln_x = nn.GroupNorm(self.n_head, args.dim_att)\n    if 'r3' in os.environ[\"RWKV_MY_TESTING\"]:\n        @MyFunction\n        def jit_func(self, x):\n            B, TT, C = x.size()\n            xx = self.time_shift(x) # Mix x with the previous timestep to produce xk, xv, xr\n            xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n            xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n            xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n            xg = x * self.time_mix_g + xx * (1 - self.time_mix_g)\n            r = self.receptance(xr).view(B, TT, self.n_head, self.head_size).transpose(1, 2)            # BTC -> BHTS\n            k "
        },
        {
            "comment": "This code is defining and implementing a function for an attention mechanism in a transformer model. It calculates the attention scores, performs weighted sum of values, updates the states, and applies non-linearity before returning the final output.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":203-227",
            "content": "= self.key(xk).view(B, TT, self.n_head, self.head_size).transpose(1, 2).transpose(-2, -1) # BTC -> BHTS -> BHST\n            v = self.value(xv).view(B, TT, self.n_head, -1).transpose(1, 2)                 # BTC -> BHTS\n            g = F.silu(self.gate(xg))\n            return r, k, v, g\n        @MyFunction\n        def jit_func_2(self, r, k, v, g, w, wk, wb, ws):\n            B, H, TT, S = r.size()\n            T = self.chunk_len\n            s = torch.zeros(B, H, S, S, device=r.device, dtype=r.dtype)  # state\n            x = torch.zeros(B, H, TT, S, device=r.device, dtype=r.dtype) # output\n            for i in range(TT // T):\n                rr = r[:, :, i*T:i*T+T, :]\n                kk = k[:, :, :, i*T:i*T+T]\n                vv = v[:, :, i*T:i*T+T, :]\n                x[:, :, i*T:i*T+T, :] = ((rr @ kk) * w) @ vv  +  (rr @ s) * wb\n                s = ws * s + (kk * wk) @ vv\n            x = x.transpose(1, 2).contiguous().view(B * TT, H*S) # BHTS -> BTHS -> BTC\n            x = self.ln_x(x / self.head_size_divisor).view(B, TT, H*S) * g"
        },
        {
            "comment": "This code defines a class with two JIT functions. The first function takes an input tensor x and performs time shifting, then separates it into three components (k, v, r). The second function takes the output of the first function and creates another tensor s using torch.zeros. Both functions use jitted methods for faster execution.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":228-250",
            "content": "            return self.output(x)\n    else:\n        @MyFunction\n        def jit_func(self, x):\n            B, TT, C = x.size()\n            xx = self.time_shift(x) # Mix x with the previous timestep to produce xk, xv, xr\n            xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n            xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n            xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n            r = self.receptance(xr).view(B, TT, self.n_head, self.head_size).transpose(1, 2)            # BTC -> BHTS\n            k = self.key(xk).view(B, TT, self.n_head, self.head_size).transpose(1, 2).transpose(-2, -1) # BTC -> BHTS -> BHST\n            v = self.value(xv).view(B, TT, self.n_head, self.head_size).transpose(1, 2)                 # BTC -> BHTS\n            return r, k, v\n        @MyFunction\n        def jit_func_2(self, r, k, v, w, wk, wb, ws):\n            B, H, TT, S = r.size()\n            T = self.chunk_len\n            s = torch.zeros(B, H, S, S, device=r.device, dtype=r.dtype)  # state"
        },
        {
            "comment": "Initializes a tensor with zeros, performs matrix operations to compute the output tensor, and applies layer normalization before returning the final output. The code also includes variable assignments for time-based decay and initial values for other computations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":251-280",
            "content": "            x = torch.zeros(B, H, TT, S, device=r.device, dtype=r.dtype) # output\n            for i in range(TT // T):\n                rr = r[:, :, i*T:i*T+T, :]\n                kk = k[:, :, :, i*T:i*T+T]\n                vv = v[:, :, i*T:i*T+T, :]\n                x[:, :, i*T:i*T+T, :] = ((rr @ kk) * w) @ vv  +  (rr @ s) * wb\n                s = ws * s + (kk * wk) @ vv\n            x = x.transpose(1, 2).contiguous().view(B * TT, H*S) # BHTS -> BTHS -> BTC\n            x = self.ln_x(x / self.head_size_divisor).view(B, TT, H*S)\n            return self.output(x)\n    def forward(self, x):\n        H = self.n_head\n        T = self.chunk_len\n        if 'r3' in os.environ[\"RWKV_MY_TESTING\"]:\n            r, k, v, g = self.jit_func(x)\n        else:\n            r, k, v = self.jit_func(x)\n        w = torch.exp(-torch.exp(self.time_decay.float())).unsqueeze(-1)\n        if 'r2' in os.environ[\"RWKV_MY_TESTING\"]:\n            u = self.time_faaaa.float().unsqueeze(-1)\n        else:\n            u = torch.exp(self.time_first.float()).unsqueeze(-1)"
        },
        {
            "comment": "This code segment is manipulating and reshaping a tensor 'w' by repeating, powering, transposing, and padding it. It then reshapes the result to specific dimensions and performs type conversions before returning from either jit_func_2 function depending on the environment variable \"RWKV_MY_TESTING\".",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":282-310",
            "content": "################################################################################\n########\n        ws = w.pow(T).reshape(1, H, 1, 1)\n        ind = torch.arange(T-1, -1, -1, device=r.device).unsqueeze(0).repeat(H, 1)\n        w = w.repeat(1, T).pow(ind)\n        wk = w.reshape(1, H, 1, T)\n        wb = wk.transpose(-2, -1).flip(2)\n        w = torch.cat([w[:, 1:], u], dim=1)\n        w = F.pad(w, (0, T))\n        w = torch.tile(w, [T])\n        w = w[:, :-T].reshape(-1, T, 2 * T - 1)\n        w = w[:, :, T-1:].reshape(1, H, T, T)\n########\n################################################################################\n        w = w.to(dtype=r.dtype)\n        wk = wk.to(dtype=r.dtype)\n        wb = wb.to(dtype=r.dtype)\n        ws = ws.to(dtype=r.dtype)\n        if 'r3' in os.environ[\"RWKV_MY_TESTING\"]:\n            return self.jit_func_2(r, k, v, g, w, wk, wb, ws)\n        else:\n            return self.jit_func_2(r, k, v, w, wk, wb, ws)        \n########################################################################################################"
        },
        {
            "comment": "This code defines a WKV_5 function as a wrapper for CUDA RWKV5 kernel, which performs tensor operations on B, T, C, H, r, k, v, w, and u tensors. The kernel is loaded from the specified sources, with optional environment variables controlling its size. It asserts that certain conditions are met, such as matching data types and sizes for each input tensor, before proceeding with further computations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":311-333",
            "content": "# CUDA RWKV5 Kernel\n########################################################################################################\nif 'r4' in os.environ[\"RWKV_MY_TESTING\"]:\n    HEAD_SIZE = int(os.environ[\"RWKV_HEAD_SIZE_A\"])\n    wkv5_cuda = load(name=\"wkv5\", sources=[\"cuda/wkv5_op.cpp\", f\"cuda/wkv5_cuda.cu\"],\n                    verbose=True, extra_cuda_cflags=[\"-res-usage\", \"--use_fast_math\", \"-O3\", \"-Xptxas -O3\", \"--extra-device-vectorization\", f\"-D_N_={HEAD_SIZE}\"])\n    class WKV_5(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, B, T, C, H, r, k, v, w, u):\n            with torch.no_grad():\n                assert r.dtype == torch.bfloat16\n                assert k.dtype == torch.bfloat16\n                assert v.dtype == torch.bfloat16\n                assert w.dtype == torch.bfloat16\n                assert u.dtype == torch.bfloat16\n                assert HEAD_SIZE == C // H\n                ctx.B = B\n                ctx.T = T\n                ctx.C = C\n                ctx.H = H\n                assert r.is_contiguous()"
        },
        {
            "comment": "This code snippet is checking the contiguity of tensors k, v, w, and u. It then initializes eew and ew as exponentiated versions of w, saves them along with r, k, v, and u for backpropagation. Next, it creates an empty tensor y for forward pass with specific device, dtype, and memory format. Finally, it defines a backward method to compute gradients for backpropagation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":334-355",
            "content": "                assert k.is_contiguous()\n                assert v.is_contiguous()\n                assert w.is_contiguous()\n                assert u.is_contiguous()\n                ew = (-torch.exp(w.float())).contiguous()\n                eew = (torch.exp(ew)).contiguous()\n                ctx.save_for_backward(r, k, v, eew, ew, u)\n                y = torch.empty((B, T, C), device=r.device, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)\n                wkv5_cuda.forward(B, T, C, H, r, k, v, eew, u, y)\n                return y\n        @staticmethod\n        def backward(ctx, gy):\n            with torch.no_grad():\n                assert gy.dtype == torch.bfloat16\n                B = ctx.B\n                T = ctx.T\n                C = ctx.C\n                H = ctx.H\n                assert gy.is_contiguous()\n                r, k, v, eew, ew, u = ctx.saved_tensors\n                gr = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)"
        },
        {
            "comment": "This code initializes four tensors with uniform random values for model parameters, then calls a CUDA function to perform calculations on the input data. The resulting gradients are stored in gk and gu and returned along with other results.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":356-366",
            "content": "                gk = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)\n                gv = torch.empty((B, T, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)\n                gw = torch.empty((B, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)\n                gu = torch.empty((B, C), device=gy.device, requires_grad=False, dtype=torch.bfloat16, memory_format=torch.contiguous_format) # .uniform_(-1, 1)\n                wkv5_cuda.backward(B, T, C, H, r, k, v, eew, ew, u, gy, gr, gk, gv, gw, gu)\n                gw = torch.sum(gw, 0).view(H, C//H)\n                gu = torch.sum(gu, 0).view(H, C//H)\n                return (None, None, None, None, gr, gk, gv, gw, gu)\n    def RUN_CUDA_RWKV5(B, T, C, H, r, k, v, w, u):\n        return WKV_5.apply(B, T, C, H, r, k, v, w, u)"
        },
        {
            "comment": "The code initializes the RWKV_TimeMix_RWKV5 class, sets up layer parameters based on input arguments, and creates time_mix_k and time_mix_v parameters using a combination of ratio-based calculations and input arguments.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":368-391",
            "content": "########################################################################################################\nclass RWKV_TimeMix_RWKV5(MyModule):\n    def __init__(self, args, layer_id):\n        super().__init__()\n        self.args = args\n        self.layer_id = layer_id\n        self.head_size = args.head_size_a\n        assert HEAD_SIZE == self.head_size # change HEAD_SIZE to match args.head_size_a\n        self.n_head = args.dim_att // self.head_size\n        assert args.dim_att % self.n_head == 0\n        self.head_size_divisor = args.head_size_divisor\n        with torch.no_grad():\n            ratio_0_to_1 = layer_id / (args.n_layer - 1)  # 0 to 1\n            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)  # 1 to ~0\n            ddd = torch.ones(1, 1, args.n_embd)\n            for i in range(args.n_embd):\n                ddd[0, 0, i] = i / args.n_embd\n            # fancy time_mix\n            self.time_mix_k = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n            self.time_mix_v = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0) + 0.3 * ratio_0_to_1)"
        },
        {
            "comment": "This code initializes three learnable parameters: time_mix_r, time_mix_g, and time_faaaa. It also includes a decay speed parameter (time_decay) and a zero-padding operation (time_shift). These parameters are used for time-related operations in the model. The receptance layer is initialized as a linear layer without bias, taking input embeddings of size args.n_embd to args.dim_att. This code likely contributes to the model's ability to process temporal information effectively.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":392-410",
            "content": "            self.time_mix_r = nn.Parameter(torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n            self.time_mix_g = nn.Parameter(torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n            # fancy time_decay\n            decay_speed = torch.ones(args.dim_att)\n            for n in range(args.dim_att):\n                decay_speed[n] = -6 + 5 * (n / (args.dim_att - 1)) ** (0.7 + 1.3 * ratio_0_to_1)\n            self.time_decay = nn.Parameter(decay_speed.reshape(self.n_head, self.head_size))\n            # print(layer_id, self.time_decay.flatten()[:3].cpu().numpy(), '...', self.time_decay.flatten()[-3:].cpu().numpy())\n            tmp = torch.zeros(args.dim_att)\n            for n in range(args.dim_att):\n                zigzag = ((n + 1) % 3 - 1) * 0.1\n                tmp[n] = ratio_0_to_1 * (1 - (n / (args.dim_att - 1))) + zigzag\n            self.time_faaaa = nn.Parameter(tmp.reshape(self.n_head, self.head_size))\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        self.receptance = nn.Linear(args.n_embd, args.dim_att, bias=False)"
        },
        {
            "comment": "The code defines a model with four linear layers and one group normalization layer. The `jit_func` method performs time-shifted mixing of input `x` to produce key, value, residual, and gate tensors, which are then passed through their respective layers and normalized. The `jit_func_2` method applies group normalization and divides by the head size divisor before reshaping the tensor.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":411-440",
            "content": "        self.key = nn.Linear(args.n_embd, args.dim_att, bias=False)\n        self.value = nn.Linear(args.n_embd, args.dim_att, bias=False)\n        self.output = nn.Linear(args.dim_att, args.n_embd, bias=False)\n        self.gate = nn.Linear(args.n_embd, args.dim_att, bias=False)\n        self.ln_x = nn.GroupNorm(self.n_head, args.dim_att)\n    @MyFunction\n    def jit_func(self, x):\n        B, T, C = x.size()\n        xx = self.time_shift(x) # Mix x with the previous timestep to produce xk, xv, xr\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        xg = x * self.time_mix_g + xx * (1 - self.time_mix_g)\n        r = self.receptance(xr)\n        k = self.key(xk)\n        v = self.value(xv)\n        g = F.silu(self.gate(xg))\n        return r, k, v, g\n    @MyFunction\n    def jit_func_2(self, x, g):\n        B, T, C = x.size()\n        x = x.view(B * T, C)\n        x = self.ln_x(x / self.head_size_divisor).view(B, T, C)"
        },
        {
            "comment": "This code defines a class `RWKV_TimeMix` that inherits from `MyModule`. It initializes attributes such as `args`, `layer_id`, `ctx_len`, and `n_embd`. It also performs a fancy init by calculating ratios based on the layer ID and number of layers.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":441-471",
            "content": "        x = self.output(x * g)\n        return x\n    def forward(self, x):\n        B, T, C = x.size()\n        H = self.n_head\n        r, k, v, g = self.jit_func(x)\n        x = RUN_CUDA_RWKV5(B, T, C, H, r, k, v, w=self.time_decay, u=self.time_faaaa)\n        return self.jit_func_2(x, g)\n########################################################################################################\n# RWKV: RWKV Time-mix + RWKV Channel-mix\n########################################################################################################\nclass RWKV_TimeMix(MyModule):\n    def __init__(self, args, layer_id):\n        super().__init__()\n        self.args = args\n        self.layer_id = layer_id\n        self.ctx_len = args.ctx_len\n        self.n_embd = args.n_embd\n        with torch.no_grad():  # fancy init\n            ratio_0_to_1 = layer_id / (args.n_layer - 1)  # 0 to 1\n            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)  # 1 to ~0\n            ddd = torch.ones(1, 1, args.n_embd)\n            for i in range(args.n_embd):"
        },
        {
            "comment": "This code is initializing parameters for a time-based transformer model layer. It sets the decay rate, first value in time series, and mixing coefficients using fancy techniques to achieve more complexity in the attention mechanism. The time shift operation pads the input, and a linear transformation is applied to create the key values.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":472-491",
            "content": "                ddd[0, 0, i] = i / args.n_embd\n            # fancy time_decay\n            decay_speed = torch.ones(args.dim_att)\n            for h in range(args.dim_att):\n                decay_speed[h] = -5 + 8 * (h / (args.dim_att - 1)) ** (0.7 + 1.3 * ratio_0_to_1)\n            self.time_decay = nn.Parameter(decay_speed)\n            # print(layer_id, self.time_decay.flatten()[:3].cpu().numpy(), '...', self.time_decay.flatten()[-3:].cpu().numpy())\n            # fancy time_first\n            zigzag = torch.tensor([(i + 1) % 3 - 1 for i in range(args.dim_att)]) * 0.5\n            self.time_first = nn.Parameter(torch.ones(args.dim_att) * math.log(0.3) + zigzag)\n            # fancy time_mix\n            self.time_mix_k = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n            self.time_mix_v = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0) + 0.3 * ratio_0_to_1)\n            self.time_mix_r = nn.Parameter(torch.pow(ddd, 0.5 * ratio_1_to_almost0))\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        self.key = nn.Linear(args.n_embd, args.dim_att, bias=False)"
        },
        {
            "comment": "This code initializes layers for a transformer model and includes optional testing-specific buffers and parameters. The 'MyFunction' decorator suggests additional functionality is defined elsewhere.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":492-509",
            "content": "        self.value = nn.Linear(args.n_embd, args.dim_att, bias=False)\n        self.receptance = nn.Linear(args.n_embd, args.dim_att, bias=False)\n        self.output = nn.Linear(args.dim_att, args.n_embd, bias=False)\n        if 'a' in os.environ[\"RWKV_MY_TESTING\"]:\n            self.register_buffer(\"att_mask\", torch.tril(torch.ones(args.ctx_len, args.ctx_len)))\n            d_qkv = args.n_embd // 16\n            self.qq = nn.Linear(args.n_embd, d_qkv, bias=False)\n            self.kk = nn.Linear(args.n_embd, d_qkv, bias=False)\n            self.vv = nn.Linear(args.n_embd, d_qkv, bias=False)\n            self.oo = nn.Linear(d_qkv, args.n_embd, bias=False)\n            with torch.no_grad():\n                self.time_mix_qq = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n                self.time_mix_kk = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n                self.time_mix_vv = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0) + 0.3 * ratio_0_to_1)\n    if 'a' not in os.environ[\"RWKV_MY_TESTING\"]:\n        @MyFunction"
        },
        {
            "comment": "This function defines jit_func, which mixes the current timestep with the previous one for x, k, and v. Then forward function calculates RWKV using these variables and applies softmax to attentions. QKV function computes attention scores between query (q) and key (k), masking zero-valued elements and applying softmax along dim=-1.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":510-532",
            "content": "        def jit_func(self, x):\n            xx = self.time_shift(x) # Mix x with the previous timestep to produce xk, xv, xr\n            xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n            xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n            xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n            k = self.key(xk)\n            v = self.value(xv)\n            r = self.receptance(xr)\n            sr = torch.sigmoid(r)\n            return sr, k, v\n        def forward(self, x):\n            B, T, C = x.size()  # x = (Batch,Time,Channel)\n            sr, k, v = self.jit_func(x)\n            rwkv = sr * RUN_CUDA(B, T, self.args.dim_att, self.time_decay, self.time_first, k, v)\n            return self.output(rwkv)\n    if 'a' in os.environ[\"RWKV_MY_TESTING\"]:\n        @MyFunction\n        def QKV(self, q, k, v):\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.att_mask == 0, float('-inf'))\n            att = F.softmax(att, dim = -1)"
        },
        {
            "comment": "The code defines a function `jit_funcQKV` that takes an input tensor `x`, applies time shifting and mixing, then extracts key, value, receptance, qq, kk, vv from the result. The forward function reshapes input tensor `x` to (Batch, Time, Channel) and calls `jit_funcQKV` to compute sr, k, v, qq, kk, vv. It then uses these outputs with another function `RUN_CUDA` to calculate the final output rwkv.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":533-557",
            "content": "            x = att @ v\n            return x\n        @MyFunction\n        def jit_funcQKV(self, x):\n            xx = self.time_shift(x) # Mix x with the previous timestep to produce xk, xv, xr\n            xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n            xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n            xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n            xqq = x * self.time_mix_qq + xx * (1 - self.time_mix_qq)\n            xkk = x * self.time_mix_kk + xx * (1 - self.time_mix_kk)\n            xvv = x * self.time_mix_vv + xx * (1 - self.time_mix_vv)\n            k = self.key(xk)\n            v = self.value(xv)\n            r = self.receptance(xr)\n            sr = torch.sigmoid(r)\n            qq = self.qq(xqq)\n            kk = self.kk(xkk)\n            vv = self.vv(xvv)\n            return sr, k, v, qq, kk, vv\n        def forward(self, x):\n            B, T, C = x.size()  # x = (Batch,Time,Channel)\n            sr, k, v, qq, kk, vv = self.jit_funcQKV(x)\n            rwkv = sr * RUN_CUDA(B, T, self.args.dim_att, self.time_decay, self.time_first, k, v)"
        },
        {
            "comment": "The code defines a class `RWKV_ChannelMix` that extends the `MyModule` class. It initializes the layer with parameters based on the input arguments and layer ID, and includes time mixing and linear layers for key, receptance, and value operations.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":558-580",
            "content": "            rwkv = self.output(rwkv) + self.oo(self.QKV(qq, kk, vv))\n            return rwkv\n########################################################################################################\nclass RWKV_ChannelMix(MyModule):\n    def __init__(self, args, layer_id):\n        super().__init__()\n        self.args = args\n        self.layer_id = layer_id\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        with torch.no_grad():  # fancy init of time_mix\n            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)  # 1 to ~0\n            ddd = torch.ones(1, 1, args.n_embd)\n            for i in range(args.n_embd):\n                ddd[0, 0, i] = i / args.n_embd\n            self.time_mix_k = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n            self.time_mix_r = nn.Parameter(torch.pow(ddd, ratio_1_to_almost0))\n        self.key = nn.Linear(args.n_embd, args.dim_ffn, bias=False)\n        self.receptance = nn.Linear(args.n_embd, args.n_embd, bias=False)\n        self.value = nn.Linear(args.dim_ffn, args.n_embd, bias=False)"
        },
        {
            "comment": "The code defines a class \"MishGLU\" that inherits from \"MyModule\". It initializes the object with parameters and layers. The class contains a \"forward\" method which performs time shifting, mixing, and calculations using various linear layers and functions. It uses parameters that depend on the layer ID and number of layers for time-based mixing. The output is obtained through a combination of sigmoid and value calculation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":582-609",
            "content": "    @MyFunction\n    def forward(self, x):\n        xx = self.time_shift(x)\n        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        k = self.key(xk)\n        k = torch.relu(k) ** 2\n        kv = self.value(k)\n        return torch.sigmoid(self.receptance(xr)) * kv\nclass MishGLU(MyModule):\n    def __init__(self, args, layer_id):\n        super().__init__()\n        self.args = args\n        self.layer_id = layer_id\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        with torch.no_grad():\n            ratio_1_to_almost0 = 1.0 - (layer_id / args.n_layer)\n            x = torch.ones(1, 1, args.n_embd)\n            for i in range(args.n_embd):\n                x[0, 0, i] = i / args.n_embd\n            self.time_mix_k = nn.Parameter(torch.pow(x, ratio_1_to_almost0))\n            self.time_mix_r = nn.Parameter(torch.pow(x, ratio_1_to_almost0))\n            self.aa = nn.Linear(args.n_embd, args.dim_ffn, bias=False)\n            self.bb = nn.Linear(args.n_embd, args.dim_ffn, bias=False)"
        },
        {
            "comment": "The code defines a model with a block class. It contains linear layers, layer normalization, and Mish activation function. The block has different layer norms for each stage, with the first layer having an additional normalization layer. There is also a parameter for positional embedding if enabled in the arguments.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":610-638",
            "content": "            self.value = nn.Linear(args.dim_ffn, args.n_embd, bias=False)\n    @MyFunction\n    def forward(self, x):\n        xx = self.time_shift(x)\n        xa = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n        xb = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n        a = self.aa(xa)\n        b = self.bb(xb)\n        return self.value(a * F.mish(b))\n########################################################################################################\n# The RWKV Model with our blocks\n########################################################################################################\nclass Block(nn.Module):\n    def __init__(self, args, layer_id):\n        super().__init__()\n        self.args = args\n        self.layer_id = layer_id\n        self.ln1 = nn.LayerNorm(args.n_embd)\n        self.ln2 = nn.LayerNorm(args.n_embd)\n        if self.layer_id == 0:\n            self.ln0 = nn.LayerNorm(args.n_embd)\n            if args.my_pos_emb > 0:\n                self.pos_emb_x = nn.Parameter(torch.zeros((1,args.my_pos_emb,args.n_embd)))"
        },
        {
            "comment": "The code initializes the model's parameters, creates layers for position embedding and various attention mechanisms based on environment variables, and includes a MishGLU or RWKV_ChannelMix layer depending on the environment variable. If 'g' is in the environment variable, it uses a different ffn (fully connected feedforward) layer. Additionally, if args.tiny\\_att\\_dim is greater than 0 and the current layer matches args.tiny\\_att\\_layer, it adds specific layers for tiny attention with LayerNorm and linear layers for query, key, and value.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":639-660",
            "content": "                self.pos_emb_y = nn.Parameter(torch.zeros((args.my_pos_emb,1,args.n_embd)))\n        if self.layer_id == 0 and self.args.pre_ffn > 0:\n            self.ffnPre = RWKV_ChannelMix(args, 0)\n        else:\n            if 'r4' in os.environ[\"RWKV_MY_TESTING\"]:\n                self.att = RWKV_TimeMix_RWKV5(args, layer_id)\n            elif 'r' in os.environ[\"RWKV_MY_TESTING\"]:\n                self.att = RWKV_TimeMix_RWKV5_Preview(args, layer_id)\n            else:\n                self.att = RWKV_TimeMix(args, layer_id)\n        if 'g' in os.environ[\"RWKV_MY_TESTING\"]:\n            self.ffn = MishGLU(args, layer_id)\n        else:\n            self.ffn = RWKV_ChannelMix(args, layer_id)\n        if args.tiny_att_dim > 0 and self.layer_id == args.tiny_att_layer:\n            self.tiny_ln = nn.LayerNorm(args.n_embd)\n            self.tiny_q = nn.Linear(args.n_embd, args.tiny_att_dim, bias=False)\n            self.tiny_k = nn.Linear(args.n_embd, args.tiny_att_dim, bias=False)\n            self.tiny_v = nn.Linear(args.n_embd, args.n_embd, bias=False)"
        },
        {
            "comment": "The code defines a forward function for a model and includes dropout layers, LayerNorm (lnX), attention layer (att), feed-forward network (ffnPre and ffn), position embedding (pos_emb_x and pos_emb_y), and a triangular mask (tiny_mask). It applies the layers based on input parameters like dropout rate, pre-ffn flag, and layer index.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":661-687",
            "content": "            self.register_buffer(\"tiny_mask\", torch.tril(torch.ones(args.ctx_len, args.ctx_len)))\n        if args.dropout > 0:\n            self.drop0 = nn.Dropout(p = args.dropout)\n            self.drop1 = nn.Dropout(p = args.dropout)\n    def forward(self, x, x_emb=None):\n        args = self.args\n        B, T, C = x.size()\n        if self.layer_id == 0:\n            x = self.ln0(x)\n            if args.my_pos_emb > 0:\n                pos_emb = (self.pos_emb_x + self.pos_emb_y).reshape(T+1, -1)[:-1,:]\n                x = x + pos_emb\n        if self.args.dropout == 0:\n            if self.layer_id == 0 and args.pre_ffn > 0:\n                x = x + self.ffnPre(self.ln1(x))\n            else:\n                x = x + self.att(self.ln1(x))\n            x = x + self.ffn(self.ln2(x))\n        else:\n            if self.layer_id == 0 and args.pre_ffn > 0:\n                x = self.drop0(x + self.ffnPre(self.ln1(x)))\n            else:\n                x = self.drop0(x + self.att(self.ln1(x)))\n            x = self.drop1(x + self.ffn(self.ln2(x)))"
        },
        {
            "comment": "The code is implementing an attention mechanism in the RWKV model. It checks if the layer id matches a specific layer and if a smaller attention dimension is desired. If both conditions are met, it applies the scaled dot product attention, masks out padding tokens, and adds the result to the input. The L2Wrap function is used to implement an L2 regularization term in the loss computation.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":689-720",
            "content": "        if args.tiny_att_dim > 0 and self.layer_id == args.tiny_att_layer:\n            xx = self.tiny_ln(x)\n            q = self.tiny_q(xx)[:, :T, :]\n            k = self.tiny_k(xx)[:, :T, :]\n            c = (q @ k.transpose(-2, -1)) * (args.tiny_att_dim ** (-0.5))\n            c = c.masked_fill(self.tiny_mask[:T, :T] == 0, 0)\n            x = x + c @ self.tiny_v(x_emb)\n        return x\nclass L2Wrap(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, loss, y):\n        ctx.save_for_backward(y)\n        return loss\n    @staticmethod\n    def backward(ctx, grad_output):\n        y = ctx.saved_tensors[0]\n        # to encourage the logits to be close to 0\n        factor = 1e-4 / (y.shape[0] * y.shape[1])\n        maxx, ids = torch.max(y, -1, keepdim=True)\n        gy = torch.zeros_like(y)\n        gy.scatter_(-1, ids, maxx * factor)\n        return (grad_output, gy)\nclass RWKV(pl.LightningModule):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        if not hasattr(args, 'dim_att'):"
        },
        {
            "comment": "This code initializes a transformer model by setting attributes based on argument values, creating embedding and linear layers, and organizing blocks within the model. Assertions ensure that certain dimensions are multiples of 32 for optimization purposes. If certain attributes aren't set, the code assigns default values. Additionally, if dropout or key/query projection is enabled, corresponding layers are created.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":721-744",
            "content": "            args.dim_att = args.n_embd\n        if not hasattr(args, 'dim_ffn'):\n            args.dim_ffn = args.n_embd * 4\n        if not hasattr(args, 'tiny_att_layer'):\n            args.tiny_att_layer = -1\n        if not hasattr(args, 'tiny_att_dim'):\n            args.tiny_att_dim = -1\n        assert args.n_embd % 32 == 0\n        assert args.dim_att % 32 == 0\n        assert args.dim_ffn % 32 == 0\n        self.emb = nn.Embedding(args.vocab_size, args.n_embd)\n        self.blocks = nn.ModuleList([Block(args, i) for i in range(args.n_layer)])\n        self.ln_out = nn.LayerNorm(args.n_embd)\n        self.head = nn.Linear(args.n_embd, args.vocab_size, bias=False)\n        if args.head_qk > 0:\n            self.head_q = nn.Linear(args.n_embd, args.head_qk, bias=False)\n            self.head_k = nn.Linear(args.n_embd, args.head_qk, bias=False)\n            self.register_buffer(\"copy_mask\", torch.tril(torch.ones(args.ctx_len, args.ctx_len)))\n        if args.dropout > 0:\n            self.drop0 = nn.Dropout(p = args.dropout)"
        },
        {
            "comment": "This function sets up optimizers based on parameter names and provided arguments. It categorizes parameters into different learning rate groups: 1x, 2x, or 3x multipliers for time-related layers; decay rate for weight decay; and all others with a single learning rate.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":746-774",
            "content": "    def configure_optimizers(self):\n        args = self.args\n        lr_decay = set()\n        lr_1x = set()\n        lr_2x = set()\n        lr_3x = set()\n        for n, p in self.named_parameters():\n            if (\"time_mix\" in n) and (args.layerwise_lr > 0):\n                if args.my_pile_stage == 2:\n                    lr_2x.add(n)\n                else:\n                    lr_1x.add(n)\n            elif (\"time_decay\" in n) and (args.layerwise_lr > 0):\n                if args.my_pile_stage == 2:\n                    lr_3x.add(n)\n                else:\n                    lr_2x.add(n)\n            elif (\"time_faaaa\" in n) and (args.layerwise_lr > 0):\n                if args.my_pile_stage == 2:\n                    lr_2x.add(n)\n                else:\n                    lr_1x.add(n)\n            elif (\"time_first\" in n) and (args.layerwise_lr > 0):\n                lr_3x.add(n)\n            elif (len(p.squeeze().shape) >= 2) and (args.weight_decay > 0):\n                lr_decay.add(n)\n            else:\n                lr_1x.add(n)"
        },
        {
            "comment": "This code is initializing optimizer groups for layerwise learning rate (LR) in a neural network model. It sorts the LR values and creates separate optimizer groups for each level of scaling, assigning different layers to each group based on their corresponding scaling factor. The layer-specific scaling is applied when args.my_pile_stage == 2, otherwise, it uses a different set of scales. This allows for more fine-grained control over the learning rates for different layers during training.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":776-796",
            "content": "        lr_decay = sorted(list(lr_decay))\n        lr_1x = sorted(list(lr_1x))\n        lr_2x = sorted(list(lr_2x))\n        lr_3x = sorted(list(lr_3x))\n        # print('decay', lr_decay)\n        # print('1x', lr_1x)\n        # print('2x', lr_2x)\n        # print('3x', lr_3x)\n        param_dict = {n: p for n, p in self.named_parameters()}\n        if args.layerwise_lr > 0:\n            if args.my_pile_stage == 2:\n                optim_groups = [\n                    {\"params\": [param_dict[n] for n in lr_1x], \"weight_decay\": 0.0, \"my_lr_scale\": 1.0},\n                    {\"params\": [param_dict[n] for n in lr_2x], \"weight_decay\": 0.0, \"my_lr_scale\": 5.0},# test: 2e-3 / args.lr_init},\n                    {\"params\": [param_dict[n] for n in lr_3x], \"weight_decay\": 0.0, \"my_lr_scale\": 5.0},# test: 3e-3 / args.lr_init},\n                ]\n            else:\n                optim_groups = [\n                    {\"params\": [param_dict[n] for n in lr_1x], \"weight_decay\": 0.0, \"my_lr_scale\": 1.0},\n                    {\"params\": [param_dict[n] for n in lr_2x], \"weight_decay\": 0.0, \"my_lr_scale\": 2.0},"
        },
        {
            "comment": "This code initializes optimizer groups for model parameters based on learning rates and weight decay. If weight decay is provided, it adds separate optimization group with the specified weight decay. Depending on Deepspeed offload flag, it returns either DeepSpeedCPUAdam or FusedAdam optimizer instances.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":797-809",
            "content": "                    {\"params\": [param_dict[n] for n in lr_3x], \"weight_decay\": 0.0, \"my_lr_scale\": 3.0},\n                ]\n        else:\n            optim_groups = [{\"params\": [param_dict[n] for n in lr_1x], \"weight_decay\": 0.0, \"my_lr_scale\": 1.0}]\n        if args.weight_decay > 0:\n            optim_groups += [{\"params\": [param_dict[n] for n in lr_decay], \"weight_decay\": args.weight_decay, \"my_lr_scale\": 1.0}]\n            if self.deepspeed_offload:\n                return DeepSpeedCPUAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, adamw_mode=True, amsgrad=False)\n            return FusedAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, adam_w_mode=True, amsgrad=False)\n        else:\n            if self.deepspeed_offload:\n                return DeepSpeedCPUAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, adamw_mode=False, weight_decay=0, amsgrad=False)"
        },
        {
            "comment": "This code snippet defines a model that uses either FusedAdam or ZeroOneAdam optimizer based on the deepspeed_offload property. The function forward takes an index and processes it through embedding, potentially dropout, and then passes it to multiple blocks if grad_cp is 1. The deepspeed_offload property checks a DeepSpeedStrategy configuration for offloading settings.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":810-833",
            "content": "            return FusedAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, adam_w_mode=False, weight_decay=0, amsgrad=False)\n        # return ZeroOneAdam(optim_groups, lr=self.args.lr_init, betas=self.args.betas, eps=self.args.adam_eps, bias_correction=True, weight_decay=0, amsgrad=False, cuda_aware=False)\n    @property\n    def deepspeed_offload(self) -> bool:\n        strategy = self.trainer.strategy\n        if isinstance(strategy, DeepSpeedStrategy):\n            cfg = strategy.config[\"zero_optimization\"]\n            return cfg.get(\"offload_optimizer\") or cfg.get(\"offload_param\")\n        return False\n    def forward(self, idx):\n        args = self.args\n        B, T = idx.size()\n        assert T <= args.ctx_len, \"Cannot forward, model ctx_len is exhausted.\"\n        x = self.emb(idx)\n        x_emb = x\n        if args.dropout > 0:\n            x = self.drop0(x)\n        if args.tiny_att_dim > 0:\n            for block in self.blocks:\n                if args.grad_cp == 1:"
        },
        {
            "comment": "This code is responsible for handling the checkpointing and layer execution within a specific block in the RWKV model. It also handles head attention calculations and applies one-hot encoding based on the floating-point mode environment variable.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":834-859",
            "content": "                    x = deepspeed.checkpointing.checkpoint(block, x, x_emb)\n                else:\n                    x = block(x, x_emb)\n        else:\n            for block in self.blocks:\n                if args.grad_cp == 1:\n                    x = deepspeed.checkpointing.checkpoint(block, x)\n                else:\n                    x = block(x)\n        x = self.ln_out(x)\n        if args.head_qk > 0:\n            q = self.head_q(x)[:, :T, :]\n            k = self.head_k(x)[:, :T, :]\n            c = (q @ k.transpose(-2, -1)) * (1.0 / args.head_qk)\n            c = c.masked_fill(self.copy_mask[:T, :T] == 0, 0)\n            if \"32\" in os.environ[\"RWKV_FLOAT_MODE\"]:\n                c = c @ F.one_hot(idx, num_classes=args.vocab_size)\n            elif os.environ[\"RWKV_FLOAT_MODE\"] == \"fp16\":\n                c = c @ F.one_hot(idx, num_classes=args.vocab_size).half()\n            elif os.environ[\"RWKV_FLOAT_MODE\"] == \"bf16\":\n                c = c @ F.one_hot(idx, num_classes=args.vocab_size).bfloat16()\n            x = self.head(x) + c"
        },
        {
            "comment": "This code contains two methods: '__call__' and 'training_step'. The '__call__' method performs a forward pass through the model by calling the 'head' method if not in training mode, otherwise it returns the input 'x' as-is. In the 'training_step' method, it checks the argument 'args.my_qa_mask' to determine whether to perform training or not. If 'args.my_qa_mask != 1', it calculates and returns a loss using cross entropy loss function. If 'args.my_qa_mask == 1', it applies a mask to the targets during training.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":860-888",
            "content": "        else:\n            x = self.head(x)\n        return x\n    def training_step(self, batch, batch_idx):\n        args = self.args\n        if args.my_qa_mask != 1:\n            idx, targets = batch\n            logits = self(idx)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n            # if '0' in os.environ[\"RWKV_MY_TESTING\"]:\n            #     print('logits', logits)\n            #     torch.set_printoptions(threshold=10000)\n            #     print('idx', idx)\n            #     exit(0)\n        else:\n            idx, targets, mask = batch\n            mask = mask.view(-1)\n            sum_mask = torch.sum(mask).item()\n            # if sum_mask == 0:\n            #     return torch.tensor([0.0], requires_grad=True)\n            logits = self(idx)\n            if sum_mask == mask.shape[0]:\n                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n                # print('rank', self.global_rank, 'loss', loss.item())\n            else:\n                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction='none')"
        },
        {
            "comment": "This code is defining the model, loss calculation, and training step functions for a neural network. It calculates the loss based on a masked sum of the loss_raw values, and then performs an all-gather operation to collect the losses from all ranks before proceeding to the next steps. The output is wrapped with L2 norm using the apply function. The generate_init_weight function prints some information at the start of training.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":889-915",
            "content": "                # loss_raw = loss\n                loss = torch.sum(loss * mask) / sum_mask\n                # torch.set_printoptions(threshold=10000)\n                # if True: #self.global_rank == 1:\n                #     tmp = ''\n                #     sss = 0\n                #     ccc = 0\n                #     for i in range(mask.shape[0]):\n                #         if mask[i] > 0:\n                #             tmp += str(idx.view(-1)[i].item()) + ','\n                #             sss += loss_raw.view(-1)[i].float().item()\n                #             ccc += 1\n                #     print('rank', self.global_rank, 'loss', loss.item(), 'lavg', sss / ccc)#, 'tmp', tmp, 'input', idx)\n        return L2Wrap.apply(loss, logits)\n    def training_step_end(self, batch_parts):\n        if pl.__version__[0]!='2':\n            all = self.all_gather(batch_parts)\n            if self.trainer.is_global_zero:\n                self.trainer.my_loss_all = all\n    def generate_init_weight(self):\n        print(\n            f\"\"\"\n############################################################################"
        },
        {
            "comment": "Initializing model weights for large models is slow, so the code handles this process by iterating over each item in self.state_dict(). For certain specific named parameters (like 'ln_x.weight', '.att.output.', etc.), it sets values accordingly. Otherwise, it simply copies the original values. The lr_init is used to set scale and gain based on the shape of the parameter. If the environment variable RWKV_MY_TESTING contains 'r', certain parameters are excluded from initialization.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":916-942",
            "content": "#\n# Init model weight (slow for large models)...\n#\n############################################################################\n\"\"\"\n        )\n        m = {}\n        for n in self.state_dict():\n            p = self.state_dict()[n]\n            shape = p.shape\n            gain = 1.0\n            scale = 1.0\n            if \"ln_\" in n or \".ln\" in n or \"time_\" in n or \"_mask\" in n or \"pos_emb\" in n or '.mask.' in n:\n                if 'ln_x.weight' in n:\n                    layer_scale = (1+int(n.split('.')[1])) / self.args.n_layer\n                    m[n] = (p * 0.0) + (layer_scale ** 0.7)\n                else:\n                    m[n] = p\n            else:\n                if n == \"emb.weight\":\n                    scale = -1 * self.args.lr_init\n                else:\n                    if shape[0] > shape[1]:\n                        gain = math.sqrt(shape[0] / shape[1])\n                    if 'r' in os.environ[\"RWKV_MY_TESTING\"]:\n                        zero = [\".att.output.\", \".ffn.value.\", \".ffn.receptance.\", \".ffnPre.value.\", \".ffnPre.receptance.\", \"head_q.\", '.oo.', '.rr.']"
        },
        {
            "comment": "This code checks the variable 'n' to determine its scale factor. If 'n' matches a specific set of keys, it sets the scale to 0 (zero initialization), 0.5, or 0.1. It then creates an empty tensor based on GPU accelerator settings and initializes it with zero or uniform values depending on the scale.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":943-965",
            "content": "                    else:\n                        zero = [\".att.key.\", \".att.receptance.\", \".att.output.\", \".ffn.value.\", \".ffn.receptance.\", \".ffnPre.value.\", \".ffnPre.receptance.\", \"head_q.\", '.oo.', '.rr.']\n                    for kk in zero:\n                        if kk in n:\n                            scale = 0\n                    if n == \"head.weight\":\n                        scale = 0.5\n                    if \"head_k.\" in n:\n                        scale = 0.1\n                    if \"head_q.\" in n:\n                        scale = 0\n                print(f\"{str(shape[0]).ljust(5)} {str(shape[1]).ljust(5)} {str(scale).ljust(4)} {n}\")\n                if self.args.accelerator.upper() == \"GPU\":\n                    m[n] = torch.empty((shape[0], shape[1]), device=\"cuda\")\n                else:\n                    m[n] = torch.empty((shape[0], shape[1]))\n                if scale == 0:\n                    nn.init.zeros_(m[n])\n                elif scale < 0:\n                    nn.init.uniform_(m[n], a=scale, b=-scale)"
        },
        {
            "comment": "This code initializes the model's weights using orthogonal initialization with a specified gain and scale. It then moves the weights to CPU and converts them based on the RWKV_FLOAT_MODE environment variable. Finally, it collects garbage and empties the CUDA cache before returning the updated model parameters.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v4neo/src/model.py\":966-980",
            "content": "                else:\n                    nn.init.orthogonal_(m[n], gain=gain * scale)\n            m[n] = m[n].cpu()\n            if os.environ[\"RWKV_FLOAT_MODE\"] == \"fp16\":\n                m[n] = m[n].half()\n            elif os.environ[\"RWKV_FLOAT_MODE\"] == \"bf16\":\n                m[n] = m[n].bfloat16()\n            # if n == \"emb.weight\":\n            #     print(m[n])\n        gc.collect()\n        torch.cuda.empty_cache()\n        return m"
        }
    ]
}