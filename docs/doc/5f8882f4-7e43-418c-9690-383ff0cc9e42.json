{
    "summary": "This code defines a tokenizer class using a trie data structure, with methods for encoding and decoding bytes. The TRIE_TOKENIZER initializes the trie by reading lines from a file and includes a 'find_longest' method for finding the longest token within a key string.",
    "details": [
        {
            "comment": "Class \"TRIE\" for RWKV Language Model - represents a Trie data structure used for tokenizing input strings, storing values associated with each character path in the trie.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/tokenizer/rwkv_tokenizer.py\":0-31",
            "content": "########################################################################################################\n# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n########################################################################################################\nclass TRIE:\n    __slots__ = tuple(\"ch,to,values,front\".split(\",\"))\n    to:list\n    values:set\n    def __init__(self, front=None, ch=None):\n        self.ch = ch\n        self.to = [None for ch in range(256)]\n        self.values = set()\n        self.front = front\n    def __repr__(self):\n        fr = self\n        ret = []\n        while(fr!=None):\n            if(fr.ch!=None):\n                ret.append(fr.ch)\n            fr = fr.front\n        return \"<TRIE %s %s>\"%(ret[::-1], self.values)\n    def add(self, key:bytes, idx:int=0, val=None):\n        if(idx == len(key)):\n            if(val is None):\n                val = key\n            self.values.add(val)\n            return self\n        ch = key[idx]\n        if(self.to[ch] is None):\n            self.to[ch] = TRIE(front=self, ch=ch)"
        },
        {
            "comment": "Code is defining a TRIE data structure for tokenizing and mapping indices to tokens in a specified file. The `TRIE_TOKENIZER` class initializes the data structure by reading lines from a file, converting values to bytes, storing them in the TRIE, and creating a reverse index. The `find_longest` method is used for finding the longest token within a given key string.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/tokenizer/rwkv_tokenizer.py\":32-65",
            "content": "        return self.to[ch].add(key, idx=idx+1, val=val)\n    def find_longest(self, key:bytes, idx:int=0):\n        u:TRIE = self\n        ch:int = key[idx]\n        while(u.to[ch] is not None):\n            u = u.to[ch]\n            idx += 1\n            if(u.values):\n                ret = idx, u, u.values\n            if(idx==len(key)):\n                break\n            ch = key[idx]\n        return ret\nclass TRIE_TOKENIZER():\n    def __init__(self, file_name):\n        self.idx2token = {}\n        sorted = [] # must be already sorted\n        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n        for l in lines:\n            idx = int(l[:l.index(' ')])\n            x = eval(l[l.index(' '):l.rindex(' ')])\n            x = x.encode(\"utf-8\") if isinstance(x, str) else x\n            assert isinstance(x, bytes)\n            assert len(x) == int(l[l.rindex(' '):])\n            sorted += [x]\n            self.idx2token[idx] = x\n        self.token2idx = {}\n        for k,v in self.idx2token.items():\n            self.token2idx[v] = int(k)"
        },
        {
            "comment": "This code defines a tokenizer class that can encode and decode bytes using a trie data structure. The encodeBytes method converts input bytes to tokens, while the decodeBytes method reconverts tokens back into bytes. The encode and decode methods handle Unicode strings. The printTokens method prints the tokens along with their indices.",
            "location": "\"/media/root/Prima/works/RWKV-LM/docs/src/RWKV-v5/tokenizer/rwkv_tokenizer.py\":67-102",
            "content": "        self.root = TRIE()\n        for t, i in self.token2idx.items():\n            _ = self.root.add(t, val=(t, i))\n    def encodeBytes(self, src:bytes):\n        idx:int = 0\n        tokens = []\n        while (idx < len(src)):\n            _idx:int = idx\n            idx, _, values = self.root.find_longest(src, idx)\n            assert(idx != _idx)\n            _, token = next(iter(values))            \n            tokens.append(token)\n        return tokens\n    def decodeBytes(self, tokens):\n        return b''.join(map(lambda i: self.idx2token[i], tokens))\n    def encode(self, src):\n        return self.encodeBytes(src.encode(\"utf-8\"))\n    def decode(self, tokens):\n        try:\n            return self.decodeBytes(tokens).decode('utf-8')\n        except:\n            return '\\ufffd' # bad utf-8\n    def printTokens(self, tokens):\n        for i in tokens:\n            s = self.idx2token[i]\n            try:\n                s = s.decode('utf-8')\n            except:\n                pass\n            print(f'{repr(s)}{i}', end=' ')\n        print()"
        }
    ]
}